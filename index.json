[{"summary":"\u003cstyle\u003e\r\n  html, body {\r\n    background: #f0f2f9; \r\n  }\r\n\u003c/style\u003e\r\n\u003cp\u003e\u003ca href=\"https://chat.falconllm.tii.ae\" class=\"btn external\" target=\"_blank\"\u003eFalcon CHAT\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df\" class=\"btn external\" target=\"_blank\"\u003eHugging Face\u003c/a\u003e\n\u003ca href=\"https://github.com/tiiuae/falcon-h1\" class=\"btn external\" target=\"_blank\"\u003eGithub\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/spaces/tiiuae/Falcon-H1-Playground\" class=\"btn external\" target=\"_blank\"\u003eDEMO\u003c/a\u003e\n\u003ca href=\"https://discord.gg/vfw6k2G3\" class=\"btn external\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\r\n  \u003cdiv style=\"position: relative; width: 100%; max-width: 700px; aspect-ratio: 700 /600;\"\u003e\r\n    \u003ciframe \r\n      src=\"/plots_h1/falcon_h1_performance_scatter_2.html\" \r\n      style=\"border: none; position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"\r\n      allowfullscreen\r\n    \u003e\u003c/iframe\u003e\r\n  \u003c/div\u003e\r\n\u003c/div\u003e\r\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eToday, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers.\u003c/p\u003e","title":"Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance"},{"summary":"\u003cp\u003eWe are excited to introduce \u003cstrong\u003eFalcon-Arabic\u003c/strong\u003e, a 7B parameter language model that sets a new benchmark for Arabic NLP. Built on the Falcon 3 architecture, Falcon-Arabic is a multilingual model that supports Arabic, English, and several other languages. It excels in general knowledge, Arabic grammar, mathematical reasoning, complex problem solving, and understanding the rich diversity of Arabic dialects. Falcon-Arabic supports a context length of 32,000 tokens, allowing it to handle long documents and enabling advanced applications like retrieval-augmented generation (RAG), in-depth content creation, and knowledge-intensive tasks.\u003c/p\u003e","title":"Falcon-Arabic: A Breakthrough in Arabic Language Models"},{"summary":"\u003cp\u003eIn this blogpost, we present the key highlights and rationales about the \u003cem\u003eFalcon-Edge\u003c/em\u003e series - a collection of \u003cem\u003epowerful\u003c/em\u003e, \u003cem\u003euniversal\u003c/em\u003e, and \u003cem\u003efine-tunable\u003c/em\u003e language models available in ternary format, based on the BitNet architecture.\u003c/p\u003e\n\u003cp\u003eDrawing from our experience with BitNet, \u003cstrong\u003eFalcon-Edge\u003c/strong\u003e introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.\u003c/p\u003e","title":"Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models."},{"summary":"\u003cp\u003e\u003ca href=\"https://chat.falconllm.tii.ae\" class=\"btn external\" target=\"_blank\"\u003eFalcon CHAT\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026\" class=\"btn external\" target=\"_blank\"\u003eHugging Face\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/spaces/tiiuae/Falcon3-demo\" class=\"btn external\" target=\"_blank\"\u003eDEMO\u003c/a\u003e\n\u003ca href=\"https://discord.gg/vfw6k2G3\" class=\"btn external\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"welcome-to-the-falcon-3-family-of-open-models\"\u003eWelcome to the Falcon 3 Family of Open Models!\u003c/h1\u003e\n\u003cp\u003eWe introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by\n\u003ca href=\"https://www.tii.ae/ai-and-digital-science\"\u003eTechnology Innovation Institute (TII)\u003c/a\u003e in Abu Dhabi. By pushing the\nboundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open\nand accessible large foundation models.\u003c/p\u003e\n\u003cp\u003eFalcon3 represents a natural evolution from previous releases, emphasizing expanding the models\u0026rsquo; science, math, and code capabilities.\u003c/p\u003e","title":"Welcome to the Falcon 3 Family of Open Models!"},{"summary":"\u003cp\u003e\u003ca href=\"https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html\"\u003eFalcon Mamba\u003c/a\u003e is a new model by \u003ca href=\"https://www.tii.ae/ai-and-digital-science\"\u003eTechnology Innovation Institute (TII)\u003c/a\u003e in Abu Dhabi released under the \u003ca href=\"https://falconllm.tii.ae/falcon-mamba-7b-terms-and-conditions.html\"\u003eTII Falcon Mamba 7B License 1.0\u003c/a\u003e. The model is open access and available within the Hugging Face ecosystem \u003ca href=\"https://huggingface.co/tiiuae/falcon-mamba-7b\"\u003ehere\u003c/a\u003e for anyone to use for their research or application purposes.\u003c/p\u003e\n\u003cp\u003eIn this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.\u003c/p\u003e","title":"Welcome Falcon Mamba: The first strong attention-free 7B model"},{"summary":"","title":"Falcon Mamba: The First Competitive Attention-free 7B Language Model"},{"summary":"","title":"Falcon2-11B Technical Report"},{"summary":"\u003ch1 id=\"falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-and-11-languages\"\u003eFalcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages\u003c/h1\u003e\n\u003cp\u003e\u003ca name=\"the-falcon-models\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"the-falcon-2-models\"\u003eThe Falcon 2 Models\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"www.tii.ae\"\u003eTII\u003c/a\u003e is launching a new generation of models, \u003ca href=\"https://falconllm.tii.ae/\"\u003eFalcon 2\u003c/a\u003e, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.\u003c/p\u003e\n\u003cp\u003eThe first generation of Falcon models, featuring \u003ca href=\"https://huggingface.co/tiiuae/falcon-40b\"\u003eFalcon-40B\u003c/a\u003e and \u003ca href=\"https://huggingface.co/tiiuae/falcon-180B\"\u003eFalcon-180B\u003c/a\u003e, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html\"\u003eRefinedWeb, Penedo et al., 2023\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2311.16867\"\u003eThe Falcon Series of Open Language Models, Almazrouei et al., 2023\u003c/a\u003e papers, and the \u003ca href=\"https://huggingface.co/blog/falcon\"\u003eFalcon\u003c/a\u003e and \u003ca href=\"https://huggingface.co/blog/falcon-180b\"\u003eFalcon-180B\u003c/a\u003e blog posts.\u003c/p\u003e","title":"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages"},{"summary":"","title":"The Falcon Series of Open Language Models"},{"summary":"\u003ch1 id=\"spread-your-wings-falcon-180b-is-here\"\u003eSpread Your Wings: Falcon 180B is here\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eToday, we\u0026rsquo;re excited to welcome \u003ca href=\"https://falconllm.tii.ae/\"\u003eTII\u0026rsquo;s\u003c/a\u003e Falcon 180B to HuggingFace!\u003c/strong\u003e Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII\u0026rsquo;s \u003ca href=\"https://huggingface.co/datasets/tiiuae/falcon-refinedweb\"\u003eRefinedWeb\u003c/a\u003e dataset. This represents the longest single-epoch pretraining for an open model.\u003c/p\u003e\n\u003cp\u003eYou can find the model on the Hugging Face Hub (\u003ca href=\"https://huggingface.co/tiiuae/falcon-180B\"\u003ebase\u003c/a\u003e and \u003ca href=\"https://huggingface.co/tiiuae/falcon-180B-chat\"\u003echat\u003c/a\u003e model) and interact with the model on the \u003ca href=\"https://huggingface.co/spaces/tiiuae/falcon-180b-chat\"\u003eFalcon Chat Demo Space\u003c/a\u003e.\u003c/p\u003e","title":"Spread Your Wings: Falcon 180B is here"},{"summary":"","title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}]