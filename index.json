[{"summary":"\u003cp\u003e\u003ca href=\"https://chat.falconllm.tii.ae\" class=\"btn external\" target=\"_blank\"\u003eFalcon CHAT\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/tiiuae/falcon-h1r\" class=\"btn external\" target=\"_blank\"\u003eHugging Face\u003c/a\u003e\n\u003ca href=\"https://discord.gg/Cbek57PrZE\" class=\"btn external\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIntroducing \u003cspan class=\"bold\"\u003e\u003ca href=\"https://huggingface.co/tiiuae/Falcon-H1R-7B-FP8\"\u003eFalcon H1R 7B FP8\u003c/a\u003e\u003c/span\u003e, a fully quantized version of the \u003ca href=\"https://huggingface.co/tiiuae/Falcon-H1R-7B\"\u003eFalcon H1R 7B\u003c/a\u003e model that packs both weights and activations into FP8 format. Using \u003ca href=\"https://github.com/NVIDIA/Model-Optimizer\"\u003eNVIDIA Model Optimizer\u003c/a\u003e and post-training quantization (PTQ) workflow, the FP8 quantized model preserves the original BF16 quality performance while delivering a 1.2×–1.5× throughput boost and halving GPU memory footprint.\u003c/p\u003e\n\u003ch1 id=\"evaluations\"\u003eEvaluations\u003c/h1\u003e\n\u003cp\u003eThe FP8 variant retains essentially the same accuracy as BF16 across all three major reasoning tasks: AIME25 drops only 0.8 % (from 83.1 % to 82.3 %), LCB‑v6 falls by 1 % (68.6 % → 67.6 %), and GPQA‑D shows a negligible 0.1 % difference (61.3 % → 61.2 %). These results confirm that the FP8 PTQ preserves benchmark performance while delivering substantial memory and throughput gains.\u003c/p\u003e","title":"Falcon‑H1R-FP8: Accelerating Inference with Quantized Precision"},{"summary":"\u003cp\u003e\u003ca href=\"https://chat.falconllm.tii.ae\" class=\"btn external\" target=\"_blank\"\u003eFalcon CHAT\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/tiiuae/falcon-h1r\" class=\"btn external\" target=\"_blank\"\u003eHugging Face\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/spaces/tiiuae/Falcon-H1R-playground\" class=\"btn external\" target=\"_blank\"\u003eDEMO\u003c/a\u003e\n\u003ca href=\"https://discord.gg/Cbek57PrZE\" class=\"btn external\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"introducing-falcon-h1r-7b\"\u003eIntroducing Falcon H1R 7B\u003c/h1\u003e\n\u003cp\u003eWe’re excited to unveil \u003cspan class=\"bold\"\u003eFalcon H1R 7B\u003c/span\u003e, a decoder-only large language model, developed by the \u003ca href=\"https://www.tii.ae/ai-and-digital-science\"\u003eTechnology Innovation Institute (TII)\u003c/a\u003e in Abu Dhabi. Building upon the robust foundation of Falcon-H1 Base model, \u003cspan class=\"bold\"\u003eFalcon H1R 7B\u003c/span\u003e takes a major leap forward in reasoning capabilities.\u003c/p\u003e\n\u003cp\u003eDespite its modest 7 billion‑parameter size, \u003cspan class=\"bold\"\u003eFalcon H1R 7B\u003c/span\u003e matches or outperforms state‑of‑the‑art reasoning models that are 2–7× larger, proving its exceptional parameter efficiency and does so consistently across a wide range of reasoning‑intensive benchmarks.\u003c/p\u003e","title":"Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling"},{"summary":"\u003cblockquote\u003e\n\u003cp\u003eCheck out the \u003ca href=\"https://falcon-lm.github.io/ar/blog/falcon-h1-arabic/\"\u003eArabic version\u003c/a\u003e translated by \u003cstrong\u003eFalcon-H1-Arabic\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThe journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we\u0026rsquo;re excited to announce \u003cstrong\u003eFalcon-H1-Arabic\u003c/strong\u003e, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in \u003cstrong\u003ethree\u003c/strong\u003e powerful models that set new standards for Arabic natural language processing.\u003c/p\u003e","title":"Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture"},{"summary":"\u003cblockquote\u003e\n\u003cp\u003eCheck out the \u003ca href=\"https://falcon-lm.github.io/ar/blog/falcon-arabic/\"\u003eArabic version\u003c/a\u003e translated by \u003cstrong\u003eFalcon-Arabic\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eWe are excited to introduce \u003cstrong\u003eFalcon-Arabic\u003c/strong\u003e, a 7B parameter Language Model that sets a new benchmark for Arabic NLP. Built on the Falcon 3 architecture, Falcon-Arabic is a multilingual model that supports Arabic, English, and several other languages. It excels in general knowledge, Arabic grammar, mathematical reasoning, complex problem solving, and understanding the rich diversity of Arabic dialects. Falcon-Arabic supports a context length of 32,000 tokens, allowing it to handle long documents and enabling advanced applications like retrieval-augmented generation (RAG), in-depth content creation, and knowledge-intensive tasks.\u003c/p\u003e","title":"Falcon-Arabic: A Breakthrough in Arabic Language Models"},{"summary":"\u003cstyle\u003e\r\n  html, body {\r\n    background: #f0f2f9;\r\n  }\r\n\u003c/style\u003e\r\n\u003cp\u003e\u003ca href=\"https://chat.falconllm.tii.ae\" class=\"btn external\" target=\"_blank\"\u003eFalcon CHAT\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df\" class=\"btn external\" target=\"_blank\"\u003eHugging Face\u003c/a\u003e\n\u003ca href=\"https://arxiv.org/abs/2507.22448\" class=\"btn external\" target=\"_blank\"\u003ePaper\u003c/a\u003e\n\u003ca href=\"https://github.com/tiiuae/falcon-h1\" class=\"btn external\" target=\"_blank\"\u003eGithub\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/spaces/tiiuae/Falcon-H1-Playground\" class=\"btn external\" target=\"_blank\"\u003eDEMO\u003c/a\u003e\n\u003ca href=\"https://discord.gg/Cbek57PrZE\" class=\"btn external\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: center;\"\u003e\r\n  \u003cdiv style=\"position: relative; width: 100%; max-width: 700px; aspect-ratio: 700 /600;\"\u003e\r\n    \u003ciframe\r\n      src=\"/plots_h1/falcon_h1_performance_scatter_2.html\"\r\n      style=\"border: none; position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"\r\n      allowfullscreen\r\n    \u003e\u003c/iframe\u003e\r\n  \u003c/div\u003e\r\n\u003c/div\u003e\r\n\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eToday, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers.\u003c/p\u003e","title":"Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance"},{"summary":"\u003cp\u003eIn this blogpost, we present the key highlights and rationales about the \u003cem\u003eFalcon-Edge\u003c/em\u003e series - a collection of \u003cem\u003epowerful\u003c/em\u003e, \u003cem\u003euniversal\u003c/em\u003e, and \u003cem\u003efine-tunable\u003c/em\u003e language models available in ternary format, based on the BitNet architecture.\u003c/p\u003e\n\u003cp\u003eDrawing from our experience with BitNet, \u003cstrong\u003eFalcon-Edge\u003c/strong\u003e introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.\u003c/p\u003e","title":"Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models."},{"summary":"\u003cstyle\u003e\ntable {\n  border-collapse: collapse;\n  width: 100%;\n  background-color: transparent; /* nền theo system/theme */\n  border-radius: 8px;\n  overflow: hidden;\n  color: inherit; /* chữ theo theme */\n}\nth, td {\n  padding: 12px 16px;\n  border: 1px solid; /* rõ ràng */\n  border-color: rgba(0,0,0,0.3); /* mặc định cho light */\n}\n@media (prefers-color-scheme: dark) {\n  th, td {\n    border-color: rgba(255,255,255,0.2); /* rõ hơn trong dark */\n  }\n  tr:nth-child(even) td {\n    background-color: rgba(255,255,255,0.05); /* xen kẽ màu dark */\n  }\n  tr:hover td {\n    background-color: rgba(255,255,255,0.1); /* hover rõ dark */\n  }\n}\n@media (prefers-color-scheme: light) {\n  tr:nth-child(even) td {\n    background-color: rgba(0,0,0,0.05); /* xen kẽ màu light */\n  }\n  tr:hover td {\n    background-color: rgba(0,0,0,0.1); /* hover rõ light */\n  }\n}\nth {\n  font-weight: bold;\n}\n\nblockquote {\n  border-left: 4px solid rgba(128,128,128,0.4);\n  margin: 1em 0;\n  padding: 0.5em 1em;\n  background-color: transparent;\n  color: inherit;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;\n  font-size: 0.9em;\n  font-style: italic;\n}\nblockquote p {\n  margin: 0;\n}\nblockquote p strong {\n  font-weight: bold;\n  font-style: italic;\n}\nblockquote p::before {\n  content: \"“\";\n}\nblockquote p::after {\n  content: \"”\";\n}\n\nul + p strong:first-child,\nul + p:has(strong:first-child) {\n  display: block;\n  margin-top: 1.5em;\n  margin-bottom: 0.5em;\n}\np strong:only-child {\n  display: inline-block;\n  margin-top: 1em;\n  margin-bottom: 0.5em;\n}\n\u003c/style\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eNot every task is about retrieving facts, sometimes you need the model to think, weigh options, and walk through a problem in multiple steps. That is where the new Falcon reasoning models come in. Built to deliver structured and logical outputs, they extend the capabilities of the Falcon family into domains like problem solving, mathematics, code logic, and multi stage decision making, while still being lightweight enough to run on laptops or compact devices. We have already gone through the series guiding you on how to work with the instruct model versions, including \u003ca href=\"http://localhost:1313/tutorials/falcon-h1/\"\u003eFalcon H1 Instruct\u003c/a\u003e  and \u003ca href=\"http://localhost:1313/tutorials/falcon-3/\"\u003eFalcon3 Instruct\u003c/a\u003e , and now it is time to start exploring reasoning models to see how they change the way we approach complex workloads.\u003c/p\u003e","title":"Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI"},{"summary":"\u003cstyle\u003e\ntable {\n  border-collapse: collapse;\n  width: 100%;\n  background-color: transparent; /* nền theo system/theme */\n  border-radius: 8px;\n  overflow: hidden;\n  color: inherit; /* chữ theo theme */\n}\nth, td {\n  padding: 12px 16px;\n  border: 1px solid; /* rõ ràng */\n  border-color: rgba(0,0,0,0.3); /* mặc định cho light */\n}\n@media (prefers-color-scheme: dark) {\n  th, td {\n    border-color: rgba(255,255,255,0.2); /* rõ hơn trong dark */\n  }\n  tr:nth-child(even) td {\n    background-color: rgba(255,255,255,0.05); /* xen kẽ màu dark */\n  }\n  tr:hover td {\n    background-color: rgba(255,255,255,0.1); /* hover rõ dark */\n  }\n}\n@media (prefers-color-scheme: light) {\n  tr:nth-child(even) td {\n    background-color: rgba(0,0,0,0.05); /* xen kẽ màu light */\n  }\n  tr:hover td {\n    background-color: rgba(0,0,0,0.1); /* hover rõ light */\n  }\n}\nth {\n  font-weight: bold;\n}\n\nblockquote {\n  border-left: 4px solid rgba(128,128,128,0.4);\n  margin: 1em 0;\n  padding: 0.5em 1em;\n  background-color: transparent;\n  color: inherit;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;\n  font-size: 0.9em;\n  font-style: italic;\n}\nblockquote p {\n  margin: 0;\n}\nblockquote p strong {\n  font-weight: bold;\n  font-style: italic;\n}\nblockquote p::before {\n  content: \"“\";\n}\nblockquote p::after {\n  content: \"”\";\n}\n\nul + p strong:first-child,\nul + p:has(strong:first-child) {\n  display: block;\n  margin-top: 1.5em;\n  margin-bottom: 0.5em;\n}\np strong:only-child {\n  display: inline-block;\n  margin-top: 1em;\n  margin-bottom: 0.5em;\n}\n\u003c/style\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eIf you have ever wanted to run a powerful AI model directly on your laptop for quick experiments or personal projects, Falcon 3 makes it simple. You do not need expensive servers or constant internet access, just your device and the model running locally. It is built for speed, lightweight setup, and customization so you can focus on creating, not configuring.\u003c/p\u003e","title":"Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI"},{"summary":"\u003cstyle\u003e\ntable {\n  border-collapse: collapse;\n  width: 100%;\n  background-color: transparent; /* nền theo system/theme */\n  border-radius: 8px;\n  overflow: hidden;\n  color: inherit; /* chữ theo theme */\n}\nth, td {\n  padding: 12px 16px;\n  border: 1px solid; /* rõ ràng */\n  border-color: rgba(0,0,0,0.3); /* mặc định cho light */\n}\n@media (prefers-color-scheme: dark) {\n  th, td {\n    border-color: rgba(255,255,255,0.2); /* rõ hơn trong dark */\n  }\n  tr:nth-child(even) td {\n    background-color: rgba(255,255,255,0.05); /* xen kẽ màu dark */\n  }\n  tr:hover td {\n    background-color: rgba(255,255,255,0.1); /* hover rõ dark */\n  }\n}\n@media (prefers-color-scheme: light) {\n  tr:nth-child(even) td {\n    background-color: rgba(0,0,0,0.05); /* xen kẽ màu light */\n  }\n  tr:hover td {\n    background-color: rgba(0,0,0,0.1); /* hover rõ light */\n  }\n}\nth {\n  font-weight: bold;\n}\n\nblockquote {\n  border-left: 4px solid rgba(128,128,128,0.4);\n  margin: 1em 0;\n  padding: 0.5em 1em;\n  background-color: transparent;\n  color: inherit;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;\n  font-size: 0.9em;\n  font-style: italic;\n}\nblockquote p {\n  margin: 0;\n}\nblockquote p strong {\n  font-weight: bold;\n  font-style: italic;\n}\nblockquote p::before {\n  content: \"“\";\n}\nblockquote p::after {\n  content: \"”\";\n}\n\nul + p strong:first-child,\nul + p:has(strong:first-child) {\n  display: block;\n  margin-top: 1.5em;\n  margin-bottom: 0.5em;\n}\np strong:only-child {\n  display: inline-block;\n  margin-top: 1em;\n  margin-bottom: 0.5em;\n}\n\u003c/style\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eSometimes your projects are about more than quick answers. You might need to read through hundreds of pages, keep a long conversation going, or work in multiple languages. Falcon H1 was designed for exactly that. By combining Transformer-based attention with State Space Models (SSM), it understands complex information while still running efficiently on everyday hardware.\u003c/p\u003e","title":"Falcon-H1 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI"},{"summary":"\u003cp\u003e\u003ca href=\"https://chat.falconllm.tii.ae\" class=\"btn external\" target=\"_blank\"\u003eFalcon CHAT\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026\" class=\"btn external\" target=\"_blank\"\u003eHugging Face\u003c/a\u003e\n\u003ca href=\"https://huggingface.co/spaces/tiiuae/Falcon3-demo\" class=\"btn external\" target=\"_blank\"\u003eDEMO\u003c/a\u003e\n\u003ca href=\"https://discord.gg/Cbek57PrZE\" class=\"btn external\" target=\"_blank\"\u003eDISCORD\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"welcome-to-the-falcon-3-family-of-open-models\"\u003eWelcome to the Falcon 3 Family of Open Models!\u003c/h1\u003e\n\u003cp\u003eWe introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by\n\u003ca href=\"https://www.tii.ae/ai-and-digital-science\"\u003eTechnology Innovation Institute (TII)\u003c/a\u003e in Abu Dhabi. By pushing the\nboundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open\nand accessible large foundation models.\u003c/p\u003e\n\u003cp\u003eFalcon3 represents a natural evolution from previous releases, emphasizing expanding the models\u0026rsquo; science, math, and code capabilities.\u003c/p\u003e","title":"Welcome to the Falcon 3 Family of Open Models!"},{"summary":"\u003cp\u003e\u003ca href=\"https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html\"\u003eFalcon Mamba\u003c/a\u003e is a new model by \u003ca href=\"https://www.tii.ae/ai-and-digital-science\"\u003eTechnology Innovation Institute (TII)\u003c/a\u003e in Abu Dhabi released under the \u003ca href=\"https://falconllm.tii.ae/falcon-mamba-7b-terms-and-conditions.html\"\u003eTII Falcon Mamba 7B License 1.0\u003c/a\u003e. The model is open access and available within the Hugging Face ecosystem \u003ca href=\"https://huggingface.co/tiiuae/falcon-mamba-7b\"\u003ehere\u003c/a\u003e for anyone to use for their research or application purposes.\u003c/p\u003e\n\u003cp\u003eIn this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.\u003c/p\u003e","title":"Welcome Falcon Mamba: The first strong attention-free 7B model"},{"summary":"","title":"Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance"},{"summary":"","title":"Falcon Mamba: The First Competitive Attention-free 7B Language Model"},{"summary":"","title":"Falcon2-11B Technical Report"},{"summary":"\u003ch1 id=\"falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-and-11-languages\"\u003eFalcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages\u003c/h1\u003e\n\u003cp\u003e\u003ca name=\"the-falcon-models\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"the-falcon-2-models\"\u003eThe Falcon 2 Models\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"www.tii.ae\"\u003eTII\u003c/a\u003e is launching a new generation of models, \u003ca href=\"https://falconllm.tii.ae/\"\u003eFalcon 2\u003c/a\u003e, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.\u003c/p\u003e\n\u003cp\u003eThe first generation of Falcon models, featuring \u003ca href=\"https://huggingface.co/tiiuae/falcon-40b\"\u003eFalcon-40B\u003c/a\u003e and \u003ca href=\"https://huggingface.co/tiiuae/falcon-180B\"\u003eFalcon-180B\u003c/a\u003e, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html\"\u003eRefinedWeb, Penedo et al., 2023\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/2311.16867\"\u003eThe Falcon Series of Open Language Models, Almazrouei et al., 2023\u003c/a\u003e papers, and the \u003ca href=\"https://huggingface.co/blog/falcon\"\u003eFalcon\u003c/a\u003e and \u003ca href=\"https://huggingface.co/blog/falcon-180b\"\u003eFalcon-180B\u003c/a\u003e blog posts.\u003c/p\u003e","title":"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages"},{"summary":"","title":"The Falcon Series of Open Language Models"},{"summary":"\u003ch1 id=\"spread-your-wings-falcon-180b-is-here\"\u003eSpread Your Wings: Falcon 180B is here\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eToday, we\u0026rsquo;re excited to welcome \u003ca href=\"https://falconllm.tii.ae/\"\u003eTII\u0026rsquo;s\u003c/a\u003e Falcon 180B to HuggingFace!\u003c/strong\u003e Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII\u0026rsquo;s \u003ca href=\"https://huggingface.co/datasets/tiiuae/falcon-refinedweb\"\u003eRefinedWeb\u003c/a\u003e dataset. This represents the longest single-epoch pretraining for an open model.\u003c/p\u003e\n\u003cp\u003eYou can find the model on the Hugging Face Hub (\u003ca href=\"https://huggingface.co/tiiuae/falcon-180B\"\u003ebase\u003c/a\u003e and \u003ca href=\"https://huggingface.co/tiiuae/falcon-180B-chat\"\u003echat\u003c/a\u003e model) and interact with the model on the \u003ca href=\"https://huggingface.co/spaces/tiiuae/falcon-180b-chat\"\u003eFalcon Chat Demo Space\u003c/a\u003e.\u003c/p\u003e","title":"Spread Your Wings: Falcon 180B is here"},{"summary":"","title":"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}]