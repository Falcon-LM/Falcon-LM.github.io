<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script>!function(e,t){if(!e.rdt){var s,o,n=e.rdt=function(){n.sendEvent?n.sendEvent.apply(n,arguments):n.callQueue.push(arguments)};n.callQueue=[],s=t.createElement("script"),s.src="https://www.redditstatic.com/ads/pixel.js",s.async=!0,o=t.getElementsByTagName("script")[0],o.parentNode.insertBefore(s,o)}}(window,document),rdt("init","a2_f0we4sxffrnm"),rdt("track","PageVisit")</script><script async src="https://www.googletagmanager.com/gtag/js?id=AW-16575356988"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","AW-16575356988")</script><title>Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI | Falcon</title>
<meta name=keywords content><meta name=description content="
Overview
Not every task is about retrieving facts, sometimes you need the model to think, weigh options, and walk through a problem in multiple steps. That is where the new Falcon reasoning models come in. Built to deliver structured and logical outputs, they extend the capabilities of the Falcon family into domains like problem solving, mathematics, code logic, and multi stage decision making, while still being lightweight enough to run on laptops or compact devices. We have already gone through the series guiding you on how to work with the instruct model versions, including Falcon H1 Instruct  and Falcon3 Instruct , and now it is time to start exploring reasoning models to see how they change the way we approach complex workloads."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/tutorials/falcon-h1r/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/tutorials/falcon-h1r/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI"><meta property="og:description" content="
Overview
Not every task is about retrieving facts, sometimes you need the model to think, weigh options, and walk through a problem in multiple steps. That is where the new Falcon reasoning models come in. Built to deliver structured and logical outputs, they extend the capabilities of the Falcon family into domains like problem solving, mathematics, code logic, and multi stage decision making, while still being lightweight enough to run on laptops or compact devices. We have already gone through the series guiding you on how to work with the instruct model versions, including Falcon H1 Instruct  and Falcon3 Instruct , and now it is time to start exploring reasoning models to see how they change the way we approach complex workloads."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/tutorials/falcon-h1r/"><meta property="og:image" content="https://falcon-lm.github.io/tutorials/falcon-h1r/images/overview.png"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2025-02-24T12:00:00+00:00"><meta property="article:modified_time" content="2025-02-24T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/tutorials/falcon-h1r/images/overview.png"><meta name=twitter:title content="Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI"><meta name=twitter:description content="
Overview
Not every task is about retrieving facts, sometimes you need the model to think, weigh options, and walk through a problem in multiple steps. That is where the new Falcon reasoning models come in. Built to deliver structured and logical outputs, they extend the capabilities of the Falcon family into domains like problem solving, mathematics, code logic, and multi stage decision making, while still being lightweight enough to run on laptops or compact devices. We have already gone through the series guiding you on how to work with the instruct model versions, including Falcon H1 Instruct  and Falcon3 Instruct , and now it is time to start exploring reasoning models to see how they change the way we approach complex workloads."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Tutorials","item":"https://falcon-lm.github.io/tutorials/"},{"@type":"ListItem","position":2,"name":"Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI","item":"https://falcon-lm.github.io/tutorials/falcon-h1r/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI","name":"Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI","description":" Overview Not every task is about retrieving facts, sometimes you need the model to think, weigh options, and walk through a problem in multiple steps. That is where the new Falcon reasoning models come in. Built to deliver structured and logical outputs, they extend the capabilities of the Falcon family into domains like problem solving, mathematics, code logic, and multi stage decision making, while still being lightweight enough to run on laptops or compact devices. We have already gone through the series guiding you on how to work with the instruct model versions, including Falcon H1 Instruct and Falcon3 Instruct , and now it is time to start exploring reasoning models to see how they change the way we approach complex workloads.\n","keywords":[],"articleBody":" Overview Not every task is about retrieving facts, sometimes you need the model to think, weigh options, and walk through a problem in multiple steps. That is where the new Falcon reasoning models come in. Built to deliver structured and logical outputs, they extend the capabilities of the Falcon family into domains like problem solving, mathematics, code logic, and multi stage decision making, while still being lightweight enough to run on laptops or compact devices. We have already gone through the series guiding you on how to work with the instruct model versions, including Falcon H1 Instruct and Falcon3 Instruct , and now it is time to start exploring reasoning models to see how they change the way we approach complex workloads.\nThe lineup includes the Falcon H1R 7B, offering the highest reasoning capacity in the series, and two ultra portable versions, Falcon H1 Tiny R 0.6B and Falcon H1 Tiny R 90M, designed for situations where resources are limited but intelligent analysis is still essential.\nWhat sets Falcon reasoning models apart\nPurpose built for step by step reasoning instead of purely inductive response generation. Optimized efficiency to run on consumer GPUs, Apple Silicon, and even low spec CPUs for mobile and embedded scenarios. Multiple size tiers, 7B, 0.6B, 90M to fit everything from full desktop deployments to ultra minimal setups. Robust in handling structured problem solving, whether it is complex math, long chain of thought analysis, or rule driven workflows. Getting started\nThese models are provided in gguf format for llama.cpp, which makes local deployment both fast and straightforward. Once served, you can connect them to OpenWebUI to get a clean, interactive workspace where the model walks through problems step by step. All of this runs entirely on your machine, no need for heavy cloud infrastructure, which makes it ideal for testing, experimenting, or building offline workflows. In the next section, we will cover the exact steps to set up Falcon reasoning models for maximum local performance.\nLearn more:\nFalcon H1R Release Blog Post Falcon H1R Paper Falcon H1 Tiny Release Blog Post Mamba Paper Mamba2 Paper MLX Framework Overview llama.cpp Github Repository Open-Web UI Model Variants The Falcon reasoning series brings together two main product lines, Falcon H1R and Falcon H1 Tiny R, each engineered to balance reasoning capability with deployment flexibility across a range of hardware environments. All models in this series are open source under the Falcon LLM license and are optimized for handling structured problem solving, multi stage analysis, and other logic driven tasks.\nFalcon H1R targets high performance reasoning in a compact form factor. The lineup includes the flagship Falcon H1R 7B, along with a Falcon H1R 7B GGUF quantized edition for llama.cpp. Powered by a hybrid Transformer Mamba architecture, these models combine advanced attention mechanisms with efficient sequence modeling, delivering state of the art reasoning performance while remaining deployable on high end consumer GPUs, Apple Silicon devices, and performance oriented CPU setups.\nFalcon H1 Tiny R comes from a separate track - Falcon-H1-Tiny series which focuses on ultra lightweight deployments. You can refer to the official blogpost for more details about how these models have been trained. The Falcon-H1-Tiny-R series is available in 0.6B and 90M parameter versions, these models are designed to fit scenarios where hardware resources are extremely limited, such as low spec laptops, compact desktops, or embedded devices, yet robust reasoning performance is still needed. They retain the core architectural strengths of the Falcon reasoning design, enabling efficient step by step logic without relying on large scale infrastructure.\nWhether selecting the 7B model for maximum reasoning power or choosing a Tiny R variant for portability, the Falcon reasoning series offers flexible options to match computational budgets and deployment goals. The complete set of available versions can be explored on Hugging Face. GGUF formats are also provided for llama.cpp, with details on quantization options available in Hugging Face GGUF Quantization Types.\nPrerequisites Before configuring Falcon H1 reasoning models on your system, it is essential to verify that the local environment satisfies the minimum specifications necessary for efficient model execution.\nOperating Systems Linux: Recommended for llama.cpp with full CPU/GPU support. macOS: Supported via llama.cpp, with GPU acceleration through Metal, optimized for Apple Silicon. Windows: Supports llama.cpp with CPU execution; GPU acceleration is available through CUDA for compatible NVIDIA graphics cards. Python Utilize a virtual environment such as conda to manage dependencies. While llama.cpp operates natively without Python, optional Python bindings are available through the llama-cpp-python package. Hugging Face Account Active account required: Hugging Face A free account meets most needs. Installation The installation section is identical as the one from Falcon3 tutorial - in case you went through already on the previous tutorial, you can directly move to the download section\nDownload Model Weights from Hugging Face With your environment ready and dependencies installed, the next step is to obtain the model weights for local deployment. For this guide, we’ll work with two reasoning models from TII’s official repositories: Falcon-H1-7B-GGUF and Falcon-H1-Tiny-R-0.6B-GGUF.\nTip: You can also choose from quantized .gguf versions tailored for different bit depths such as Q4 or Q8. Pick a variant that aligns with your hardware’s performance profile. The 0.6B tiny model is ideal for quick tests or resource-limited systems, while the 7B model delivers stronger reasoning capabilities for more demanding tasks. Finding the best quantization scheme often relies on multiple testing with your local setup and usecase.\nWe’ll use the hf CLI tool to download model weights. Ensure it is installed and authenticated with your Hugging Face account:\nhf auth whoami If authentication is not set up, follow the steps in the CLI guide.\nDownload Falcon H1R 7B (.gguf format)\nhf download tiiuae/Falcon-H1R-7B-GGUF Falcon-H1R-7B-Q4_K_M.gguf --local-dir Falcon-H1R-7B-GGUF Choose a quantization level based on your system’s performance limits.\nDownload Falcon H1-tiny-R-0.6B-GGUF\nhf download tiiuae/Falcon-H1-Tiny-R-0.6B-GGUF Falcon-H1R-0.6B-BF16.gguf --local-dir Falcon-H1-Tiny-R-0.6B-GGUF By default, files are stored in ~/.cache/huggingface/hub. To save them to a different location, use --local-dir with your desired path. Once downloaded, navigate to the target folder to verify that the model weights are present. For example:\n.gguf model (7B reasoning) .gguf model (tiny 0.6B reasoning) Serving Falcon-H1R-7B-GGUF The environment and all requirements for running the model are now fully set up. Next, we’ll walk through the steps for using each framework to serve LLMs directly on your system. We will start with deploying the LLM using the llama.cpp framework.\nRunning the llama.cpp Server First, make sure the server starts successfully and that it is running as expected.\nTo confirm the binary works, run:\nllama-server --help This will display a list of available options. Some will be explained here, while others you can explore on your own. For now, the key options to note are:\n-m, --model FNAME: Path to the model (default: models/$filename taken from --hf-file or --model-url if set, otherwise models/7B/ggml-model-f16.gguf). Environment variable: LLAMA_ARG_MODEL. --host HOST: IP address for the server to listen on (default: 127.0.0.1). Environment variable: LLAMA_ARG_HOST. --port PORT: Port for the server to listen on (default: 8080). Environment variable: LLAMA_ARG_PORT. To run the server with a quantized Falcon-H1R GGUF model, execute:\nllama-server -m ./Falcon-H1R-7B-GGUF/Falcon-H1R-7B-Q4_K_M.gguf --host 0.0.0.0 --port 5000 You should see similar output after running this command:\nServer Settings in llama.cpp The llama-server includes a Web UI accessible via the API above. It shows a small set of configuration options focused on LLM sampling. For the full list, run:\nllama-server --help These options can affect both how your model behaves and the speed of text generation. Many of them can be set via environment variables, which are consistent across other llama.cpp executables.\nClick to view key general parameters --threads/--threads-batch (LLAMA_ARG_THREADS): Number of CPU threads to use. Default -1 automatically detects available cores. --ctx-size (LLAMA_ARG_CTX_SIZE): The model’s context size (how many tokens it can remember). Larger sizes require more memory. --predict (LLAMA_ARG_N_PREDICT): Maximum tokens to generate. Default -1 means continuous generation. --batch-size/--ubatch-size (LLAMA_ARG_BATCH/LLAMA_ARG_UBATCH): Number of tokens processed per step. --flash-attn (LLAMA_ARG_FLASH_ATTN): Enables flash attention optimization for supported models. --mlock (LLAMA_ARG_MLOCK): Keeps the model in memory to avoid swapping. --no-mmap (LLAMA_ARG_NO_MMAP): Disables memory mapping. --gpu-layers (LLAMA_ARG_N_GPU_LAYERS): Number of layers to offload to the GPU (requires GPU-enabled build). Click to view server-specific parameters --no-context-shift (LLAMA_ARG_NO_CONTEXT_SHIFT): Stops generation when the context is full instead of discarding old tokens. --cont-batching (LLAMA_ARG_CONT_BATCHING): Allows prompts to be processed in parallel with generation. --alias (LLAMA_ARG_ALIAS): Sets a model name alias for the REST API. --slots (LLAMA_ARG_ENDPOINT_SLOTS): Enables the /slots endpoint. --props (LLAMA_ARG_ENDPOINT_PROPS): Enables the /props endpoint. Serving Falcon-H1-Tiny-R-0.6B-GGUF And now, if your choice is the Tiny 0.6B model, we will move on to serving it on your machine. Basically, we will still use the llama-server command just like we did with Falcon H1R 7B GGUF. You can adjust the configuration parameters in the command to suit your own usage needs.\nFirst, let’s serve this model using the command line below:\nllama-server -m ./Falcon-H1-Tiny-R-0.6B-GGUF/Falcon-H1R-0.6B-BF16.gguf -sp --host 0.0.0.0 --port 5000 Note: Pay attention to the -sp (or --special) flag at the end of the command. This flag ensures a clear separation between thinking traces and the final answer. You must include it to prevent the model from returning both in the same response.\nTo confirm that the model has been served successfully, check the terminal logs and make sure you see output similar to the screenshot below:\nIf no other process is bound to your chosen port, you can freely set them to match your preference.\nFor example, the server was run on 0.0.0.0 with a custom port 5000.\nOnce it starts, an OpenAI compatible API will be available locally at:\nhttp://:5000 Note: IP 0.0.0.0 and port 5000 were chosen instead of the default configuration of localhost and 8080 to allow public access. You can use the same setup, as it is flexible when running the model on a compact machine and connecting to another device, for example using the model hosted on one laptop from another laptop. When using this configuration, you will need to replace 0.0.0.0 with the actual IP address of your machine. If everything is running on the same machine, you can simply keep the default configuration.\nCongratulations! If you’ve chosen the Falcon H1 reasoning models, you have now successfully served it on your laptop and it’s ready to use. Next, let’s look at integrating the model into OpenWebUI for chatting.\nIntegrating with OpenwebUI Once your selected serving framework is running, the next step is to connect Falcon-H1 to Open WebUI, giving you a simple chat-based interface for interacting with the model.\nStarting Open WebUI First, let’s proceed with installing open-webui cli. Use the command below to install:\npip install open-webui Note: You should install open-webui cli within your existing conda virtual environment. If you are using MLX to serve local llm and already have a conda environment set up, install open-webui cli in that same one. Then activate your environment to proceed.\nAfter successful installation, let’s launch the server.\nopen-webui serve Default access: http://localhost:8080 Custom port: open-webui serve --port 3000 --host 0.0.0.0 Connecting llama.cpp to Open WebUI Open Open WebUI in your browser. Navigate to ⚙️ Admin Settings → Connections → OpenAI. Click Add Connection. Under Standard / Compatible (if tabs are visible), configure the following: URL: http://:5000/v1. API Key: Leave blank or enter a specific key if configured. Once the connection is saved, Open WebUI will use your local llama.cpp server or mlx server as its backend.\nKeep the following points in mind when configuring Open WebUI for the Falcon H1 reasoning models:\nThe reasoning model outputs thinking traces in addition to the final answer. Therefore, you need to set up the thinking tag so Open WebUI can detect, extract, and display the thinking content before showing the answer. By default, these thinking traces are enclosed in tags, which is the standard tag Open WebUI uses to retrieve this information. Fortunately, Falcon H1 reasoning models follow this convention, so you only need to enable thinking mode. Open WebUI offers multiple ways to configure thinking mode, which you can explore in link. If you plan to run multiple models on Open WebUI simultaneously using different ports, it’s best to set up a dedicated configuration for Falcon H1 reasoning models. Go to ⚙️ Admin Settings -\u003e Models Click Edit. You should see the model configuration section as shown below. In Advanced Params, enable Reasoning Tags. Alright, let’s head back to the main screen and try out a few questions to test the model’s intelligence. Now we’ll start with the Falcon-H1-R-7B model, followed by the Falcon-H1-Tiny-R-0.6B model Congratulations — you now have your very own chatbot powered by the Falcon models running right on your laptop.\nConclusion This guide has walked through the process of running Falcon H1 Reasoning models locally, covering both the MLX framework for macOS-optimized deployments and the llama.cpp framework for GGUF-based execution, along with integrating the setup into Open WebUI for an interactive chat experience. We have covered system prerequisites, environment preparation, model weight downloads, server configuration, integration steps, and important considerations before going live.\nBy following these steps, you can deploy Falcon H1 Reasoning models in a way that aligns with your hardware resources and performance requirements. MLX provides seamless integration with Apple’s hardware for high efficiency, while llama.cpp offers broad compatibility and portability for GGUF models, giving you flexible options depending on your use case.\nYou can further explore additional Falcon H1 Reasoning variants on Hugging Face, test different quantization levels, or embed the models into custom workflows and interfaces. With the right configuration, Falcon H1 Reasoning models can become a powerful, adaptable component of your local AI development toolkit.\n","wordCount":"2252","inLanguage":"en","image":"https://falcon-lm.github.io/tutorials/falcon-h1r/images/overview.png","datePublished":"2025-02-24T12:00:00Z","dateModified":"2025-02-24T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/tutorials/falcon-h1r/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/tutorials/ title=Tutorials><span>Tutorials</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:url(./images/overview.png)50%/cover no-repeat fixed"></div><div class=hero-gradient></div><div class=hero-blur></div><div class="hero text-light"><h1 class=post-title>Falcon H1 Reasoning Models Local Deployment Guide: llama.cpp and OpenWebUI</h1><div class=post-meta><span title='2025-02-24 12:00:00 +0000 UTC'>February 24, 2025</span>&nbsp;•&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><figure class=entry-cover><a href=./images/overview.png target=_blank rel="noopener noreferrer"><img loading=lazy src=./images/overview.png alt></a></figure><div class=post-content><style>table{border-collapse:collapse;width:100%;background-color:transparent;border-radius:8px;overflow:hidden;color:inherit}th,td{padding:12px 16px;border:1px solid;border-color:rgba(0,0,0,.3)}@media(prefers-color-scheme:dark){th,td{border-color:rgba(255,255,255,.2)}tr:nth-child(even) td{background-color:rgba(255,255,255,5%)}tr:hover td{background-color:rgba(255,255,255,.1)}}@media(prefers-color-scheme:light){tr:nth-child(even) td{background-color:rgba(0,0,0,5%)}tr:hover td{background-color:rgba(0,0,0,.1)}}th{font-weight:700}blockquote{border-left:4px solid rgba(128,128,128,.4);margin:1em 0;padding:.5em 1em;background-color:transparent;color:inherit;font-family:-apple-system,BlinkMacSystemFont,segoe ui,Roboto,helvetica neue,Arial,sans-serif;font-size:.9em;font-style:italic}blockquote p{margin:0}blockquote p strong{font-weight:700;font-style:italic}blockquote p::before{content:"“"}blockquote p::after{content:"”"}ul+p strong:first-child,ul+p:has(strong:first-child){display:block;margin-top:1.5em;margin-bottom:.5em}p strong:only-child{display:inline-block;margin-top:1em;margin-bottom:.5em}</style><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>Not every task is about retrieving facts, sometimes you need the model to think, weigh options, and walk through a problem in multiple steps. That is where the new Falcon reasoning models come in. Built to deliver structured and logical outputs, they extend the capabilities of the Falcon family into domains like problem solving, mathematics, code logic, and multi stage decision making, while still being lightweight enough to run on laptops or compact devices. We have already gone through the series guiding you on how to work with the instruct model versions, including <a href=http://localhost:1313/tutorials/falcon-h1/>Falcon H1 Instruct</a> and <a href=http://localhost:1313/tutorials/falcon-3/>Falcon3 Instruct</a> , and now it is time to start exploring reasoning models to see how they change the way we approach complex workloads.</p><p>The lineup includes the Falcon H1R 7B, offering the highest reasoning capacity in the series, and two ultra portable versions, Falcon H1 Tiny R 0.6B and Falcon H1 Tiny R 90M, designed for situations where resources are limited but intelligent analysis is still essential.</p><p><strong>What sets Falcon reasoning models apart</strong></p><ol><li>Purpose built for step by step reasoning instead of purely inductive response generation.</li><li>Optimized efficiency to run on consumer GPUs, Apple Silicon, and even low spec CPUs for mobile and embedded scenarios.</li><li>Multiple size tiers, 7B, 0.6B, 90M to fit everything from full desktop deployments to ultra minimal setups.</li><li>Robust in handling structured problem solving, whether it is complex math, long chain of thought analysis, or rule driven workflows.</li></ol><p><strong>Getting started</strong></p><p>These models are provided in gguf format for llama.cpp, which makes local deployment both fast and straightforward. Once served, you can connect them to OpenWebUI to get a clean, interactive workspace where the model walks through problems step by step. All of this runs entirely on your machine, no need for heavy cloud infrastructure, which makes it ideal for testing, experimenting, or building offline workflows. In the next section, we will cover the exact steps to set up Falcon reasoning models for maximum local performance.</p><p>Learn more:</p><ul><li><a href=https://falcon-lm.github.io/blog/falcon-h1r-7b/>Falcon H1R Release Blog Post</a></li><li><a href=https://huggingface.co/papers/2601.02346>Falcon H1R Paper</a></li><li><a href=https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost>Falcon H1 Tiny Release Blog Post</a></li><li><a href=https://arxiv.org/abs/2405.21060>Mamba Paper</a></li><li><a href=https://arxiv.org/abs/2312.00752>Mamba2 Paper</a></li><li><a href=https://mlx-framework.org/#features>MLX Framework Overview</a></li><li><a href=https://github.com/ggml-org/llama.cpp>llama.cpp Github Repository</a></li><li><a href=https://openwebui.com>Open-Web UI</a></li></ul><h2 id=model-variants>Model Variants<a hidden class=anchor aria-hidden=true href=#model-variants>#</a></h2><p>The Falcon reasoning series brings together two main product lines, <strong>Falcon H1R</strong> and <strong>Falcon H1 Tiny R</strong>, each engineered to balance reasoning capability with deployment flexibility across a range of hardware environments. All models in this series are open source under the Falcon LLM license and are optimized for handling structured problem solving, multi stage analysis, and other logic driven tasks.</p><p>Falcon H1R targets high performance reasoning in a compact form factor. The lineup includes the flagship Falcon H1R 7B, along with a Falcon H1R 7B GGUF quantized edition for llama.cpp. Powered by a hybrid Transformer Mamba architecture, these models combine advanced attention mechanisms with efficient sequence modeling, delivering state of the art reasoning performance while remaining deployable on high end consumer GPUs, Apple Silicon devices, and performance oriented CPU setups.</p><p>Falcon H1 Tiny R comes from a separate track - <a href=https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost>Falcon-H1-Tiny series</a> which focuses on ultra lightweight deployments. You can refer to the official blogpost for more details about how these models have been trained. The Falcon-H1-Tiny-R series is available in <strong>0.6B</strong> and <strong>90M</strong> parameter versions, these models are designed to fit scenarios where hardware resources are extremely limited, such as low spec laptops, compact desktops, or embedded devices, yet robust reasoning performance is still needed. They retain the core architectural strengths of the Falcon reasoning design, enabling efficient step by step logic without relying on large scale infrastructure.</p><p>Whether selecting the 7B model for maximum reasoning power or choosing a Tiny R variant for portability, the Falcon reasoning series offers flexible options to match computational budgets and deployment goals. The complete set of available versions can be explored on <a href=https://huggingface.co/tiiuae>Hugging Face</a>. GGUF formats are also provided for llama.cpp, with details on quantization options available in <a href=https://huggingface.co/docs/hub/gguf#quantization-types>Hugging Face GGUF Quantization Types</a>.</p><h2 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h2><p>Before configuring Falcon H1 reasoning models on your system, it is essential to verify that the local environment satisfies the minimum specifications necessary for efficient model execution.</p><h3 id=operating-systems>Operating Systems<a hidden class=anchor aria-hidden=true href=#operating-systems>#</a></h3><ul><li>Linux: Recommended for llama.cpp with full CPU/GPU support.</li><li>macOS: Supported via llama.cpp, with GPU acceleration through Metal, optimized for Apple Silicon.</li><li>Windows: Supports llama.cpp with CPU execution; GPU acceleration is available through CUDA for compatible NVIDIA graphics cards.</li></ul><h3 id=python>Python<a hidden class=anchor aria-hidden=true href=#python>#</a></h3><ul><li>Utilize a virtual environment such as conda to manage dependencies.</li><li>While llama.cpp operates natively without Python, optional Python bindings are available through the <a href=https://github.com/abetlen/llama-cpp-python>llama-cpp-python</a> package.</li></ul><h3 id=hugging-face-account>Hugging Face Account<a hidden class=anchor aria-hidden=true href=#hugging-face-account>#</a></h3><ul><li>Active account required: <a href=https://huggingface.co/>Hugging Face</a></li><li>A free account meets most needs.</li></ul><h2 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h2><p>The installation section is identical as the one from <a href=http://localhost:1313/tutorials/falcon-3/>Falcon3 tutorial</a> - in case you went through already on the previous tutorial, you can directly move to the download section</p><h2 id=download-model-weights-from-hugging-face>Download Model Weights from Hugging Face<a hidden class=anchor aria-hidden=true href=#download-model-weights-from-hugging-face>#</a></h2><p>With your environment ready and dependencies installed, the next step is to obtain the model weights for local deployment. For this guide, we’ll work with two reasoning models from TII’s official repositories: <a href=https://huggingface.co/tiiuae/Falcon-H1R-7B-GGUF/>Falcon-H1-7B-GGUF</a> and <a href=https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-0.6B-GGUF/>Falcon-H1-Tiny-R-0.6B-GGUF</a>.</p><blockquote><p><strong>Tip:</strong> You can also choose from quantized <code>.gguf</code> versions tailored for different bit depths such as Q4 or Q8. Pick a variant that aligns with your hardware’s performance profile. The 0.6B tiny model is ideal for quick tests or resource-limited systems, while the 7B model delivers stronger reasoning capabilities for more demanding tasks. Finding the best quantization scheme often relies on multiple testing with your local setup and usecase.</p></blockquote><p>We’ll use the <strong>hf</strong> CLI tool to download model weights. Ensure it is installed and authenticated with your Hugging Face account:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hf auth whoami
</span></span></code></pre></div><p>If authentication is not set up, follow the steps in <a href=https://huggingface.co/docs/huggingface_hub/guides/cli>the CLI guide</a>.</p><p><strong>Download Falcon H1R 7B (.gguf format)</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hf download tiiuae/Falcon-H1R-7B-GGUF Falcon-H1R-7B-Q4_K_M.gguf --local-dir Falcon-H1R-7B-GGUF
</span></span></code></pre></div><blockquote><p>Choose a quantization level based on your system’s performance limits.</p></blockquote><p><strong>Download Falcon H1-tiny-R-0.6B-GGUF</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hf download tiiuae/Falcon-H1-Tiny-R-0.6B-GGUF Falcon-H1R-0.6B-BF16.gguf --local-dir Falcon-H1-Tiny-R-0.6B-GGUF
</span></span></code></pre></div><p>By default, files are stored in <code>~/.cache/huggingface/hub</code>. To save them to a different location, use <code>--local-dir</code> with your desired path. Once downloaded, navigate to the target folder to verify that the model weights are present. For example:</p><ul><li><code>.gguf</code> model (7B reasoning)</li></ul><img src=./images/model-h1r-7b.png alt=Falcon-H1R-7B-GGUF style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><ul><li><code>.gguf</code> model (tiny 0.6B reasoning)</li></ul><img src=./images/model-h1-tiny-r-0.6b.png alt=Falcon-H1-Tiny-R-0.6B style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><h2 id=serving-falcon-h1r-7b-gguf>Serving Falcon-H1R-7B-GGUF<a hidden class=anchor aria-hidden=true href=#serving-falcon-h1r-7b-gguf>#</a></h2><p>The environment and all requirements for running the model are now fully set up. Next, we’ll walk through the steps for using each framework to serve LLMs directly on your system.
We will start with deploying the LLM using the <strong>llama.cpp</strong> framework.</p><h3 id=running-the-llamacpp-server>Running the llama.cpp Server<a hidden class=anchor aria-hidden=true href=#running-the-llamacpp-server>#</a></h3><p>First, make sure the server starts successfully and that it is running as expected.<br>To confirm the binary works, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>llama-server --help
</span></span></code></pre></div><p>This will display a list of available options. Some will be explained here, while others you can explore on your own. For now, the key options to note are:</p><ul><li><code>-m, --model FNAME</code>: Path to the model (default: <code>models/$filename</code> taken from <code>--hf-file</code> or <code>--model-url</code> if set, otherwise <code>models/7B/ggml-model-f16.gguf</code>). Environment variable: <code>LLAMA_ARG_MODEL</code>.</li><li><code>--host HOST</code>: IP address for the server to listen on (default: <code>127.0.0.1</code>). Environment variable: <code>LLAMA_ARG_HOST</code>.</li><li><code>--port PORT</code>: Port for the server to listen on (default: <code>8080</code>). Environment variable: <code>LLAMA_ARG_PORT</code>.</li></ul><p>To run the server with a quantized Falcon-H1R GGUF model, execute:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>llama-server -m ./Falcon-H1R-7B-GGUF/Falcon-H1R-7B-Q4_K_M.gguf --host 0.0.0.0 --port <span class=m>5000</span>
</span></span></code></pre></div><p>You should see similar output after running this command:</p><img src=./images/serving-llama.png alt=llama-server-output style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><h3 id=server-settings-in-llamacpp>Server Settings in llama.cpp<a hidden class=anchor aria-hidden=true href=#server-settings-in-llamacpp>#</a></h3><p>The <code>llama-server</code> includes a Web UI accessible via the API above. It shows a small set of configuration options focused on LLM sampling. For the full list, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>llama-server --help
</span></span></code></pre></div><p>These options can affect both how your model behaves and the speed of text generation. Many of them can be set via environment variables, which are consistent across other llama.cpp executables.</p><details><summary><strong>Click to view key general parameters</strong></summary><ul><li><code>--threads/--threads-batch (LLAMA_ARG_THREADS)</code>: Number of CPU threads to use. Default <code>-1</code> automatically detects available cores.</li><li><code>--ctx-size (LLAMA_ARG_CTX_SIZE)</code>: The model’s context size (how many tokens it can remember). Larger sizes require more memory.</li><li><code>--predict (LLAMA_ARG_N_PREDICT)</code>: Maximum tokens to generate. Default <code>-1</code> means continuous generation.</li><li><code>--batch-size/--ubatch-size (LLAMA_ARG_BATCH/LLAMA_ARG_UBATCH)</code>: Number of tokens processed per step.</li><li><code>--flash-attn (LLAMA_ARG_FLASH_ATTN)</code>: Enables flash attention optimization for supported models.</li><li><code>--mlock (LLAMA_ARG_MLOCK)</code>: Keeps the model in memory to avoid swapping.</li><li><code>--no-mmap (LLAMA_ARG_NO_MMAP)</code>: Disables memory mapping.</li><li><code>--gpu-layers (LLAMA_ARG_N_GPU_LAYERS)</code>: Number of layers to offload to the GPU (requires GPU-enabled build).</li></ul></details><details><summary><strong>Click to view server-specific parameters</strong></summary><ul><li><code>--no-context-shift (LLAMA_ARG_NO_CONTEXT_SHIFT)</code>: Stops generation when the context is full instead of discarding old tokens.</li><li><code>--cont-batching (LLAMA_ARG_CONT_BATCHING)</code>: Allows prompts to be processed in parallel with generation.</li><li><code>--alias (LLAMA_ARG_ALIAS)</code>: Sets a model name alias for the REST API.</li><li><code>--slots (LLAMA_ARG_ENDPOINT_SLOTS)</code>: Enables the <code>/slots</code> endpoint.</li><li><code>--props (LLAMA_ARG_ENDPOINT_PROPS)</code>: Enables the <code>/props</code> endpoint.</li></ul></details><h2 id=serving-falcon-h1-tiny-r-06b-gguf>Serving Falcon-H1-Tiny-R-0.6B-GGUF<a hidden class=anchor aria-hidden=true href=#serving-falcon-h1-tiny-r-06b-gguf>#</a></h2><p>And now, if your choice is the Tiny 0.6B model, we will move on to serving it on your machine. Basically, we will still use the <code>llama-server</code> command just like we did with <a href=/tutorials/falcon-h1r/#serving-falcon-h1r-7b-gguf>Falcon H1R 7B GGUF</a>. You can adjust the configuration parameters in the command to suit your own usage needs.</p><p>First, let’s serve this model using the command line below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>llama-server -m ./Falcon-H1-Tiny-R-0.6B-GGUF/Falcon-H1R-0.6B-BF16.gguf -sp --host 0.0.0.0 --port <span class=m>5000</span>
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> Pay attention to the <code>-sp</code> (or <code>--special</code>) flag at the end of the command. This flag ensures a clear separation between thinking traces and the final answer. You must include it to prevent the model from returning both in the same response.</p></blockquote><p>To confirm that the model has been served successfully, check the terminal logs and make sure you see output similar to the screenshot below:<br><img src=./images/serving-llama-2.png alt=llama-server-output style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"></p><p>If no other process is bound to your chosen port, you can freely set them to match your preference.<br>For example, the server was run on <code>0.0.0.0</code> with a custom port <code>5000</code>.<br>Once it starts, an OpenAI compatible API will be available locally at:</p><pre tabindex=0><code>http://&lt;your_device_ip_address&gt;:5000
</code></pre><blockquote><p><strong>Note:</strong> IP <code>0.0.0.0</code> and port <code>5000</code> were chosen instead of the default configuration of <code>localhost</code> and <code>8080</code> to allow public access. You can use the same setup, as it is flexible when running the model on a compact machine and connecting to another device, for example using the model hosted on one laptop from another laptop. When using this configuration, you will need to replace <code>0.0.0.0</code> with the actual IP address of your machine. If everything is running on the same machine, you can simply keep the default configuration.</p></blockquote><p>Congratulations! If you’ve chosen the Falcon H1 reasoning models, you have now successfully served it on your laptop and it’s ready to use. Next, let’s look at integrating the model into OpenWebUI for chatting.</p><h2 id=integrating-with-openwebui>Integrating with OpenwebUI<a hidden class=anchor aria-hidden=true href=#integrating-with-openwebui>#</a></h2><p>Once your selected serving framework is running, the next step is to connect Falcon-H1 to Open WebUI, giving you a simple chat-based interface for interacting with the model.</p><h3 id=starting-open-webui>Starting Open WebUI<a hidden class=anchor aria-hidden=true href=#starting-open-webui>#</a></h3><p>First, let&rsquo;s proceed with installing open-webui cli. Use the command below to install:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install open-webui
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> You should install open-webui cli within your existing conda virtual environment. If you are using MLX to serve local llm and already have a conda environment set up, install open-webui cli in that same one. Then activate your environment to proceed.</p></blockquote><p>After successful installation, let&rsquo;s launch the server.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>open-webui serve
</span></span></code></pre></div><ul><li>Default access: <a href=http://localhost:8080>http://localhost:8080</a></li><li>Custom port:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>open-webui serve --port <span class=m>3000</span> --host 0.0.0.0
</span></span></code></pre></div><h3 id=connecting-llamacpp-to-open-webui>Connecting llama.cpp to Open WebUI<a hidden class=anchor aria-hidden=true href=#connecting-llamacpp-to-open-webui>#</a></h3><ol><li>Open <strong>Open WebUI</strong> in your browser.</li><li>Navigate to <strong>⚙️ Admin Settings → Connections → OpenAI</strong>.</li><li>Click <strong>Add Connection</strong>.</li><li>Under <strong>Standard / Compatible</strong> (if tabs are visible), configure the following:<ul><li><strong>URL:</strong> <code>http://&lt;your_device_ip_address>:5000/v1</code>.</li><li><strong>API Key:</strong> Leave blank or enter a specific key if configured.</li></ul></li></ol><p>Once the connection is saved, Open WebUI will use your local <strong>llama.cpp</strong> server or <strong>mlx</strong> server as its backend.</p><p>Keep the following points in mind when configuring Open WebUI for the Falcon H1 reasoning models:</p><ul><li>The reasoning model outputs <em>thinking traces</em> in addition to the final answer. Therefore, you need to set up the thinking tag so Open WebUI can detect, extract, and display the thinking content before showing the answer. By default, these thinking traces are enclosed in <code>&lt;think>&lt;/think></code> tags, which is the standard tag Open WebUI uses to retrieve this information. Fortunately, Falcon H1 reasoning models follow this convention, so you only need to enable <strong>thinking mode</strong>.</li><li>Open WebUI offers multiple ways to configure thinking mode, which you can explore in <a href=https://docs.openwebui.com/features/chat-features/reasoning-models/>link</a>. If you plan to run multiple models on Open WebUI simultaneously using different ports, it’s best to set up a dedicated configuration for Falcon H1 reasoning models.</li></ul><ol><li>Go to <strong>⚙️ Admin Settings -> Models</strong></li><li>Click <strong>Edit</strong>. You should see the model configuration section as shown below.</li><li>In <strong>Advanced Params</strong>, enable <strong>Reasoning Tags</strong>.</li></ol><p>Alright, let’s head back to the main screen and try out a few questions to test the model’s intelligence. Now we’ll start with the Falcon-H1-R-7B model,
<img src=./images/demo-h1r.png alt=falcon-h1-r style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"></p><p>followed by the Falcon-H1-Tiny-R-0.6B model
<img src=./images/demo-h1-tiny.png alt=falcon-h1-r style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"></p><p>Congratulations — you now have your very own chatbot powered by the Falcon models running right on your laptop.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This guide has walked through the process of running Falcon H1 Reasoning models locally, covering both the MLX framework for macOS-optimized deployments and the llama.cpp framework for GGUF-based execution, along with integrating the setup into Open WebUI for an interactive chat experience. We have covered system prerequisites, environment preparation, model weight downloads, server configuration, integration steps, and important considerations before going live.</p><p>By following these steps, you can deploy Falcon H1 Reasoning models in a way that aligns with your hardware resources and performance requirements. MLX provides seamless integration with Apple’s hardware for high efficiency, while llama.cpp offers broad compatibility and portability for GGUF models, giving you flexible options depending on your use case.</p><p>You can further explore additional Falcon H1 Reasoning variants on Hugging Face, test different quantization levels, or embed the models into custom workflows and interfaces. With the right configuration, Falcon H1 Reasoning models can become a powerful, adaptable component of your local AI development toolkit.</p><div class=post-contributors><div class=contributors-section><h4>Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/FalconLLM.webp alt="Falcon LLM team" class=contributor-image><p class=contributor-name>Falcon LLM team</p></div></div></div></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span><a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html rel="noopener noreferrer" target=_blank>| Terms and Conditions</a>
</span><span><a href=https://www.tii.ae/privacy-policy rel="noopener noreferrer" target=_blank>| Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>