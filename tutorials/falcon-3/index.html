<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script>!function(e,t){if(!e.rdt){var s,o,n=e.rdt=function(){n.sendEvent?n.sendEvent.apply(n,arguments):n.callQueue.push(arguments)};n.callQueue=[],s=t.createElement("script"),s.src="https://www.redditstatic.com/ads/pixel.js",s.async=!0,o=t.getElementsByTagName("script")[0],o.parentNode.insertBefore(s,o)}}(window,document),rdt("init","a2_f0we4sxffrnm"),rdt("track","PageVisit")</script><script async src="https://www.googletagmanager.com/gtag/js?id=AW-16575356988"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","AW-16575356988")</script><title>Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI | Falcon</title>
<meta name=keywords content><meta name=description content="
Overview
If you have ever wanted to run a powerful AI model directly on your laptop for quick experiments or personal projects, Falcon 3 makes it simple. You do not need expensive servers or constant internet access, just your device and the model running locally. It is built for speed, lightweight setup, and customization so you can focus on creating, not configuring."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/tutorials/falcon-3/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/tutorials/falcon-3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI"><meta property="og:description" content="
Overview
If you have ever wanted to run a powerful AI model directly on your laptop for quick experiments or personal projects, Falcon 3 makes it simple. You do not need expensive servers or constant internet access, just your device and the model running locally. It is built for speed, lightweight setup, and customization so you can focus on creating, not configuring."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/tutorials/falcon-3/"><meta property="og:image" content="https://falcon-lm.github.io/tutorials/falcon-3/images/falcon3-family-logo.svg"><meta property="article:section" content="tutorials"><meta property="article:published_time" content="2025-02-24T12:00:00+00:00"><meta property="article:modified_time" content="2025-02-24T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/tutorials/falcon-3/images/falcon3-family-logo.svg"><meta name=twitter:title content="Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI"><meta name=twitter:description content="
Overview
If you have ever wanted to run a powerful AI model directly on your laptop for quick experiments or personal projects, Falcon 3 makes it simple. You do not need expensive servers or constant internet access, just your device and the model running locally. It is built for speed, lightweight setup, and customization so you can focus on creating, not configuring."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Tutorials","item":"https://falcon-lm.github.io/tutorials/"},{"@type":"ListItem","position":2,"name":"Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI","item":"https://falcon-lm.github.io/tutorials/falcon-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI","name":"Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI","description":" Overview If you have ever wanted to run a powerful AI model directly on your laptop for quick experiments or personal projects, Falcon 3 makes it simple. You do not need expensive servers or constant internet access, just your device and the model running locally. It is built for speed, lightweight setup, and customization so you can focus on creating, not configuring.\n","keywords":[],"articleBody":" Overview If you have ever wanted to run a powerful AI model directly on your laptop for quick experiments or personal projects, Falcon 3 makes it simple. You do not need expensive servers or constant internet access, just your device and the model running locally. It is built for speed, lightweight setup, and customization so you can focus on creating, not configuring.\nWhy Falcon 3 is a great fit for you\nModel sizes from 1B to 10B parameters so you can pick what works best for your hardware. The model implements a decoder only architecture with Grouped Query Attention mechanism, enabling efficient processing and resource optimization. Handles up to 32K context tokens for richer responses (8K for the smaller 1B model). Two flavors: base for general tasks and instruct for friendly conversational AI. How to get started Falcon 3 can be deployed on Apple Silicon using MLX or on CPU/GPU laptops with llama.cpp in gguf format. When integrated with OpenWebUI, it provides a streamlined conversational interface for immediate interaction. This guide will walk you through the detailed steps for implementation.\nLearn more:\nFalcon 3 Release Blog Post MLX Framework Overview llama.cpp Github Repository Open-Web UI Model Variants The Falcon 3 series offers a diverse range of open-sourced text-only models, including base, instruct, with sizes ranging from 1B to 10B. You can find more details at this link. All variants are distributed under the permissive Falcon-LLM license.\nThe smaller 1B model is especially well-suited for on-device deployment on personal computers, laptops, and other compact hardware platforms. Conversely, the larger models, such as 3B, 7B and 10B, deliver superior computational performance, albeit necessitating higher computational resources. The complete range of available versions can be explored on Hugging Face.\nThe collection also includes GGUF formats, for further information on the gguf quantization formats, please refer to Hugging Face GGUF Quantization Types\nPrerequisites Before start deploying Falcon 3 on your system, it is essential to verify that the local environment satisfies the minimum specifications necessary for efficient model execution.\nOperating Systems Linux: Recommended for llama.cpp with full CPU/GPU support. macOS: Supported via MLX and llama.cpp, with GPU acceleration through Metal, optimized for Apple Silicon. Windows: Supports llama.cpp with CPU execution; GPU acceleration is available through CUDA for compatible NVIDIA graphics cards. Python Utilize a virtual environment such as conda to manage dependencies. While llama.cpp operates natively without Python, optional Python bindings are available through the llama-cpp-python package. Hugging Face Account Active account required: Hugging Face A free account meets most needs. Installation With the minimum requirements verified, we can proceed to configure the local environment and install the essential tools together with all required dependencies. We will begin by installing the fundamental libraries.\nInstalling fundamental libraries When utilizing the llama.cpp framework for model deployment, this section presents instructions for installing llama.cpp on macOS, Windows, and Linux. Various package managers can be employed for the installation process, and the following provides installation support details corresponding to each package manager for the respective operating systems.\nInstall via Windows Mac Linux Winget ✅ Homebrew ✅ ✅ MacPorts ✅ Nix ✅ ✅ Note: Ensure that the package managers are successfully installed in your operating system environment.\nWinget (Windows) winget install llama.cpp Homebrew (Mac and Linux) Note: The commands apply equally to both macOS and Linux.\nbrew install llama.cpp MacPorts (Mac) sudo port install llama.cpp See also: MacPorts llama.cpp details\nNix (Mac and Linux) Note: The commands apply equally to both macOS and Linux.\nFor flake-enabled installs nix profile install nixpkgs#llama-cpp For non-flake-enabled installs nix-env --file '' --install --attr llama-cpp Note: This expression is automatically updated within the nixpkgs repo.\nAfter installing llama.cpp, verify the installation by running the following command to ensure the framework is correctly set up in your local environment.\nllama-cli --version The terminal should display output similar to the example shown below.\nCongratulations, the llama.cpp framework has been successfully installed on your system.\nNow, let’s move on to installing MLX in case you choose this framework for serving. For model deployment with MLX, keep in mind that it is only supported on macOS. The next section provides a detailed installation guide. First, create a virtual environment using conda.\nconda create -n mlx python=3.11 conda activate mlx Alright, with the environment activated, install the mlx-lm library.\npip install mlx-lm For more information about the mlx-lm library, visit the Git repository\nTo confirm that the mlx-lm library was installed successfully, you should see a log in the terminal similar to the one shown when running the same command.\nDownload Model Weights from Hugging Face With the environment set up and all necessary tools and dependencies installed, the next step is to download a model. As this guide focuses on the Falcon 1B Instruct model, let’s get the Falcon 3 1B model from the official Hugging Face repository maintained by TII.\nThe Hugging Face collection offers complete model weights in multiple formats, including .safetensors and .gguf. The default format for storing model weights is .safetensors. Keep in mind that llama.cpp can serve models only in the .gguf format, while the MLX framework supports both .gguf and .safetensors.\nNote: Currently, TII provides ready-to-use .gguf and quantized model versions on their official Hugging Face repository. This means you can directly download the model weights to your local machine for use. When working with the llama.cpp framework, you’ll need a .gguf model as mentioned above. Make sure to choose a version that matches your hardware’s minimum requirements. In this example, we’ll experiment with the Falcon 1B Instruct .gguf model. If you’re using the mlx framework, you can use the model weights available on mlx-community Hugging Face organization\nTo download the model weights, we’ll use the hf CLI tool. Ensure that you have it installed on your laptop and that you’re logged in with your Hugging Face account. To verify your installation and authentication, run:\nhf auth whoami If the installation or authentication hasn’t been completed, follow the instructions provided in huggingface link.\nNow, let’s download the .gguf model version by running the following command:\nhf download tiiuae/Falcon3-1B-Instruct-GGUF Falcon3-1B-Instruct-q4_k_m.gguf --local-dir Falcon3-1B-Instruct-GGUF Note: that multiple quantized versions are available, so choose one that matches your hardware capabilities.\nFor demonstrating model serving with MLX, we’ll download the 4-bit quantized model version with this command:\nhf download mlx-community/Falcon3-1B-Instruct-4bit --local-dir Falcon3-1B-Instruct-4bit Note: This script will download the entire Hugging Face repository to your local machine.\nBy default, downloaded models are saved to Hugging Face’s ~/.cache/huggingface/hub folder. However, if you prefer to specify a custom storage location, you can use the --local-dir tag and provide the exact folder path where you want the model saved. Once the downloads are complete, navigate to the folder where you saved the model weights to verify them. You should see output similar to the example shown below:\n.gguf model .safetensors quantized model Serving Falcon-3 The environment and all requirements for running the model are now fully set up. Next, we’ll walk through the steps for using each framework to serve LLMs directly on your system. We will start with deploying the LLM using the llama.cpp framework.\nRunning the llama.cpp Server First, make sure the server starts successfully and that it is running as expected.\nTo confirm the binary works, run:\nllama-server --help This will display a list of available options. Some will be explained here, while others you can explore on your own. For now, the key options to note are:\n-m, --model FNAME: Path to the model (default: models/$filename taken from --hf-file or --model-url if set, otherwise models/7B/ggml-model-f16.gguf). Environment variable: LLAMA_ARG_MODEL. --host HOST: IP address for the server to listen on (default: 127.0.0.1). Environment variable: LLAMA_ARG_HOST. --port PORT: Port for the server to listen on (default: 8080). Environment variable: LLAMA_ARG_PORT. To run the server with a quantized Falcon-3 GGUF model, execute:\nllama-server -m ./Falcon3-1B-Instruct-GGUF/Falcon3-1B-Instruct-q4_k_m.gguf You should see similar output after running this command:\nIf nothing else is running on 127.0.0.1:8080, the default host and port can be kept. You can also use environment variables instead of command-line arguments for convenience.\nWhen the server starts, it will expose a local OpenAI-compatible API at:\nhttp://127.0.0.1:8080 Server Settings in llama.cpp The llama-server includes a Web UI accessible via the API above. It shows a small set of configuration options focused on LLM sampling. For the full list, run:\nllama-server --help These options can affect both how your model behaves and the speed of text generation. Many of them can be set via environment variables, which are consistent across other llama.cpp executables.\nClick to view key general parameters --threads/--threads-batch (LLAMA_ARG_THREADS): Number of CPU threads to use. Default -1 automatically detects available cores. --ctx-size (LLAMA_ARG_CTX_SIZE): The model’s context size (how many tokens it can remember). Larger sizes require more memory. --predict (LLAMA_ARG_N_PREDICT): Maximum tokens to generate. Default -1 means continuous generation. --batch-size/--ubatch-size (LLAMA_ARG_BATCH/LLAMA_ARG_UBATCH) – Number of tokens processed per step. --flash-attn (LLAMA_ARG_FLASH_ATTN): Enables flash attention optimization for supported models. --mlock (LLAMA_ARG_MLOCK): Keeps the model in memory to avoid swapping. --no-mmap (LLAMA_ARG_NO_MMAP): Disables memory mapping. --gpu-layers (LLAMA_ARG_N_GPU_LAYERS): Number of layers to offload to the GPU (requires GPU-enabled build). Click to view server-specific parameters --no-context-shift (LLAMA_ARG_NO_CONTEXT_SHIFT): Stops generation when the context is full instead of discarding old tokens. --cont-batching (LLAMA_ARG_CONT_BATCHING): Allows prompts to be processed in parallel with generation. --alias (LLAMA_ARG_ALIAS): Sets a model name alias for the REST API. --slots (LLAMA_ARG_ENDPOINT_SLOTS): Enables the /slots endpoint. --props (LLAMA_ARG_ENDPOINT_PROPS): Enables the /props endpoint. Deployment with the MLX Framework on macOS You now know how to serve the model on your laptop using llama.cpp. Next, I’ll show you how to use the MLX framework in case you have a MacBook. Another option for serving LLMs is the MLX framework, which is built specifically for macOS and optimized for Apple hardware.\nRunning the MLX Server Start the server with:\nmlx_lm.server --model ./Falcon3-1B-Instruct-4bit You should see similar output after running this command:\nThis starts a text generation service on localhost:8080 using the Falcon 3 1B Instruct model. If the model is not already in your local cache, it will be downloaded from the specified Hugging Face repository. However, I recommend downloading the model to your local system first, as described earlier.\nTo see all available options, run:\nmlx_lm.server --help To make a request to the model:\ncurl localhost:8080/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"temperature\": 0.7 }' Request Fields Click to view all request fields messages: List of message objects with role and content. role_mapping: Optional dictionary to customize role prefixes. stop: Optional stop sequences. max_tokens: Maximum tokens to generate (default: 512). stream: Boolean to stream responses (default: false). temperature: Controls randomness (default: 0.0). top_p: Nucleus sampling parameter (default: 1.0). top_k: Top-K sampling parameter (default: 0). min_p: Minimum probability sampling parameter (default: 0.0). repetition_penalty: Penalty for repeated tokens (default: 1.0). repetition_context_size: Context size for applying repetition penalty (default: 20). logit_bias: Mapping of token IDs to bias values. logprobs: Number of top token probabilities to return (1–10). model: Path or Hugging Face repo for the model. adapters: Path to low-rank adapters. draft_model: Smaller model for speculative decoding. num_draft_tokens: Draft tokens predicted at once (default: 3). Response Fields Click to view all response fields id: Unique chat ID. system_fingerprint: System identifier. object: Response type (chat.completion, chat.completion.chunk, or text.completion). model: Model path or repo name. created: Timestamp of request. choices: Output list with index, optional log probabilities, finish reason, and message text. usage: Token statistics: prompt, completion, and total tokens. Integrating with OpenwebUI Once your selected serving framework is running, the next step is to connect Falcon-3 to Open WebUI, giving you a simple chat-based interface for interacting with the model.\nStarting Open WebUI First, let’s proceed with installing open-webui cli. Use the command below to install:\npip install open-webui Note: You should install open-webui cli within your existing conda virtual environment. If you are using MLX to serve local llm and already have a conda environment set up, install open-webui cli in that same one. Then activate your environment to proceed. After successful installation, let’s launch the server.\nopen-webui serve Default access: http://localhost:8080 Custom port: open-webui serve --port 3000 --host 0.0.0.0 Connecting llama.cpp to Open WebUI Open Open WebUI in your browser. Navigate to Admin Settings → Connections → OpenAI. Click Add Connection. Under Standard / Compatible (if tabs are visible), configure the following: URL: http://127.0.0.1:8080/v1. API Key: Leave blank or enter a specific key if configured. Once the connection is saved, Open WebUI will use your local llama.cpp server or mlx server as its backend.\nAlright, let’s head back to the main screen and try out a few questions to test the model’s intelligence. Congratulations — you now have your very own chatbot powered by the Falcon models running right on your laptop.\nConclusion This guide has outlined the process of deploying Falcon-3 locally using either the MLX framework for macOS-optimized workflows or the llama.cpp framework for GGUF model formats, along with integrating the chosen backend into Open WebUI to provide an interactive chat interface. The topics covered include minimum system requirements, environment setup, downloading model weights, server configuration, integration steps, and key considerations before deployment.\nFollowing these steps allows Falcon-3 to be deployed in a way that matches your hardware capabilities and performance goals, ensuring both efficiency and ease of use. The MLX framework delivers smooth integration with Apple’s hardware ecosystem, while llama.cpp offers portability and compatibility with GGUF models, giving you flexible options for various AI applications.\nFor additional experimentation, you can explore other Falcon-3 variants available on Hugging Face, apply performance optimizations, or incorporate the model into custom workflows and interfaces. With the right setup, Falcon-3 can be a powerful and versatile asset in your local AI development environment.\n","wordCount":"2266","inLanguage":"en","image":"https://falcon-lm.github.io/tutorials/falcon-3/images/falcon3-family-logo.svg","datePublished":"2025-02-24T12:00:00Z","dateModified":"2025-02-24T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/tutorials/falcon-3/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/tutorials/ title=Tutorials><span>Tutorials</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:url(./images/falcon3-family-logo.svg)50%/cover no-repeat fixed"></div><div class=hero-gradient></div><div class=hero-blur></div><div class="hero text-light"><h1 class=post-title>Falcon-3 Local Deployment Guide: MLX, llama.cpp, and OpenWebUI</h1><div class=post-meta><span title='2025-02-24 12:00:00 +0000 UTC'>February 24, 2025</span>&nbsp;•&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><figure class=entry-cover><a href=./images/falcon3-family-logo.svg target=_blank rel="noopener noreferrer"><img loading=lazy src=./images/falcon3-family-logo.svg alt></a></figure><div class=post-content><style>table{border-collapse:collapse;width:100%;background-color:transparent;border-radius:8px;overflow:hidden;color:inherit}th,td{padding:12px 16px;border:1px solid;border-color:rgba(0,0,0,.3)}@media(prefers-color-scheme:dark){th,td{border-color:rgba(255,255,255,.2)}tr:nth-child(even) td{background-color:rgba(255,255,255,5%)}tr:hover td{background-color:rgba(255,255,255,.1)}}@media(prefers-color-scheme:light){tr:nth-child(even) td{background-color:rgba(0,0,0,5%)}tr:hover td{background-color:rgba(0,0,0,.1)}}th{font-weight:700}blockquote{border-left:4px solid rgba(128,128,128,.4);margin:1em 0;padding:.5em 1em;background-color:transparent;color:inherit;font-family:-apple-system,BlinkMacSystemFont,segoe ui,Roboto,helvetica neue,Arial,sans-serif;font-size:.9em;font-style:italic}blockquote p{margin:0}blockquote p strong{font-weight:700;font-style:italic}blockquote p::before{content:"“"}blockquote p::after{content:"”"}ul+p strong:first-child,ul+p:has(strong:first-child){display:block;margin-top:1.5em;margin-bottom:.5em}p strong:only-child{display:inline-block;margin-top:1em;margin-bottom:.5em}</style><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>If you have ever wanted to run a powerful AI model directly on your laptop for quick experiments or personal projects, Falcon 3 makes it simple. You do not need expensive servers or constant internet access, just your device and the model running locally. It is built for speed, lightweight setup, and customization so you can focus on creating, not configuring.</p><p><strong>Why Falcon 3 is a great fit for you</strong></p><ul><li>Model sizes from 1B to 10B parameters so you can pick what works best for your hardware.</li><li>The model implements a decoder only architecture with Grouped Query Attention mechanism, enabling efficient processing and resource optimization.</li><li>Handles up to 32K context tokens for richer responses (8K for the smaller 1B model).</li><li>Two flavors: base for general tasks and instruct for friendly conversational AI.</li></ul><p><strong>How to get started</strong>
Falcon 3 can be deployed on Apple Silicon using MLX or on CPU/GPU laptops with llama.cpp in gguf format. When integrated with OpenWebUI, it provides a streamlined conversational interface for immediate interaction. This guide will walk you through the detailed steps for implementation.</p><p>Learn more:</p><ul><li><a href=https://huggingface.co/blog/falcon3>Falcon 3 Release Blog Post</a></li><li><a href=https://mlx-framework.org/#features>MLX Framework Overview</a></li><li><a href=https://github.com/ggml-org/llama.cpp>llama.cpp Github Repository</a></li><li><a href=https://openwebui.com>Open-Web UI</a></li></ul><h2 id=model-variants>Model Variants<a hidden class=anchor aria-hidden=true href=#model-variants>#</a></h2><p>The Falcon 3 series offers a diverse range of open-sourced text-only models, including base, instruct, with sizes ranging from 1B to 10B. You can find more details at this <a href=https://falconllm.tii.ae/falcon3/index.html>link</a>. All variants are distributed under the permissive Falcon-LLM license.</p><p>The smaller 1B model is especially well-suited for on-device deployment on personal computers, laptops, and other compact hardware platforms. Conversely, the larger models, such as 3B, 7B and 10B, deliver superior computational performance, albeit necessitating higher computational resources. The complete range of available versions can be explored on <a href=https://huggingface.co/collections/tiiuae/falcon3>Hugging Face</a>.</p><p>The collection also includes GGUF formats, for further information on the gguf quantization formats, please refer to <a href=https://huggingface.co/docs/hub/gguf#quantization-types>Hugging Face GGUF Quantization Types</a></p><h2 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h2><p>Before start deploying Falcon 3 on your system, it is essential to verify that the local environment satisfies the minimum specifications necessary for efficient model execution.</p><h3 id=operating-systems>Operating Systems<a hidden class=anchor aria-hidden=true href=#operating-systems>#</a></h3><ul><li>Linux: Recommended for llama.cpp with full CPU/GPU support.</li><li>macOS: Supported via MLX and llama.cpp, with GPU acceleration through Metal, optimized for Apple Silicon.</li><li>Windows: Supports llama.cpp with CPU execution; GPU acceleration is available through CUDA for compatible NVIDIA graphics cards.</li></ul><h3 id=python>Python<a hidden class=anchor aria-hidden=true href=#python>#</a></h3><ul><li>Utilize a virtual environment such as conda to manage dependencies.</li><li>While llama.cpp operates natively without Python, optional Python bindings are available through the <a href=https://github.com/abetlen/llama-cpp-python>llama-cpp-python</a> package.</li></ul><h3 id=hugging-face-account>Hugging Face Account<a hidden class=anchor aria-hidden=true href=#hugging-face-account>#</a></h3><ul><li>Active account required: <a href=https://huggingface.co/>Hugging Face</a></li><li>A free account meets most needs.</li></ul><h2 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h2><p>With the minimum requirements verified, we can proceed to configure the local environment and install the essential tools together with all required dependencies. We will begin by installing the fundamental libraries.</p><h3 id=installing-fundamental-libraries>Installing fundamental libraries<a hidden class=anchor aria-hidden=true href=#installing-fundamental-libraries>#</a></h3><p>When utilizing the llama.cpp framework for model deployment, this section presents instructions for installing llama.cpp on macOS, Windows, and Linux. Various package managers can be employed for the installation process, and the following provides installation support details corresponding to each package manager for the respective operating systems.</p><table><thead><tr><th>Install via</th><th>Windows</th><th>Mac</th><th>Linux</th></tr></thead><tbody><tr><td>Winget</td><td>✅</td><td></td><td></td></tr><tr><td>Homebrew</td><td></td><td>✅</td><td>✅</td></tr><tr><td>MacPorts</td><td></td><td>✅</td><td></td></tr><tr><td>Nix</td><td></td><td>✅</td><td>✅</td></tr></tbody></table><blockquote><p><strong>Note:</strong> Ensure that the package managers are successfully installed in your operating system environment.</p></blockquote><h4 id=winget-windows>Winget (Windows)<a hidden class=anchor aria-hidden=true href=#winget-windows>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>winget install llama.cpp
</span></span></code></pre></div><h4 id=homebrew-mac-and-linux>Homebrew (Mac and Linux)<a hidden class=anchor aria-hidden=true href=#homebrew-mac-and-linux>#</a></h4><blockquote><p><strong>Note:</strong> The commands apply equally to both macOS and Linux.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>brew install llama.cpp
</span></span></code></pre></div><h4 id=macports-mac>MacPorts (Mac)<a hidden class=anchor aria-hidden=true href=#macports-mac>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo port install llama.cpp
</span></span></code></pre></div><p><strong>See also:</strong> <a href=https://ports.macports.org/port/llama.cpp/details/>MacPorts llama.cpp details</a></p><h4 id=nix-mac-and-linux>Nix (Mac and Linux)<a hidden class=anchor aria-hidden=true href=#nix-mac-and-linux>#</a></h4><blockquote><p><strong>Note:</strong> The commands apply equally to both macOS and Linux.</p></blockquote><h4 id=for-flake-enabled-installs>For flake-enabled installs<a hidden class=anchor aria-hidden=true href=#for-flake-enabled-installs>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nix profile install nixpkgs#llama-cpp
</span></span></code></pre></div><h4 id=for-non-flake-enabled-installs>For non-flake-enabled installs<a hidden class=anchor aria-hidden=true href=#for-non-flake-enabled-installs>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>nix-env --file <span class=s1>&#39;&lt;nixpkgs&gt;&#39;</span> --install --attr llama-cpp
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> This expression is automatically updated within the <a href=https://github.com/NixOS/nixpkgs/blob/nixos-24.05/pkgs/by-name/ll/llama-cpp/package.nix#L164>nixpkgs repo</a>.</p></blockquote><p>After installing llama.cpp, verify the installation by running the following command to ensure the framework is correctly set up in your local environment.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>llama-cli --version
</span></span></code></pre></div><p>The terminal should display output similar to the example shown below.</p><img src=./images/llama-cli.png alt=llama-cli-version-gpu-info style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><p>Congratulations, the llama.cpp framework has been successfully installed on your system.</p><p>Now, let’s move on to installing MLX in case you choose this framework for serving. For model deployment with MLX, keep in mind that it is only supported on macOS. The next section provides a detailed installation guide.
First, create a virtual environment using conda.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>conda create -n mlx <span class=nv>python</span><span class=o>=</span>3.11
</span></span><span class=line><span class=cl>conda activate mlx
</span></span></code></pre></div><p>Alright, with the environment activated, install the mlx-lm library.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install mlx-lm
</span></span></code></pre></div><p>For more information about the mlx-lm library, visit the <a href=https://github.com/ml-explore/mlx-lm>Git repository</a></p><p>To confirm that the mlx-lm library was installed successfully, you should see a log in the terminal similar to the one shown when running the same command.</p><img src=./images/mlx-version.png alt=mlx-lm-installation style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><h2 id=download-model-weights-from-hugging-face>Download Model Weights from Hugging Face<a hidden class=anchor aria-hidden=true href=#download-model-weights-from-hugging-face>#</a></h2><p>With the environment set up and all necessary tools and dependencies installed, the next step is to download a model. As this guide focuses on the <strong>Falcon 1B Instruct</strong> model, let&rsquo;s get the Falcon 3 1B model from the official <a href=https://huggingface.co/collections/tiiuae/falcon3>Hugging Face</a> repository maintained by TII.</p><p>The Hugging Face collection offers complete model weights in multiple formats, including <code>.safetensors</code> and <code>.gguf</code>. The default format for storing model weights is <code>.safetensors</code>. Keep in mind that <strong>llama.cpp</strong> can serve models only in the <code>.gguf</code> format, while the <strong>MLX</strong> framework supports both <code>.gguf</code> and <code>.safetensors</code>.</p><blockquote><p><strong>Note:</strong> Currently, TII provides ready-to-use .gguf and quantized model versions on their official Hugging Face repository. This means you can directly download the model weights to your local machine for use. When working with the llama.cpp framework, you’ll need a .gguf model as mentioned above. Make sure to choose a version that matches your hardware’s minimum requirements. In this example, we’ll experiment with the Falcon 1B Instruct .gguf model. If you’re using the mlx framework, you can use the model weights available on <a href=https://huggingface.co/mlx-community>mlx-community</a> Hugging Face organization</p></blockquote><p>To download the model weights, we’ll use the <strong>hf</strong> CLI tool. Ensure that you have it installed on your laptop and that you’re logged in with your Hugging Face account. To verify your installation and authentication, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hf auth whoami
</span></span></code></pre></div><p>If the installation or authentication hasn’t been completed, follow the instructions provided in <a href=https://huggingface.co/docs/huggingface_hub/guides/cli>huggingface link</a>.</p><p>Now, let’s download the .gguf model version by running the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hf download tiiuae/Falcon3-1B-Instruct-GGUF Falcon3-1B-Instruct-q4_k_m.gguf --local-dir Falcon3-1B-Instruct-GGUF 
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> that multiple quantized versions are available, so choose one that matches your hardware capabilities.</p></blockquote><p>For demonstrating model serving with MLX, we’ll download the 4-bit quantized model version with this command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hf download mlx-community/Falcon3-1B-Instruct-4bit --local-dir Falcon3-1B-Instruct-4bit
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> This script will download the entire Hugging Face repository to your local machine.</p></blockquote><p>By default, downloaded models are saved to Hugging Face’s <code>~/.cache/huggingface/hub</code> folder. However, if you prefer to specify a custom storage location, you can use the <code>--local-dir</code> tag and provide the exact folder path where you want the model saved. Once the downloads are complete, navigate to the folder where you saved the model weights to verify them. You should see output similar to the example shown below:</p><ul><li><code>.gguf</code> model</li></ul><img src=./images/model-gguf.png alt=Falcon-3-1B-Instruct-q4_k_m.gguf style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><ul><li><code>.safetensors</code> quantized model</li></ul><img src=./images/model-4bit.png alt=Falcon-3-1B-Instruct-4bit style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><h2 id=serving-falcon-3>Serving Falcon-3<a hidden class=anchor aria-hidden=true href=#serving-falcon-3>#</a></h2><p>The environment and all requirements for running the model are now fully set up. Next, we’ll walk through the steps for using each framework to serve LLMs directly on your system.
We will start with deploying the LLM using the <strong>llama.cpp</strong> framework.</p><h3 id=running-the-llamacpp-server>Running the llama.cpp Server<a hidden class=anchor aria-hidden=true href=#running-the-llamacpp-server>#</a></h3><p>First, make sure the server starts successfully and that it is running as expected.<br>To confirm the binary works, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>llama-server --help
</span></span></code></pre></div><p>This will display a list of available options. Some will be explained here, while others you can explore on your own. For now, the key options to note are:</p><ul><li><code>-m, --model FNAME</code>: Path to the model (default: <code>models/$filename</code> taken from <code>--hf-file</code> or <code>--model-url</code> if set, otherwise <code>models/7B/ggml-model-f16.gguf</code>). Environment variable: <code>LLAMA_ARG_MODEL</code>.</li><li><code>--host HOST</code>: IP address for the server to listen on (default: <code>127.0.0.1</code>). Environment variable: <code>LLAMA_ARG_HOST</code>.</li><li><code>--port PORT</code>: Port for the server to listen on (default: <code>8080</code>). Environment variable: <code>LLAMA_ARG_PORT</code>.</li></ul><p>To run the server with a quantized Falcon-3 GGUF model, execute:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>llama-server -m ./Falcon3-1B-Instruct-GGUF/Falcon3-1B-Instruct-q4_k_m.gguf
</span></span></code></pre></div><p>You should see similar output after running this command:</p><img src=./images/llama-server.png alt=llama-server-output style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><p>If nothing else is running on <code>127.0.0.1:8080</code>, the default host and port can be kept. You can also use environment variables instead of command-line arguments for convenience.<br>When the server starts, it will expose a local OpenAI-compatible API at:</p><pre tabindex=0><code>http://127.0.0.1:8080
</code></pre><h3 id=server-settings-in-llamacpp>Server Settings in llama.cpp<a hidden class=anchor aria-hidden=true href=#server-settings-in-llamacpp>#</a></h3><p>The <code>llama-server</code> includes a Web UI accessible via the API above. It shows a small set of configuration options focused on LLM sampling. For the full list, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>llama-server --help
</span></span></code></pre></div><p>These options can affect both how your model behaves and the speed of text generation. Many of them can be set via environment variables, which are consistent across other llama.cpp executables.</p><details><summary><strong>Click to view key general parameters</strong></summary><ul><li><code>--threads/--threads-batch (LLAMA_ARG_THREADS)</code>: Number of CPU threads to use. Default <code>-1</code> automatically detects available cores.</li><li><code>--ctx-size (LLAMA_ARG_CTX_SIZE)</code>: The model’s context size (how many tokens it can remember). Larger sizes require more memory.</li><li><code>--predict (LLAMA_ARG_N_PREDICT)</code>: Maximum tokens to generate. Default <code>-1</code> means continuous generation.</li><li><code>--batch-size/--ubatch-size (LLAMA_ARG_BATCH/LLAMA_ARG_UBATCH)</code> – Number of tokens processed per step.</li><li><code>--flash-attn (LLAMA_ARG_FLASH_ATTN)</code>: Enables flash attention optimization for supported models.</li><li><code>--mlock (LLAMA_ARG_MLOCK)</code>: Keeps the model in memory to avoid swapping.</li><li><code>--no-mmap (LLAMA_ARG_NO_MMAP)</code>: Disables memory mapping.</li><li><code>--gpu-layers (LLAMA_ARG_N_GPU_LAYERS)</code>: Number of layers to offload to the GPU (requires GPU-enabled build).</li></ul></details><details><summary><strong>Click to view server-specific parameters</strong></summary><ul><li><code>--no-context-shift (LLAMA_ARG_NO_CONTEXT_SHIFT)</code>: Stops generation when the context is full instead of discarding old tokens.</li><li><code>--cont-batching (LLAMA_ARG_CONT_BATCHING)</code>: Allows prompts to be processed in parallel with generation.</li><li><code>--alias (LLAMA_ARG_ALIAS)</code>: Sets a model name alias for the REST API.</li><li><code>--slots (LLAMA_ARG_ENDPOINT_SLOTS)</code>: Enables the <code>/slots</code> endpoint.</li><li><code>--props (LLAMA_ARG_ENDPOINT_PROPS)</code>: Enables the <code>/props</code> endpoint.</li></ul></details><h3 id=deployment-with-the-mlx-framework-on-macos>Deployment with the MLX Framework on macOS<a hidden class=anchor aria-hidden=true href=#deployment-with-the-mlx-framework-on-macos>#</a></h3><p>You now know how to serve the model on your laptop using llama.cpp. Next, I’ll show you how to use the MLX framework in case you have a MacBook. Another option for serving LLMs is the <strong>MLX</strong> framework, which is built specifically for macOS and optimized for Apple hardware.</p><h3 id=running-the-mlx-server>Running the MLX Server<a hidden class=anchor aria-hidden=true href=#running-the-mlx-server>#</a></h3><p>Start the server with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>mlx_lm.server --model ./Falcon3-1B-Instruct-4bit
</span></span></code></pre></div><p>You should see similar output after running this command:</p><img src=./images/mlx-server.png alt=mlx-lm-server-output style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"><p>This starts a text generation service on <code>localhost:8080</code> using the Falcon 3 1B Instruct model. If the model is not already in your local cache, it will be downloaded from the specified Hugging Face repository. However, I recommend downloading the model to your local system first, as described earlier.</p><p>To see all available options, run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>mlx_lm.server --help
</span></span></code></pre></div><p>To make a request to the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl localhost:8080/v1/chat/completions <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -H <span class=s2>&#34;Content-Type: application/json&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  -d <span class=s1>&#39;{
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;messages&#34;: [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Hello&#34;}],
</span></span></span><span class=line><span class=cl><span class=s1>     &#34;temperature&#34;: 0.7
</span></span></span><span class=line><span class=cl><span class=s1>   }&#39;</span>
</span></span></code></pre></div><h3 id=request-fields>Request Fields<a hidden class=anchor aria-hidden=true href=#request-fields>#</a></h3><details><summary><strong>Click to view all request fields</strong></summary><ul><li><code>messages</code>: List of message objects with role and content.</li><li><code>role_mapping</code>: Optional dictionary to customize role prefixes.</li><li><code>stop</code>: Optional stop sequences.</li><li><code>max_tokens</code>: Maximum tokens to generate (default: 512).</li><li><code>stream</code>: Boolean to stream responses (default: <code>false</code>).</li><li><code>temperature</code>: Controls randomness (default: 0.0).</li><li><code>top_p</code>: Nucleus sampling parameter (default: 1.0).</li><li><code>top_k</code>: Top-K sampling parameter (default: 0).</li><li><code>min_p</code>: Minimum probability sampling parameter (default: 0.0).</li><li><code>repetition_penalty</code>: Penalty for repeated tokens (default: 1.0).</li><li><code>repetition_context_size</code>: Context size for applying repetition penalty (default: 20).</li><li><code>logit_bias</code>: Mapping of token IDs to bias values.</li><li><code>logprobs</code>: Number of top token probabilities to return (1–10).</li><li><code>model</code>: Path or Hugging Face repo for the model.</li><li><code>adapters</code>: Path to low-rank adapters.</li><li><code>draft_model</code>: Smaller model for speculative decoding.</li><li><code>num_draft_tokens</code>: Draft tokens predicted at once (default: 3).</li></ul></details><h3 id=response-fields>Response Fields<a hidden class=anchor aria-hidden=true href=#response-fields>#</a></h3><details><summary><strong>Click to view all response fields</strong></summary><ul><li><code>id</code>: Unique chat ID.</li><li><code>system_fingerprint</code>: System identifier.</li><li><code>object</code>: Response type (<code>chat.completion</code>, <code>chat.completion.chunk</code>, or <code>text.completion</code>).</li><li><code>model</code>: Model path or repo name.</li><li><code>created</code>: Timestamp of request.</li><li><code>choices</code>: Output list with index, optional log probabilities, finish reason, and message text.</li><li><code>usage</code>: Token statistics: prompt, completion, and total tokens.</li></ul></details><h2 id=integrating-with-openwebui>Integrating with OpenwebUI<a hidden class=anchor aria-hidden=true href=#integrating-with-openwebui>#</a></h2><p>Once your selected serving framework is running, the next step is to connect Falcon-3 to Open WebUI, giving you a simple chat-based interface for interacting with the model.</p><h3 id=starting-open-webui>Starting Open WebUI<a hidden class=anchor aria-hidden=true href=#starting-open-webui>#</a></h3><p>First, let&rsquo;s proceed with installing open-webui cli. Use the command below to install:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install open-webui
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> You should install open-webui cli within your existing conda virtual environment. If you are using MLX to serve local llm and already have a conda environment set up, install open-webui cli in that same one. Then activate your environment to proceed.
After successful installation, let&rsquo;s launch the server.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>open-webui serve
</span></span></code></pre></div><ul><li>Default access: <a href=http://localhost:8080>http://localhost:8080</a></li><li>Custom port:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>open-webui serve --port <span class=m>3000</span> --host 0.0.0.0
</span></span></code></pre></div><h3 id=connecting-llamacpp-to-open-webui>Connecting llama.cpp to Open WebUI<a hidden class=anchor aria-hidden=true href=#connecting-llamacpp-to-open-webui>#</a></h3><ol><li>Open <strong>Open WebUI</strong> in your browser.</li><li>Navigate to <strong>Admin Settings → Connections → OpenAI</strong>.</li><li>Click <strong>Add Connection</strong>.</li><li>Under <strong>Standard / Compatible</strong> (if tabs are visible), configure the following:<ul><li><strong>URL:</strong> <code>http://127.0.0.1:8080/v1</code>.</li><li><strong>API Key:</strong> Leave blank or enter a specific key if configured.</li></ul></li></ol><p>Once the connection is saved, Open WebUI will use your local <strong>llama.cpp</strong> server or <strong>mlx</strong> server as its backend.</p><p>Alright, let’s head back to the main screen and try out a few questions to test the model’s intelligence.
<img src=./images/instruct-demo.png alt=falcon-3-instruct style="width:100%;height:auto;border-radius:8px;box-shadow:0 4px 6px rgba(0,0,0,.1);display:block;margin:1.5em 0"></p><p>Congratulations — you now have your very own chatbot powered by the Falcon models running right on your laptop.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This guide has outlined the process of deploying Falcon-3 locally using either the MLX framework for macOS-optimized workflows or the llama.cpp framework for GGUF model formats, along with integrating the chosen backend into Open WebUI to provide an interactive chat interface. The topics covered include minimum system requirements, environment setup, downloading model weights, server configuration, integration steps, and key considerations before deployment.</p><p>Following these steps allows Falcon-3 to be deployed in a way that matches your hardware capabilities and performance goals, ensuring both efficiency and ease of use. The MLX framework delivers smooth integration with Apple’s hardware ecosystem, while llama.cpp offers portability and compatibility with GGUF models, giving you flexible options for various AI applications.</p><p>For additional experimentation, you can explore other Falcon-3 variants available on Hugging Face, apply performance optimizations, or incorporate the model into custom workflows and interfaces. With the right setup, Falcon-3 can be a powerful and versatile asset in your local AI development environment.</p><div class=post-contributors><div class=contributors-section><h4>Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/FalconLLM.webp alt="Falcon LLM team" class=contributor-image><p class=contributor-name>Falcon LLM team</p></div></div></div></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span><a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html rel="noopener noreferrer" target=_blank>| Terms and Conditions</a>
</span><span><a href=https://www.tii.ae/privacy-policy rel="noopener noreferrer" target=_blank>| Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>