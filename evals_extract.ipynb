{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \"Categogy (Used by Qwen2.5, Falcon3 7B, DeepSeek)\"  Shots  \\\n",
      "0                                       General Tasks    NaN   \n",
      "1                                                MMLU      5   \n",
      "2                                                MMLU      0   \n",
      "3                                            MMLU-Pro      5   \n",
      "4                                          MMLU-redux      5   \n",
      "5                                                 BBH      3   \n",
      "6                                               ARC-C      0   \n",
      "7                                          TruthfulQA      0   \n",
      "8                                          Winogrande      5   \n",
      "9                                           HellaSwag      0   \n",
      "10                                            AGIEval      0   \n",
      "11                                               DROP      3   \n",
      "12                                           Triviaqa      5   \n",
      "13                                               PIQA      0   \n",
      "14                                                NaN    NaN   \n",
      "15                               Math & Science Tasks    NaN   \n",
      "16                                               GPQA  5 (0)   \n",
      "17                                          MMLU-stem      0   \n",
      "18                                          MMLU-stem      5   \n",
      "19                                              GSM8k      5   \n",
      "20                                              GSM8k      4   \n",
      "21                                  MATH (hendryck's)      4   \n",
      "22                                        Math-Verify      4   \n",
      "23                                          AIME 2024    NaN   \n",
      "24                                           MATH-500    NaN   \n",
      "25                                              AMC23    NaN   \n",
      "26                                           AIME2025    NaN   \n",
      "27                                                NaN    NaN   \n",
      "28                                       Coding Tasks    NaN   \n",
      "29                                          HumanEval    NaN   \n",
      "30                                         HumanEval+    NaN   \n",
      "31                                               MBPP    NaN   \n",
      "32                                              MBPP+    NaN   \n",
      "33                                          MultiPL-E    NaN   \n",
      "34                                           CRUXEval    NaN   \n",
      "35                                                NaN    NaN   \n",
      "36                               Instruct-model Tasks    NaN   \n",
      "37                                            MTBench    NaN   \n",
      "38                                          LiveBench    NaN   \n",
      "39                                        Alpaca-Eval    NaN   \n",
      "40                                      LiveCodeBench    NaN   \n",
      "41                                       GPQA_Diamond    NaN   \n",
      "42                                        IFEval_inst    NaN   \n",
      "43                                      IFEval_prompt    NaN   \n",
      "\n",
      "   Final eval (Instruct)            Qwen3-32B  Qwen2.5 72B  \\\n",
      "0                    NaN                  NaN          NaN   \n",
      "1                      Y                80.89        84.42   \n",
      "2                    NaN                44.47        82.58   \n",
      "3                      Y                54.68        56.35   \n",
      "4                      Y                  NaN          NaN   \n",
      "5                    NaN                62.47        72.52   \n",
      "6                    NaN                48.98        46.59   \n",
      "7                    NaN                58.58        69.80   \n",
      "8                    NaN                  NaN          NaN   \n",
      "9                    NaN                68.89        68.79   \n",
      "10                     -                23.38          NaN   \n",
      "11                   NaN                10.26          NaN   \n",
      "12                   NaN                  NaN          NaN   \n",
      "13                   NaN                 79.6          NaN   \n",
      "14                   NaN                  NaN          NaN   \n",
      "15                   NaN                  NaN          NaN   \n",
      "16                     -                 30.2        37.67   \n",
      "17                   NaN                35.49        78.88   \n",
      "18                   NaN                81.64        82.59   \n",
      "19                     -                88.78        82.26   \n",
      "20                   NaN                  NaN          NaN   \n",
      "21                     -                 0.04          NaN   \n",
      "22                     Y                58.53        58.61   \n",
      "23                     Y                27.71        17.29   \n",
      "24                     Y                   82        83.60   \n",
      "25                   NaN                67.34        67.34   \n",
      "26                   NaN                19.79        15.21   \n",
      "27                   NaN                  NaN          NaN   \n",
      "28                   NaN                  NaN          NaN   \n",
      "29                     Y                90.85        87.20   \n",
      "30                     Y                85.37        80.49   \n",
      "31                     Y                86.24        89.68   \n",
      "32                     Y                71.96        75.40   \n",
      "33                     Y                  NaN          NaN   \n",
      "34                     Y                  NaN          NaN   \n",
      "35                   NaN                  NaN          NaN   \n",
      "36                   NaN                  NaN          NaN   \n",
      "37                     Y  2025-05-09 00:00:00         9.16   \n",
      "38                     Y                63.05        54.03   \n",
      "39                     Y                64.21        49.29   \n",
      "40                     Y                45.01        54.60   \n",
      "41                     Y                49.49        44.95   \n",
      "42                     Y                89.09        88.97   \n",
      "43                     Y                84.84        83.73   \n",
      "\n",
      "            Qwen2.5 32B  Mistral-Small-3.1-24B           Gemma3-27B  \\\n",
      "0                   NaN                    NaN                  NaN   \n",
      "1                  82.8                  81.14                78.01   \n",
      "2                 74.61                    NaN                74.24   \n",
      "3                 56.63                    NaN                47.81   \n",
      "4                   NaN                    NaN                  NaN   \n",
      "5                 68.72                    NaN                67.28   \n",
      "6                 44.54                    NaN                54.52   \n",
      "7                 70.28                    NaN                64.26   \n",
      "8                   NaN                    NaN                  NaN   \n",
      "9                 73.95                    NaN                57.25   \n",
      "10                54.89                    NaN                  NaN   \n",
      "11                 8.96                    NaN                  NaN   \n",
      "12                  NaN                  79.51                  NaN   \n",
      "13                77.26                    NaN                  NaN   \n",
      "14                  NaN                    NaN                  NaN   \n",
      "15                  NaN                    NaN                  NaN   \n",
      "16                34.31                    NaN                36.49   \n",
      "17                61.05                    NaN                69.46   \n",
      "18                82.37                  76.72                73.55   \n",
      "19                78.47                    NaN                90.37   \n",
      "20                80.29                    NaN  2025-07-04 00:00:00   \n",
      "21                  NaN                    NaN                  NaN   \n",
      "22                60.95                    NaN                63.22   \n",
      "23                17.92                    NaN  2025-05-27 00:00:00   \n",
      "24                 82.2                    NaN                   90   \n",
      "25                68.75                    NaN                77.81   \n",
      "26                11.46                    NaN                22.71   \n",
      "27                  NaN                    NaN                  NaN   \n",
      "28                  NaN                    NaN                  NaN   \n",
      "29                90.24                  80.49                86.59   \n",
      "30                82.32                  72.56                78.05   \n",
      "31                87.83                  81.22                88.36   \n",
      "32                74.07                  68.52                74.07   \n",
      "33                  NaN                    NaN                  NaN   \n",
      "34                  NaN                    NaN                  NaN   \n",
      "35                  NaN                    NaN                  NaN   \n",
      "36                  NaN                    NaN                  NaN   \n",
      "37  2025-09-09 00:00:00                    NaN                 8.75   \n",
      "38                52.92                    NaN                55.41   \n",
      "39                39.26                    NaN                56.16   \n",
      "40                49.12                    NaN                39.53   \n",
      "41                40.74                    NaN                47.47   \n",
      "42                85.01                    NaN                86.33   \n",
      "43                78.56                    NaN                80.04   \n",
      "\n",
      "           Llama3.1 70B         Llama4-scout  ...          SmoLM2 1.7B  \\\n",
      "0                   NaN                  NaN  ...                  NaN   \n",
      "1                 83.19                 80.4  ...                49.56   \n",
      "2                 80.74                70.82  ...                49.65   \n",
      "3                 53.47                55.58  ...                20.58   \n",
      "4                   NaN                  NaN  ...                  NaN   \n",
      "5                 69.17                 64.9  ...                35.65   \n",
      "6                 63.65                56.14  ...                40.27   \n",
      "7                 66.82                62.74  ...                45.43   \n",
      "8                   NaN                  NaN  ...                  NaN   \n",
      "9                 78.77                65.03  ...                66.54   \n",
      "10                  NaN                  NaN  ...                  NaN   \n",
      "11                  NaN                  NaN  ...                  NaN   \n",
      "12                  NaN                  NaN  ...                  NaN   \n",
      "13                  NaN                  NaN  ...                  NaN   \n",
      "14                  NaN                  NaN  ...                  NaN   \n",
      "15                  NaN                  NaN  ...                  NaN   \n",
      "16                34.48  2025-08-31 00:00:00  ...                27.77   \n",
      "17                72.28                52.87  ...                42.82   \n",
      "18                77.01                 75.2  ...                43.61   \n",
      "19                93.71                90.37  ...                48.45   \n",
      "20                  NaN                  NaN  ...                  NaN   \n",
      "21                  NaN                  NaN  ...                  NaN   \n",
      "22                38.75                57.02  ...                 5.21   \n",
      "23                12.92                27.92  ...                  NaN   \n",
      "24                 64.8                 83.2  ...                  NaN   \n",
      "25                39.38                69.06  ...                  NaN   \n",
      "26                 1.25                 8.96  ...                  NaN   \n",
      "27                  NaN                  NaN  ...                  NaN   \n",
      "28                  NaN                  NaN  ...                  NaN   \n",
      "29                79.88                 85.4  ...                34.76   \n",
      "30                72.56                 78.7  ...                28.66   \n",
      "31                83.07                 81.5  ...                55.03   \n",
      "32                64.55                 64.8  ...                44.97   \n",
      "33                  NaN                  NaN  ...                  NaN   \n",
      "34                  NaN                  NaN  ...                  NaN   \n",
      "35                  NaN                  NaN  ...                  NaN   \n",
      "36                  NaN                  NaN  ...                  NaN   \n",
      "37  2025-11-09 00:00:00                 8.98  ...                 6.49   \n",
      "38                49.36                54.21  ...                18.13   \n",
      "39                 35.5                36.26  ...  2025-04-07 00:00:00   \n",
      "40                36.59                40.12  ...                  NaN   \n",
      "41                42.09                51.18  ...                23.91   \n",
      "42                89.21                89.09  ...                60.07   \n",
      "43                84.66                83.55  ...                48.43   \n",
      "\n",
      "           Falcon3 1.7B  Falcon-H1 1.5B-16k Falcon-H1 1.5B-128k  \\\n",
      "0                   NaN                 NaN                 NaN   \n",
      "1                  46.1               62.66               62.03   \n",
      "2                 43.91               60.09               60.27   \n",
      "3                 18.49               38.30               37.80   \n",
      "4                   NaN                 NaN                 NaN   \n",
      "5                 34.47               47.86               46.47   \n",
      "6                 43.09               41.89               42.06   \n",
      "7                 42.31               46.07               45.98   \n",
      "8                   NaN                 NaN                 NaN   \n",
      "9                 58.53               63.24               63.33   \n",
      "10                  NaN                 NaN                 NaN   \n",
      "11                  NaN                 NaN                 NaN   \n",
      "12                  NaN                 NaN                 NaN   \n",
      "13                  NaN                 NaN                 NaN   \n",
      "14                  NaN                 NaN                 NaN   \n",
      "15                  NaN                 NaN                 NaN   \n",
      "16                26.76               26.43               26.34   \n",
      "17                37.61               60.10               60.07   \n",
      "18                39.64               64.29               64.13   \n",
      "19                44.05               75.97               74.98   \n",
      "20                  NaN                 NaN                 NaN   \n",
      "21                  NaN                 NaN                 NaN   \n",
      "22                  NaN               44.56               43.96   \n",
      "23                  NaN               11.46               11.25   \n",
      "24                  NaN               73.40               74.00   \n",
      "25                  NaN               47.97               43.59   \n",
      "26                  NaN                7.29                9.58   \n",
      "27                  NaN                 NaN                 NaN   \n",
      "28                  NaN                 NaN                 NaN   \n",
      "29                22.56               69.51               68.29   \n",
      "30                20.73               62.20               61.59   \n",
      "31                20.63               66.40               64.81   \n",
      "32  2025-02-17 00:00:00               57.14               56.35   \n",
      "33                  NaN                 NaN                 NaN   \n",
      "34                  NaN                 NaN                 NaN   \n",
      "35                  NaN                 NaN                 NaN   \n",
      "36                  NaN                 NaN                 NaN   \n",
      "37  2025-03-06 00:00:00                8.64                8.46   \n",
      "38  2025-01-14 00:00:00               34.33               34.13   \n",
      "39                 6.98               26.93               28.18   \n",
      "40                  NaN               20.74               17.61   \n",
      "41                31.31               34.01               35.19   \n",
      "42                59.71               85.37               84.05   \n",
      "43                 48.8               78.93               77.26   \n",
      "\n",
      "   Falcon-H1 1.5B-deep-16k  Falcon-H1 1.5B-deep-128k Unnamed: 45  \\\n",
      "0                      NaN                       NaN         NaN   \n",
      "1                    66.04                     66.11         NaN   \n",
      "2                    64.11                     64.09         NaN   \n",
      "3                     41.4                     41.89         NaN   \n",
      "4                      NaN                       NaN         NaN   \n",
      "5                    54.45                     54.43         NaN   \n",
      "6                     44.2                     43.86         NaN   \n",
      "7                    51.19                     50.48         NaN   \n",
      "8                      NaN                       NaN         NaN   \n",
      "9                     65.4                     65.54         NaN   \n",
      "10                     NaN                       NaN         NaN   \n",
      "11                     NaN                       NaN         NaN   \n",
      "12                     NaN                       NaN         NaN   \n",
      "13                     NaN                       NaN         NaN   \n",
      "14                     NaN                       NaN         NaN   \n",
      "15                     NaN                       NaN         NaN   \n",
      "16                   31.88                     33.22         NaN   \n",
      "17                   64.99                     64.99         NaN   \n",
      "18                   67.52                      67.3         NaN   \n",
      "19                   82.03                     82.34         NaN   \n",
      "20                     NaN                       NaN         NaN   \n",
      "21                     NaN                       NaN         NaN   \n",
      "22                   48.79                      46.6         NaN   \n",
      "23     2025-12-13 00:00:00                     14.37         NaN   \n",
      "24                    76.6                      77.8         NaN   \n",
      "25                    57.5                     56.56         NaN   \n",
      "26                   14.17       2025-04-11 00:00:00         NaN   \n",
      "27                     NaN                       NaN         NaN   \n",
      "28                     NaN                       NaN         NaN   \n",
      "29                   76.22                     73.78         NaN   \n",
      "30                    68.9                      68.9         NaN   \n",
      "31                   67.72                     68.25         NaN   \n",
      "32                   56.61                     56.61         NaN   \n",
      "33                     NaN                       NaN         NaN   \n",
      "34                     NaN                       NaN         NaN   \n",
      "35                     NaN                       NaN         NaN   \n",
      "36                     NaN                       NaN         NaN   \n",
      "37                    8.43                      8.53         NaN   \n",
      "38                   36.86                     36.83         NaN   \n",
      "39                   24.79       2025-12-27 00:00:00         NaN   \n",
      "40                   24.27                     23.87         NaN   \n",
      "41                   38.22                     40.57         NaN   \n",
      "42                   87.29                     86.21         NaN   \n",
      "43                   82.07                     80.78         NaN   \n",
      "\n",
      "             Qwen3-0.6B         Qwen2.5 0.5B   Falcon-H1-0.5B-16k  \n",
      "0                   NaN                  NaN                  NaN  \n",
      "1                 42.98                46.07                 53.4  \n",
      "2                  34.6                46.52                48.23  \n",
      "3                 16.95                18.73  2025-03-31 00:00:00  \n",
      "4                   NaN                  NaN                  NaN  \n",
      "5                 32.95                33.26                42.91  \n",
      "6                 31.06                33.28                 37.8  \n",
      "7                 51.65                46.19                44.12  \n",
      "8                   NaN                56.27                  NaN  \n",
      "9                 42.17                52.38                51.93  \n",
      "10                26.71                  NaN  2025-04-29 00:00:00  \n",
      "11  2025-07-06 00:00:00                  NaN  2025-08-04 00:00:00  \n",
      "12                  NaN                  NaN                 0.25  \n",
      "13                66.38                  NaN                69.48  \n",
      "14                  NaN                  NaN                  NaN  \n",
      "15                  NaN                  NaN                  NaN  \n",
      "16  2025-09-26 00:00:00                26.85                29.95  \n",
      "17                28.58                 40.5                45.77  \n",
      "18                 39.3                39.83                54.55  \n",
      "19                42.61                38.51                68.39  \n",
      "20                  NaN                20.39                  NaN  \n",
      "21                 0.02                  NaN                  NaN  \n",
      "22                18.73  2025-05-10 00:00:00                31.19  \n",
      "23                 2.71                 0.62                 3.75  \n",
      "24                   46  2025-08-27 00:00:00                 58.4  \n",
      "25                27.97  2025-05-12 00:00:00                33.13  \n",
      "26                 1.67                 0.21                 4.38  \n",
      "27                  NaN                  NaN                  NaN  \n",
      "28                  NaN                  NaN                  NaN  \n",
      "29                41.46                36.59                51.83  \n",
      "30                37.19                32.32                45.12  \n",
      "31                56.08                46.83                42.59  \n",
      "32                47.08                39.68                33.07  \n",
      "33                  NaN                  NaN                  NaN  \n",
      "34                  NaN                  NaN                  NaN  \n",
      "35                  NaN                  NaN                  NaN  \n",
      "36                  NaN                  NaN                  NaN  \n",
      "37                 5.75                 4.71  2025-06-07 00:00:00  \n",
      "38                27.78                14.27  2025-08-20 00:00:00  \n",
      "39                 9.59                 3.26                10.79  \n",
      "40                 9.78                 2.94  2025-05-07 00:00:00  \n",
      "41  2025-08-25 00:00:00                24.24                27.95  \n",
      "42                67.39                37.05                76.86  \n",
      "43                56.93                27.17                67.28  \n",
      "\n",
      "[44 rows x 49 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Basic usage\n",
    "df = pd.read_excel('/home/ubuntu/bgg/evals-2.xlsx')\n",
    "\n",
    "# With more options\n",
    "# df = pd.read_excel(\n",
    "#     'your_file.xlsx',\n",
    "#     sheet_name='Sheet1',  # Specify sheet name or index (0 is first sheet)\n",
    "#     header=0,             # Row to use as column names (0-indexed)\n",
    "#     usecols='A:D',        # Columns to read (can be range, list of indices, or names)\n",
    "#     skiprows=2,           # Skip first 2 rows\n",
    "#     nrows=100             # Read only 100 rows\n",
    "# )\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\"Final eval (Instruct)\",size:None,group:\"Final\",scores:{}\n",
      "model:\"Qwen3-32B\",size:32.0,group:\"Qwen3\",scores:{AGIEval:23.38,AIME24:27.71,AIME25:19.79,AMC23:67.34,ARC-C:48.98,Alpaca-Eval:64.21,BBH:62.47,DROP:10.26,GPQA:30.2,GPQA_Diamond:49.49,GSM8k:88.78,HellaSwag:68.89,HumanEval:90.85,HumanEval+:85.37,IFEval_inst:89.09,IFEval_prompt:84.84,LiveBench:63.05,LiveCodeBench:45.01,MATH (hendrycks):0.04,MATH-500:82.0,MBPP:86.24,MBPP+:71.96,MMLU:62.68,MMLU-Pro:54.68,MMLU-stem:58.56,Math-Verify:58.53,PIQA:79.6,TruthfulQA:58.58}\n",
      "model:\"Qwen2.5 72B\",size:72.0,group:\"Qwen2.5\",scores:{AIME24:17.29,AIME25:15.21,AMC23:67.34,ARC-C:46.59,Alpaca-Eval:49.29,BBH:72.52,GPQA:37.67,GPQA_Diamond:44.95,GSM8k:82.26,HellaSwag:68.79,HumanEval:87.2,HumanEval+:80.49,IFEval_inst:88.97,IFEval_prompt:83.73,LiveBench:54.03,LiveCodeBench:54.6,MATH-500:83.6,MBPP:89.68,MBPP+:75.4,MMLU:83.5,MMLU-Pro:56.35,MMLU-stem:80.73,MTBench:9.16,Math-Verify:58.61,TruthfulQA:69.8}\n",
      "model:\"Qwen2.5 32B\",size:32.0,group:\"Qwen2.5\",scores:{AGIEval:54.89,AIME24:17.92,AIME25:11.46,AMC23:68.75,ARC-C:44.54,Alpaca-Eval:39.26,BBH:68.72,DROP:8.96,GPQA:34.31,GPQA_Diamond:40.74,GSM8k:79.38,HellaSwag:73.95,HumanEval:90.24,HumanEval+:82.32,IFEval_inst:85.01,IFEval_prompt:78.56,LiveBench:52.92,LiveCodeBench:49.12,MATH-500:82.2,MBPP:87.83,MBPP+:74.07,MMLU:78.7,MMLU-Pro:56.63,MMLU-stem:71.71,Math-Verify:60.95,PIQA:77.26,TruthfulQA:70.28}\n",
      "model:\"Mistral-Small-3.1-24B\",size:24.0,group:\"Mistral-Small\",scores:{HumanEval:80.49,HumanEval+:72.56,MBPP:81.22,MBPP+:68.52,MMLU:81.14,MMLU-stem:76.72,Triviaqa:79.51}\n",
      "model:\"Gemma3-27B\",size:27.0,group:\"Gemma3\",scores:{AIME25:22.71,AMC23:77.81,ARC-C:54.52,Alpaca-Eval:56.16,BBH:67.28,GPQA:36.49,GPQA_Diamond:47.47,GSM8k:90.37,HellaSwag:57.25,HumanEval:86.59,HumanEval+:78.05,IFEval_inst:86.33,IFEval_prompt:80.04,LiveBench:55.41,LiveCodeBench:39.53,MATH-500:90.0,MBPP:88.36,MBPP+:74.07,MMLU:76.12,MMLU-Pro:47.81,MMLU-stem:71.5,MTBench:8.75,Math-Verify:63.22,TruthfulQA:64.26}\n",
      "model:\"Llama3.1 70B\",size:70.0,group:\"Llama3.1\",scores:{AIME24:12.92,AIME25:1.25,AMC23:39.38,ARC-C:63.65,Alpaca-Eval:35.5,BBH:69.17,GPQA:34.48,GPQA_Diamond:42.09,GSM8k:93.71,HellaSwag:78.77,HumanEval:79.88,HumanEval+:72.56,IFEval_inst:89.21,IFEval_prompt:84.66,LiveBench:49.36,LiveCodeBench:36.59,MATH-500:64.8,MBPP:83.07,MBPP+:64.55,MMLU:81.97,MMLU-Pro:53.47,MMLU-stem:74.65,Math-Verify:38.75,TruthfulQA:66.82}\n",
      "model:\"Llama4-scout\",size:None,group:\"Llama4-scout\",scores:{AIME24:27.92,AIME25:8.96,AMC23:69.06,ARC-C:56.14,Alpaca-Eval:36.26,BBH:64.9,GPQA_Diamond:51.18,GSM8k:90.37,HellaSwag:65.03,HumanEval:85.4,HumanEval+:78.7,IFEval_inst:89.09,IFEval_prompt:83.55,LiveBench:54.21,LiveCodeBench:40.12,MATH-500:83.2,MBPP:81.5,MBPP+:64.8,MMLU:75.61,MMLU-Pro:55.58,MMLU-stem:64.03,MTBench:8.98,Math-Verify:57.02,TruthfulQA:62.74}\n",
      "model:\"Falcon3.1 74B\",size:74.0,group:\"Falcon3.1\",scores:{AIME25:10.83,AMC23:60.0,ARC-C:63.4,Alpaca-Eval:48.81,BBH:71.67,GPQA:40.77,GPQA_Diamond:43.1,GSM8k:86.58,HellaSwag:81.17,HumanEval:83.54,HumanEval+:78.66,IFEval_inst:86.09,IFEval_prompt:80.04,LiveBench:45.28,LiveCodeBench:26.22,MATH-500:82.2,MBPP:87.3,MBPP+:73.54,MMLU:82.02,MMLU-Pro:56.86,MMLU-stem:80.78,MTBench:8.74,Math-Verify:52.42,TruthfulQA:56.41}\n",
      "model:\"GTP4o (mini)\",size:None,group:\"GTP4o\",scores:{}\n",
      "model:\"gpt-4o-mini-2024-07-18\",size:None,group:\"gpt\",scores:{}\n",
      "model:\"Falcon-H1-34B-16k\",size:34.0,group:\"Falcon-H1\",scores:{AIME24:21.25,AIME25:14.79,AMC23:67.81,ARC-C:61.26,Alpaca-Eval:56.84,BBH:71.67,GPQA:41.36,GPQA_Diamond:49.83,GSM8k:86.58,HellaSwag:81.7,HumanEval:87.2,HumanEval+:79.27,IFEval_inst:91.25,IFEval_prompt:87.25,LiveBench:47.32,LiveCodeBench:45.79,MATH-500:81.4,MBPP:84.39,MBPP+:71.43,MMLU:83.22,MMLU-Pro:58.8,MMLU-stem:83.38,Math-Verify:56.34,TruthfulQA:63.91}\n",
      "model:\"Falcon-H1-34B-128k\",size:34.0,group:\"Falcon-H1\",scores:{AIME24:23.75,AIME25:16.67,AMC23:69.38,ARC-C:61.01,Alpaca-Eval:48.32,BBH:70.68,GPQA:41.53,GPQA_Diamond:49.66,GSM8k:83.62,HellaSwag:81.94,HumanEval:87.2,HumanEval+:81.71,IFEval_inst:91.49,IFEval_prompt:87.25,LiveBench:46.26,LiveCodeBench:49.71,MATH-500:83.8,MBPP:83.86,MBPP+:71.43,MMLU:83.1,MMLU-Pro:58.73,MMLU-stem:82.97,Math-Verify:58.91,TruthfulQA:65.27}\n",
      "model:\"Unnamed: 15\",size:None,group:\"Unnamed:\",scores:{}\n",
      "model:\"Qwen3-14B\",size:14.0,group:\"Qwen3\",scores:{AGIEval:23.29,AIME24:29.58,AIME25:22.29,AMC23:72.97,ARC-C:42.41,Alpaca-Eval:56.18,BBH:59.5,DROP:11.79,GPQA_Diamond:50.17,GSM8k:83.02,HellaSwag:63.82,HumanEval:88.41,HumanEval+:85.97,IFEval_inst:89.57,IFEval_prompt:84.47,LiveBench:62.91,LiveCodeBench:53.62,MATH (hendrycks):0.0,MATH-500:87.0,MBPP:87.3,MBPP+:75.4,MMLU:51.71,MMLU-Pro:44.8,MMLU-stem:49.66,MTBench:8.97,Math-Verify:55.51,PIQA:74.48,TruthfulQA:55.43}\n",
      "model:\"Qwen2.5 14B\",size:14.0,group:\"Qwen2.5\",scores:{AGIEval:57.51,AIME24:13.33,AMC23:57.66,ARC-C:50.17,Alpaca-Eval:37.77,BBH:63.67,DROP:8.77,GPQA:34.06,GPQA_Diamond:36.7,GSM8k:84.11,HellaSwag:75.54,HumanEval:82.32,HumanEval+:73.78,IFEval_inst:84.29,IFEval_prompt:79.3,LiveBench:45.04,LiveCodeBench:38.36,MATH-500:80.8,MBPP:83.07,MBPP+:70.37,MMLU:77.78,MMLU-Pro:49.34,MMLU-stem:73.26,Math-Verify:55.29,PIQA:79.16,TruthfulQA:70.95}\n",
      "model:\"Qwen3-8B\",size:8.0,group:\"Qwen3\",scores:{AGIEval:23.63,AIME24:28.33,AIME25:19.17,AMC23:70.78,ARC-C:42.06,Alpaca-Eval:46.13,BBH:47.47,DROP:12.44,GPQA:25.84,GPQA_Diamond:43.1,GSM8k:78.92,HellaSwag:60.56,HumanEval:84.75,HumanEval+:79.27,IFEval_inst:86.45,IFEval_prompt:80.41,LiveBench:56.19,LiveCodeBench:45.6,MATH (hendrycks):0.0,MATH-500:83.8,MBPP:71.96,MBPP+:62.7,MMLU:60.51,MMLU-Pro:34.64,MMLU-stem:53.66,MTBench:8.74,Math-Verify:54.83,PIQA:72.52,TruthfulQA:53.19}\n",
      "model:\"Qwen2.5 7B\",size:7.0,group:\"Qwen2.5\",scores:{AIME24:12.29,AIME25:9.58,AMC23:53.91,ARC-C:41.38,Alpaca-Eval:29.48,BBH:53.76,GPQA:31.79,GPQA_Diamond:33.0,GSM8k:73.28,HellaSwag:63.4,HumanEval:82.32,HumanEval+:73.78,IFEval_inst:79.14,IFEval_prompt:71.35,LiveBench:37.13,LiveCodeBench:32.68,MATH-500:75.8,MBPP:79.63,MBPP+:68.25,MMLU:71.64,MMLU-Pro:43.23,MMLU-stem:65.78,MTBench:8.45,TruthfulQA:62.41}\n",
      "model:\"Gemma3-12B\",size:12.0,group:\"Gemma3\",scores:{AIME25:18.75,AMC23:66.88,ARC-C:51.96,Alpaca-Eval:43.55,BBH:63.36,GPQA:33.98,GPQA_Diamond:37.71,GSM8k:43.74,HellaSwag:55.63,HumanEval:84.76,HumanEval+:75.61,IFEval_inst:84.65,IFEval_prompt:78.37,LiveBench:49.23,LiveCodeBench:30.92,MATH-500:86.2,MBPP:85.71,MBPP+:72.22,MMLU:71.66,MMLU-Pro:39.88,MMLU-stem:65.89,MTBench:8.69,Math-Verify:56.27,TruthfulQA:61.02}\n",
      "model:\"Llama3.1 8B\",size:8.0,group:\"Llama3.1\",scores:{AIME24:5.42,AIME25:0.42,AMC23:22.81,ARC-C:52.39,Alpaca-Eval:25.48,BBH:48.58,GPQA:32.72,GPQA_Diamond:31.31,GSM8k:41.7,HellaSwag:71.28,HumanEval:68.29,HumanEval+:61.59,IFEval_inst:81.06,IFEval_prompt:73.01,LiveBench:31.73,LiveCodeBench:15.85,MATH-500:45.8,MBPP:68.25,MBPP+:55.03,MMLU:66.11,MMLU-Pro:36.42,MMLU-stem:55.49,MTBench:8.29,TruthfulQA:52.99}\n",
      "model:\"Falcon3 7B\",size:7.0,group:\"Falcon3\",scores:{AIME24:8.75,AIME25:6.25,AMC23:40.0,ARC-C:54.35,Alpaca-Eval:27.56,BBH:52.12,GPQA:31.21,GPQA_Diamond:37.21,GSM8k:82.22,HellaSwag:71.81,HumanEval:71.95,HumanEval+:65.85,IFEval_inst:80.34,IFEval_prompt:72.83,LiveBench:32.35,LiveCodeBench:12.72,MATH-500:69.0,MBPP:77.25,MBPP+:65.87,MMLU:69.02,MMLU-Pro:40.73,MMLU-stem:66.18,MTBench:8.73,TruthfulQA:55.58}\n",
      "model:\"Falcon3 10B\",size:10.0,group:\"Falcon3\",scores:{AIME24:9.79,AIME25:5.42,AMC23:45.78,ARC-C:54.44,Alpaca-Eval:24.31,BBH:58.09,GPQA:33.39,GPQA_Diamond:34.68,GSM8k:86.2,HellaSwag:75.57,HumanEval:82.32,HumanEval+:75.0,IFEval_inst:82.25,IFEval_prompt:75.42,LiveBench:34.3,LiveCodeBench:19.77,MATH-500:68.6,MBPP:73.28,MBPP+:64.02,MMLU:72.9,MMLU-Pro:44.05,MMLU-stem:69.58,MTBench:8.46,TruthfulQA:55.05}\n",
      "model:\"Falcon-H1-7B-16k\",size:7.0,group:\"Falcon-H1\",scores:{AIME24:16.46,AIME25:16.46,AMC23:60.78,ARC-C:60.49,Alpaca-Eval:42.26,BBH:62.78,GPQA:37.08,GPQA_Diamond:56.9,GSM8k:82.71,HellaSwag:78.68,HumanEval:82.32,HumanEval+:76.83,IFEval_inst:88.73,IFEval_prompt:84.29,LiveBench:46.74,LiveCodeBench:36.99,MATH-500:81.6,MBPP:77.25,MBPP+:66.67,MMLU:75.91,MMLU-Pro:51.62,MMLU-stem:76.3,MTBench:8.98,Math-Verify:52.79,TruthfulQA:60.75}\n",
      "model:\"Falcon-H1-7B-128k\",size:7.0,group:\"Falcon-H1\",scores:{AIME25:13.96,AMC23:56.72,ARC-C:59.98,Alpaca-Eval:40.23,BBH:62.28,GPQA:36.33,GPQA_Diamond:56.9,GSM8k:81.65,HellaSwag:75.92,HumanEval:86.59,HumanEval+:81.1,IFEval_inst:87.89,IFEval_prompt:82.81,LiveBench:45.74,LiveCodeBench:35.03,MATH-500:73.4,MBPP:80.69,MBPP+:68.78,MMLU:76.02,MMLU-Pro:51.75,MMLU-stem:76.5,MTBench:8.85,Math-Verify:48.34,TruthfulQA:59.91}\n",
      "model:\"Unnamed: 26\",size:None,group:\"Unnamed:\",scores:{}\n",
      "model:\"Qwen3-4B\",size:4.0,group:\"Qwen3\",scores:{AGIEval:24.95,AIME24:22.29,AIME25:18.96,AMC23:66.88,ARC-C:37.71,Alpaca-Eval:36.51,BBH:51.07,DROP:10.98,GPQA_Diamond:40.74,GSM8k:80.44,HellaSwag:55.31,HumanEval:84.15,HumanEval+:76.83,IFEval_inst:86.69,IFEval_prompt:81.33,LiveBench:51.34,LiveCodeBench:39.92,MATH (hendrycks):0.0,MATH-500:85.0,MBPP:68.78,MBPP+:59.79,MMLU:61.87,MMLU-Pro:29.75,MMLU-stem:58.77,MTBench:8.45,Math-Verify:54.31,PIQA:68.93,TruthfulQA:51.75}\n",
      "model:\"Qwen2.5 3B\",size:3.0,group:\"Qwen2.5\",scores:{AIME24:6.25,AIME25:3.96,AMC23:39.84,ARC-C:43.77,Alpaca-Eval:17.37,BBH:46.55,GPQA:28.69,GPQA_Diamond:35.69,GSM8k:58.64,HellaSwag:64.21,HumanEval:73.78,HumanEval+:68.29,IFEval_inst:69.18,IFEval_prompt:59.33,LiveBench:27.32,LiveCodeBench:11.74,MATH-500:64.2,MBPP:72.75,MBPP+:60.85,MMLU:64.8,MMLU-Pro:32.76,MMLU-stem:59.18,MTBench:7.79,Math-Verify:37.31,TruthfulQA:58.11}\n",
      "model:\"Gemma3-4B\",size:4.0,group:\"Gemma3\",scores:{AIME24:6.67,AIME25:13.33,AMC23:48.12,ARC-C:44.88,Alpaca-Eval:39.64,BBH:50.01,GPQA:29.19,GPQA_Diamond:28.62,GSM8k:38.7,HellaSwag:47.68,HumanEval:67.07,HumanEval+:61.59,IFEval_inst:81.18,IFEval_prompt:72.83,LiveBench:36.7,LiveCodeBench:21.14,MATH-500:76.4,MBPP:77.78,MBPP+:66.93,MMLU:55.34,MMLU-Pro:29.71,MMLU-stem:47.3,MTBench:8.24,Math-Verify:38.6,TruthfulQA:51.68}\n",
      "model:\"Llama3.2 3B\",size:3.0,group:\"Llama3.2\",scores:{AIME24:11.67,AIME25:0.21,AMC23:22.66,ARC-C:44.88,Alpaca-Eval:19.69,BBH:41.47,GPQA:28.94,GPQA_Diamond:29.97,GSM8k:77.26,HellaSwag:63.74,HumanEval:54.27,HumanEval+:50.0,IFEval_inst:78.3,IFEval_prompt:69.69,LiveBench:26.37,LiveCodeBench:2.74,MATH-500:41.2,MBPP:62.17,MBPP+:50.53,MMLU:59.36,MMLU-Pro:27.44,MMLU-stem:49.14,MTBench:7.96,TruthfulQA:50.27}\n",
      "model:\"Falcon3 3B\",size:3.0,group:\"Falcon3\",scores:{AIME24:3.96,AIME25:2.29,AMC23:29.69,ARC-C:48.21,Alpaca-Eval:14.82,BBH:45.02,GPQA:28.69,GPQA_Diamond:29.29,GSM8k:74.68,HellaSwag:64.24,HumanEval:52.44,HumanEval+:45.73,IFEval_inst:73.14,IFEval_prompt:65.06,LiveCodeBench:3.13,MATH-500:54.2,MBPP:61.9,MBPP+:55.29,MMLU:56.13,MMLU-Pro:29.71,MMLU-stem:54.94,MTBench:7.79,TruthfulQA:50.06}\n",
      "model:\"Falcon-H1-3B-16k\",size:3.0,group:\"Falcon-H1\",scores:{AIME24:10.0,AIME25:13.33,AMC23:53.91,ARC-C:49.91,Alpaca-Eval:27.64,BBH:53.93,GPQA_Diamond:42.09,GSM8k:84.38,HellaSwag:70.78,HumanEval:76.83,HumanEval+:70.12,IFEval_inst:85.61,IFEval_prompt:79.11,LiveBench:38.87,LiveCodeBench:28.18,MATH-500:75.4,MBPP:78.04,MBPP+:65.08,MMLU:67.59,MMLU-Pro:43.92,MMLU-stem:68.47,MTBench:8.88,Math-Verify:50.0,TruthfulQA:51.19}\n",
      "model:\"Falcon-H1-3B-128k\",size:3.0,group:\"Falcon-H1\",scores:{AIME24:11.88,AIME25:13.33,AMC23:55.63,ARC-C:49.57,Alpaca-Eval:31.09,BBH:53.69,GPQA:33.89,GPQA_Diamond:38.72,GSM8k:84.76,HellaSwag:69.85,HumanEval:76.83,HumanEval+:70.73,IFEval_inst:87.65,IFEval_prompt:82.44,LiveBench:36.86,LiveCodeBench:26.81,MATH-500:74.2,MBPP:79.63,MBPP+:67.46,MMLU:67.17,MMLU-Pro:43.69,MMLU-stem:68.03,MTBench:8.72,Math-Verify:50.53,TruthfulQA:53.19}\n",
      "model:\"Unnamed: 34\",size:None,group:\"Unnamed:\",scores:{}\n",
      "model:\"Qwen3-1.7B\",size:1.7,group:\"Qwen3\",scores:{AGIEval:27.61,AMC23:46.09,ARC-C:34.81,Alpaca-Eval:21.89,BBH:35.18,DROP:8.95,GPQA:27.68,GPQA_Diamond:33.33,GSM8k:69.83,HellaSwag:49.27,HumanEval:67.68,HumanEval+:60.96,IFEval_inst:75.18,IFEval_prompt:66.36,LiveBench:40.73,LiveCodeBench:14.87,MATH (hendrycks):0.0,MATH-500:73.0,MBPP:58.73,MBPP+:49.74,MMLU:51.71,MMLU-Pro:23.54,MMLU-stem:45.42,MTBench:7.61,Math-Verify:40.33,PIQA:67.74,TruthfulQA:49.39}\n",
      "model:\"Qwen2.5 1.5B\",size:1.5,group:\"Qwen2.5\",scores:{AIME24:2.29,AIME25:1.25,ARC-C:40.53,Alpaca-Eval:9.54,BBH:42.41,GPQA:26.26,GPQA_Diamond:25.59,GSM8k:57.47,HellaSwag:62.23,HumanEval:56.1,HumanEval+:50.61,IFEval_inst:50.36,IFEval_prompt:40.3,LiveBench:21.65,LiveCodeBench:12.52,MATH-500:48.4,MBPP:64.81,MBPP+:56.08,MMLU:59.06,MMLU-Pro:28.35,MMLU-stem:51.76,Math-Verify:21.68,TruthfulQA:47.05}\n",
      "model:\"Gemma3-1B\",size:1.0,group:\"Gemma3\",scores:{AIME24:0.42,AIME25:1.25,AMC23:19.22,ARC-C:34.13,Alpaca-Eval:17.87,BBH:35.86,GPQA:28.19,GPQA_Diamond:21.55,GSM8k:42.38,HellaSwag:42.24,HumanEval:40.85,HumanEval+:37.2,IFEval_inst:67.51,IFEval_prompt:55.45,LiveBench:18.79,MATH-500:45.4,MBPP:57.67,MBPP+:50.0,MMLU:36.19,MMLU-Pro:14.46,MMLU-stem:35.39,TruthfulQA:42.17}\n",
      "model:\"Llama3.2 1.2B\",size:1.2,group:\"Llama3.2\",scores:{AIME24:1.46,AIME25:0.0,AMC23:7.19,ARC-C:34.64,Alpaca-Eval:9.38,BBH:33.21,GPQA:26.59,GSM8k:44.28,HellaSwag:55.3,HumanEval:34.15,HumanEval+:29.88,IFEval_inst:61.87,IFEval_prompt:48.8,LiveBench:14.97,LiveCodeBench:2.35,MBPP:33.6,MBPP+:29.37,MMLU:40.47,MMLU-stem:34.06,MTBench:6.37,TruthfulQA:42.08}\n",
      "model:\"SmoLM2 1.7B\",size:1.7,group:\"SmoLM2\",scores:{ARC-C:40.27,BBH:35.65,GPQA:27.77,GPQA_Diamond:23.91,GSM8k:48.45,HellaSwag:66.54,HumanEval:34.76,HumanEval+:28.66,IFEval_inst:60.07,IFEval_prompt:48.43,LiveBench:18.13,MBPP:55.03,MBPP+:44.97,MMLU:49.61,MMLU-Pro:20.58,MMLU-stem:43.22,MTBench:6.49,Math-Verify:5.21,TruthfulQA:45.43}\n",
      "model:\"Falcon3 1.7B\",size:1.7,group:\"Falcon3\",scores:{ARC-C:43.09,Alpaca-Eval:6.98,BBH:34.47,GPQA:26.76,GPQA_Diamond:31.31,GSM8k:44.05,HellaSwag:58.53,HumanEval:22.56,HumanEval+:20.73,IFEval_inst:59.71,IFEval_prompt:48.8,MBPP:20.63,MMLU:45.0,MMLU-Pro:18.49,MMLU-stem:38.62,TruthfulQA:42.31}\n",
      "model:\"Falcon-H1 1.5B-16k\",size:1.5,group:\"Falcon-H1\",scores:{AIME24:11.46,AIME25:7.29,AMC23:47.97,ARC-C:41.89,Alpaca-Eval:26.93,BBH:47.86,GPQA:26.43,GPQA_Diamond:34.01,GSM8k:75.97,HellaSwag:63.24,HumanEval:69.51,HumanEval+:62.2,IFEval_inst:85.37,IFEval_prompt:78.93,LiveBench:34.33,LiveCodeBench:20.74,MATH-500:73.4,MBPP:66.4,MBPP+:57.14,MMLU:61.38,MMLU-Pro:38.3,MMLU-stem:62.2,MTBench:8.64,Math-Verify:44.56,TruthfulQA:46.07}\n",
      "model:\"Falcon-H1 1.5B-128k\",size:1.5,group:\"Falcon-H1\",scores:{AIME24:11.25,AIME25:9.58,AMC23:43.59,ARC-C:42.06,Alpaca-Eval:28.18,BBH:46.47,GPQA:26.34,GPQA_Diamond:35.19,GSM8k:74.98,HellaSwag:63.33,HumanEval:68.29,HumanEval+:61.59,IFEval_inst:84.05,IFEval_prompt:77.26,LiveBench:34.13,LiveCodeBench:17.61,MATH-500:74.0,MBPP:64.81,MBPP+:56.35,MMLU:61.15,MMLU-Pro:37.8,MMLU-stem:62.1,MTBench:8.46,Math-Verify:43.96,TruthfulQA:45.98}\n",
      "model:\"Falcon-H1 1.5B-deep-16k\",size:1.5,group:\"Falcon-H1\",scores:{AIME25:14.17,AMC23:57.5,ARC-C:44.2,Alpaca-Eval:24.79,BBH:54.45,GPQA:31.88,GPQA_Diamond:38.22,GSM8k:82.03,HellaSwag:65.4,HumanEval:76.22,HumanEval+:68.9,IFEval_inst:87.29,IFEval_prompt:82.07,LiveBench:36.86,LiveCodeBench:24.27,MATH-500:76.6,MBPP:67.72,MBPP+:56.61,MMLU:65.08,MMLU-Pro:41.4,MMLU-stem:66.25,MTBench:8.43,Math-Verify:48.79,TruthfulQA:51.19}\n",
      "model:\"Falcon-H1 1.5B-deep-128k\",size:1.5,group:\"Falcon-H1\",scores:{AIME24:14.37,AMC23:56.56,ARC-C:43.86,BBH:54.43,GPQA:33.22,GPQA_Diamond:40.57,GSM8k:82.34,HellaSwag:65.54,HumanEval:73.78,HumanEval+:68.9,IFEval_inst:86.21,IFEval_prompt:80.78,LiveBench:36.83,LiveCodeBench:23.87,MATH-500:77.8,MBPP:68.25,MBPP+:56.61,MMLU:65.1,MMLU-Pro:41.89,MMLU-stem:66.14,MTBench:8.53,Math-Verify:46.6,TruthfulQA:50.48}\n",
      "model:\"Unnamed: 45\",size:None,group:\"Unnamed:\",scores:{}\n",
      "model:\"Qwen3-0.6B\",size:0.6,group:\"Qwen3\",scores:{AGIEval:26.71,AIME24:2.71,AIME25:1.67,AMC23:27.97,ARC-C:31.06,Alpaca-Eval:9.59,BBH:32.95,GSM8k:42.61,HellaSwag:42.17,HumanEval:41.46,HumanEval+:37.19,IFEval_inst:67.39,IFEval_prompt:56.93,LiveBench:27.78,LiveCodeBench:9.78,MATH (hendrycks):0.02,MATH-500:46.0,MBPP:56.08,MBPP+:47.08,MMLU:38.79,MMLU-Pro:16.95,MMLU-stem:33.94,MTBench:5.75,Math-Verify:18.73,PIQA:66.38,TruthfulQA:51.65}\n",
      "model:\"Qwen2.5 0.5B\",size:0.5,group:\"Qwen2.5\",scores:{AIME24:0.62,AIME25:0.21,ARC-C:33.28,Alpaca-Eval:3.26,BBH:33.26,GPQA:26.85,GPQA_Diamond:24.24,GSM8k:29.45,HellaSwag:52.38,HumanEval:36.59,HumanEval+:32.32,IFEval_inst:37.05,IFEval_prompt:27.17,LiveBench:14.27,LiveCodeBench:2.94,MBPP:46.83,MBPP+:39.68,MMLU:46.3,MMLU-Pro:18.73,MMLU-stem:40.16,MTBench:4.71,TruthfulQA:46.19,Winogrande:56.27}\n",
      "model:\"Falcon-H1-0.5B-16k\",size:0.5,group:\"Falcon-H1\",scores:{AIME24:3.75,AIME25:4.38,AMC23:33.13,ARC-C:37.8,Alpaca-Eval:10.79,BBH:42.91,GPQA:29.95,GPQA_Diamond:27.95,GSM8k:68.39,HellaSwag:51.93,HumanEval:51.83,HumanEval+:45.12,IFEval_inst:76.86,IFEval_prompt:67.28,MATH-500:58.4,MBPP:42.59,MBPP+:33.07,MMLU:50.81,MMLU-stem:50.16,Math-Verify:31.19,PIQA:69.48,Triviaqa:0.25,TruthfulQA:44.12}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1.  Load / point to your dataframe\n",
    "# -------------------------------------------------\n",
    "# df = pd.read_csv(\"your_file.csv\")      # ⬅ or reuse the frame already in RAM\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2.  Helper: model‑name → (size_in_B, group_name)\n",
    "# -------------------------------------------------\n",
    "def parse_model_name(name: str):\n",
    "    m = re.search(r\"([\\d.]+)\\s*([BM])\", name, re.I)\n",
    "    size = None\n",
    "    if m:\n",
    "        num, unit = m.groups()\n",
    "        size = float(num)\n",
    "        if unit.upper() == \"M\":          # convert M ➜ billions\n",
    "            size /= 1_000\n",
    "\n",
    "    # group = everything before the first digit or space‐digit combo\n",
    "    group = re.split(r\"\\s|-(?=\\d)\", name.strip())[0]\n",
    "    return size, group\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3.  Build the benchmark‑row index automatically\n",
    "# -------------------------------------------------\n",
    "first_col = df.columns[0]\n",
    "\n",
    "all_benchmarks = (\n",
    "    df[first_col]\n",
    "    .dropna()\n",
    "    .loc[lambda s: ~s.str.endswith(\"Tasks\")]      # drop category headers\n",
    ")\n",
    "\n",
    "# series: label ➜ list(row_indices)\n",
    "bench_rows = all_benchmarks.groupby(all_benchmarks).apply(lambda x: x.index.tolist())\n",
    "\n",
    "# how to aggregate duplicates?\n",
    "handle_duplicates = \"mean\"        # [\"mean\", \"first\", \"last\", \"max\", \"min\"]\n",
    "\n",
    "def aggregate(values):\n",
    "    if handle_duplicates == \"first\":\n",
    "        return values[0]\n",
    "    if handle_duplicates == \"last\":\n",
    "        return values[-1]\n",
    "    if handle_duplicates == \"max\":\n",
    "        return max(values)\n",
    "    if handle_duplicates == \"min\":\n",
    "        return min(values)\n",
    "    return sum(values) / len(values)              # default: mean\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4.  Produce the string for every model column\n",
    "# -------------------------------------------------\n",
    "model_columns = df.columns[2:]                    # skip meta columns\n",
    "\n",
    "model_strings = []\n",
    "\n",
    "for col in model_columns:\n",
    "    size, group = parse_model_name(col)\n",
    "\n",
    "    # numeric version of the column\n",
    "    numeric = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # gather every benchmark score, keeping label unchanged\n",
    "    scores = {}\n",
    "    for label, idx_list in bench_rows.items():\n",
    "        vals = numeric.loc[idx_list].dropna().tolist()\n",
    "        if vals:                                  # at least one number present\n",
    "            scores[label] = round(aggregate(vals), 2)\n",
    "\n",
    "    # stringify\n",
    "    score_blob = \",\".join(f\"{k}:{v}\" for k, v in scores.items())\n",
    "    model_strings.append(\n",
    "        f'model:\"{col}\",size:{size},group:\"{group}\",scores:{{{score_blob}}}'\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5.  Use the result\n",
    "# -------------------------------------------------\n",
    "for s in model_strings:\n",
    "    print(s.replace(\"'\", '')#.replace('AIME 2024', 'AIME24').replace('AIME2025', 'AIME25')\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\"Qwen3-32B\",size:32.0,group:\"Qwen3\",scores:{Avg:58.46,MMLU:80.89,ARC:48.98}\n",
      "model:\"Qwen2.5 72B\",size:72.0,group:\"Qwen2.5\",scores:{Avg:63.78,MMLU:84.42,ARC:46.59}\n",
      "model:\"Qwen2.5 32B\",size:32.0,group:\"Qwen2.5\",scores:{Avg:62.35,MMLU:82.8,ARC:44.54}\n",
      "model:\"Mistral-Small-3.1-24B\",size:24.0,group:\"Mistral-Small\",scores:{Avg:77.17,MMLU:81.14}\n",
      "model:\"Gemma3-27B\",size:27.0,group:\"Gemma3\",scores:{Avg:64.14,MMLU:78.01,ARC:54.52}\n",
      "model:\"Llama3.1 70B\",size:70.0,group:\"Llama3.1\",scores:{Avg:60.3,MMLU:83.19,ARC:63.65}\n",
      "model:\"Llama4-scout\",size:None,group:\"Llama4-scout\",scores:{Avg:61.31,MMLU:80.4,ARC:56.14}\n",
      "model:\"Falcon3.1 74B\",size:74.0,group:\"Falcon3.1\",scores:{Avg:63.43,MMLU:82.33,ARC:63.4}\n",
      "model:\"GTP4o (mini)\",size:None,group:\"GTP4o\",scores:{Avg:nan}\n",
      "model:\"gpt-4o-mini-2024-07-18\",size:None,group:\"gpt\",scores:{Avg:nan}\n",
      "model:\"Falcon-H1-34B-16k\",size:34.0,group:\"Falcon-H1\",scores:{Avg:66.95,MMLU:84.18,ARC:61.26}\n",
      "model:\"Falcon-H1-34B-128k\",size:34.0,group:\"Falcon-H1\",scores:{Avg:67.09,MMLU:84.05,ARC:61.01}\n",
      "model:\"Unnamed: 15\",size:None,group:\"Unnamed:\",scores:{Avg:nan}\n",
      "model:\"Qwen3-14B\",size:14.0,group:\"Qwen3\",scores:{Avg:55.72,MMLU:76.81,ARC:42.41}\n",
      "model:\"Qwen2.5 14B\",size:14.0,group:\"Qwen2.5\",scores:{Avg:61.99,MMLU:78.76,ARC:50.17}\n",
      "model:\"Qwen3-8B\",size:8.0,group:\"Qwen3\",scores:{Avg:51.67,MMLU:71.56,ARC:42.06}\n",
      "model:\"Qwen2.5 7B\",size:7.0,group:\"Qwen2.5\",scores:{Avg:54.23,MMLU:73.64,ARC:41.38}\n",
      "model:\"Gemma3-12B\",size:12.0,group:\"Gemma3\",scores:{Avg:57.33,MMLU:72.5,ARC:51.96}\n",
      "model:\"Llama3.1 8B\",size:8.0,group:\"Llama3.1\",scores:{Avg:45.01,MMLU:68.67,ARC:52.39}\n",
      "model:\"Falcon3 7B\",size:7.0,group:\"Falcon3\",scores:{Avg:52.49,MMLU:70.81,ARC:54.35}\n",
      "model:\"Falcon3 10B\",size:10.0,group:\"Falcon3\",scores:{Avg:54.87,MMLU:74.01,ARC:54.44}\n",
      "model:\"Falcon-H1-7B-16k\",size:7.0,group:\"Falcon-H1\",scores:{Avg:60.54,MMLU:76.73,ARC:60.49}\n",
      "model:\"Falcon-H1-7B-128k\",size:7.0,group:\"Falcon-H1\",scores:{Avg:61.53,MMLU:76.83,ARC:59.98}\n",
      "model:\"Unnamed: 26\",size:None,group:\"Unnamed:\",scores:{Avg:nan}\n",
      "model:\"Qwen3-4B\",size:4.0,group:\"Qwen3\",scores:{Avg:51.14,MMLU:67.01,ARC:37.71}\n",
      "model:\"Qwen2.5 3B\",size:3.0,group:\"Qwen2.5\",scores:{Avg:46.25,MMLU:65.09,ARC:43.77}\n",
      "model:\"Gemma3-4B\",size:4.0,group:\"Gemma3\",scores:{Avg:45.74,MMLU:59.53,ARC:44.88}\n",
      "model:\"Llama3.2 3B\",size:3.0,group:\"Llama3.2\",scores:{Avg:41.48,MMLU:61.74,ARC:44.88}\n",
      "model:\"Falcon3 3B\",size:3.0,group:\"Falcon3\",scores:{Avg:42.46,MMLU:56.76,ARC:48.21}\n",
      "model:\"Falcon-H1-3B-16k\",size:3.0,group:\"Falcon-H1\",scores:{Avg:54.97,MMLU:68.66,ARC:49.91}\n",
      "model:\"Falcon-H1-3B-128k\",size:3.0,group:\"Falcon-H1\",scores:{Avg:54.5,MMLU:68.3,ARC:49.57}\n",
      "model:\"Unnamed: 34\",size:None,group:\"Unnamed:\",scores:{Avg:nan}\n",
      "model:\"Qwen3-1.7B\",size:1.7,group:\"Qwen3\",scores:{Avg:42.92,MMLU:57.04,ARC:34.81}\n",
      "model:\"Qwen2.5 1.5B\",size:1.5,group:\"Qwen2.5\",scores:{Avg:39.49,MMLU:59.76,ARC:40.53}\n",
      "model:\"Gemma3-1B\",size:1.0,group:\"Gemma3\",scores:{Avg:33.93,MMLU:40.87,ARC:34.13}\n",
      "model:\"Llama3.2 1.2B\",size:1.2,group:\"Llama3.2\",scores:{Avg:28.89,MMLU:45.93,ARC:34.64}\n",
      "model:\"SmoLM2 1.7B\",size:1.7,group:\"SmoLM2\",scores:{Avg:37.9,MMLU:49.56,ARC:40.27}\n",
      "model:\"Falcon3 1.7B\",size:1.7,group:\"Falcon3\",scores:{Avg:35.87,MMLU:46.1,ARC:43.09}\n",
      "model:\"Falcon-H1 1.5B-16k\",size:1.5,group:\"Falcon-H1\",scores:{Avg:48.73,MMLU:62.66,ARC:41.89}\n",
      "model:\"Falcon-H1 1.5B-128k\",size:1.5,group:\"Falcon-H1\",scores:{Avg:48.21,MMLU:62.03,ARC:42.06}\n",
      "model:\"Falcon-H1 1.5B-deep-16k\",size:1.5,group:\"Falcon-H1\",scores:{Avg:53.91,MMLU:66.04,ARC:44.2}\n",
      "model:\"Falcon-H1 1.5B-deep-128k\",size:1.5,group:\"Falcon-H1\",scores:{Avg:54.96,MMLU:66.11,ARC:43.86}\n",
      "model:\"Unnamed: 45\",size:None,group:\"Unnamed:\",scores:{Avg:nan}\n",
      "model:\"Qwen3-0.6B\",size:0.6,group:\"Qwen3\",scores:{Avg:32.57,MMLU:42.98,ARC:31.06}\n",
      "model:\"Qwen2.5 0.5B\",size:0.5,group:\"Qwen2.5\",scores:{Avg:29.56,MMLU:46.07,ARC:33.28}\n",
      "model:\"Falcon-H1-0.5B-16k\",size:0.5,group:\"Falcon-H1\",scores:{Avg:41.32,MMLU:53.4,ARC:37.8}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1.  Load or reuse your existing dataframe\n",
    "# -------------------------------------------\n",
    "# df = pd.read_csv(\"your_file.csv\")           # ⬅ if coming from CSV\n",
    "# (or) df = your_dataframe_variable           # ⬅ if it’s already in memory\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2.  Helper to pull size (# of B or M) and model‑family (“group”) info\n",
    "# --------------------------------------------------------------------\n",
    "def parse_model_name(name: str):\n",
    "    # normalise whitespace\n",
    "    name = name.strip().replace(\"  \", \" \")\n",
    "\n",
    "    # ----- size -----\n",
    "    m = re.search(r\"([\\d.]+)\\s*([BM])\", name, re.I)\n",
    "    if m:\n",
    "        num, unit = m.groups()\n",
    "        size = float(num)\n",
    "        if unit.upper() == \"M\":          # convert M → B so everything is in billions\n",
    "            size /= 1_000\n",
    "    else:\n",
    "        size = None                      # leave as None if pattern is missing\n",
    "\n",
    "    # ----- group / family -----\n",
    "    # “Falcon-H1-34B”   ➜ “Falcon-H1”\n",
    "    # “Qwen2.5 72B”     ➜ “Qwen2.5”\n",
    "    group = re.split(r\"\\s|-(?=\\d)\", name)[0]   # stop before the first digit/space\n",
    "\n",
    "    return size, group\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3.  Pick which rows correspond to the single‑task scores\n",
    "#     you want to surface (feel free to tweak these labels)\n",
    "# ----------------------------------------------------------\n",
    "TASK_ROWS = {\n",
    "    \"MMLU\":  \"MMLU\",      # label seen in first column\n",
    "    \"ARC\":   \"ARC-C\"      # row label for ARC‑Challenge (zero‑shot)\n",
    "}\n",
    "\n",
    "# Quick look‑up Series:  label ➜ row‑index\n",
    "task_idx = {k: int(df[df.iloc[:, 0] == v].index[0]) for k, v in TASK_ROWS.items()}\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4.  Assemble the dictionary & stringify it\n",
    "# ----------------------------------------------------------\n",
    "first_col, shots_col = df.columns[:2]           # easier handles\n",
    "\n",
    "model_columns = [c for c in df.columns[2:]      # skip first two meta‑columns\n",
    "                 if not c.lower().startswith(\"final\")]  # ignore helper cols\n",
    "\n",
    "model_strings = []\n",
    "\n",
    "for col in model_columns:\n",
    "    size, group = parse_model_name(col)\n",
    "\n",
    "    # numeric scores for that model\n",
    "    col_numeric = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    scores = {\n",
    "        \"Avg\":  round(col_numeric.mean(skipna=True), 2),\n",
    "        \"MMLU\": round(col_numeric.iloc[task_idx[\"MMLU\"]], 2)\n",
    "                 if pd.notna(col_numeric.iloc[task_idx[\"MMLU\"]]) else None,\n",
    "        \"ARC\":  round(col_numeric.iloc[task_idx[\"ARC\"]], 2)\n",
    "                 if pd.notna(col_numeric.iloc[task_idx[\"ARC\"]]) else None,\n",
    "    }\n",
    "\n",
    "    # keep only the non‑missing entries inside the braces\n",
    "    score_blob = \",\".join(f\"{k}:{v}\" for k, v in scores.items() if v is not None)\n",
    "\n",
    "    model_strings.append(\n",
    "        f'model:\"{col}\",size:{size},group:\"{group}\",scores:{{{score_blob}}}'\n",
    "    )\n",
    "\n",
    "# -------------------------------------------\n",
    "# 5.  Consume the output however you like\n",
    "# -------------------------------------------\n",
    "for s in model_strings:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
