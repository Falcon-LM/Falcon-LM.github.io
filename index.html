<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.145.0"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script>!function(e,t){if(!e.rdt){var s,o,n=e.rdt=function(){n.sendEvent?n.sendEvent.apply(n,arguments):n.callQueue.push(arguments)};n.callQueue=[],s=t.createElement("script"),s.src="https://www.redditstatic.com/ads/pixel.js",s.async=!0,o=t.getElementsByTagName("script")[0],o.parentNode.insertBefore(s,o)}}(window,document),rdt("init","a2_f0we4sxffrnm"),rdt("track","PageVisit")</script><script async src="https://www.googletagmanager.com/gtag/js?id=AW-16575356988"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","AW-16575356988")</script><title>Falcon</title>
<meta name=keywords content="multi-modal,machine learning,blog"><meta name=description content="Pioneering the Next Generation of Language Models"><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/json href=https://falcon-lm.github.io/index.json><link rel=alternate hreflang=en href=https://falcon-lm.github.io/><link rel=alternate hreflang=ar href=https://falcon-lm.github.io/ar/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Falcon"><meta property="og:description" content="Pioneering the Next Generation of Language Models"><meta property="og:type" content="website"><meta property="og:url" content="https://falcon-lm.github.io/"><meta property="og:image" content="https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Falcon"><meta name=twitter:description content="Pioneering the Next Generation of Language Models"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Falcon","url":"https://falcon-lm.github.io/","description":"Falcon","thumbnailUrl":"https://falcon-lm.github.io/img/favicon.png","sameAs":[]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container style=min-height:100vh;display:flex;justify-content:center;background-color:#000><img class=hero-background style=opacity:0 onload="this.style.opacity=1" src=img/large-bg.webp width=100%><div class=hero-gradient></div><div class=scroll-indicator><svg class="scroll-icon" viewBox="0 0 24 24" fill="none" stroke="rgba(255,255,255,0.7)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><polyline points="6 9 12 15 18 9"/></svg></div><style>body{-ms-overflow-style:none;scrollbar-width:none}body::-webkit-scrollbar{display:none}.scroll-indicator{position:fixed;bottom:5vh;left:50%;transform:translateX(-50%);display:flex;flex-direction:column;align-items:center;opacity:.7;transition:opacity .3s ease;z-index:10}.scroll-indicator:hover{opacity:1}.scroll-icon{width:48px;height:48px;margin-bottom:8px;animation:bounce 1.5s ease-in-out infinite}@keyframes bounce{0%,100%{transform:translateY(0)}50%{transform:translateY(10px)}}@media(max-width:768px){.scroll-icon{width:36px;height:36px}.scroll-text{font-size:10px}}</style><div class="hero text-light text-fade-in"><div class=hero-header><h1>Falcon</h1></div><div class=hero-content>Pioneering the Next Generation of Language Models</div><div class=hero-footer><div class=social-icons></div></div></div></div><main class="main home"><article class=post-entry><header class=entry-header><h2>Falcon-Arabic: A Breakthrough in Arabic Language Models</h2></header><div class=entry-content><p>Check out the Arabic version translated by Falcon-Arabic
We are excited to introduce Falcon-Arabic, a 7B parameter Language Model that sets a new benchmark for Arabic NLP. Built on the Falcon 3 architecture, Falcon-Arabic is a multilingual model that supports Arabic, English, and several other languages. It excels in general knowledge, Arabic grammar, mathematical reasoning, complex problem solving, and understanding the rich diversity of Arabic dialects. Falcon-Arabic supports a context length of 32,000 tokens, allowing it to handle long documents and enabling advanced applications like retrieval-augmented generation (RAG), in-depth content creation, and knowledge-intensive tasks.
...</p></div><footer class=entry-footer><span title='2025-05-21 12:00:00 +0000 UTC'>May 21, 2025</span>&nbsp;•&nbsp;7 min&nbsp;•&nbsp;1384 words&nbsp;•&nbsp;Falcon Team</footer><a class=entry-link aria-label="post link to Falcon-Arabic: A Breakthrough in Arabic Language Models" href=https://falcon-lm.github.io/blog/falcon-arabic/></a></article><article class=post-entry><header class=entry-header><h2>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance</h2></header><div class=entry-content><p>Falcon CHAT Hugging Face Github DEMO DISCORD
Introduction Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers.
...</p></div><footer class=entry-footer><span title='2025-05-20 12:00:00 +0000 UTC'>May 20, 2025</span>&nbsp;•&nbsp;11 min&nbsp;•&nbsp;2277 words&nbsp;•&nbsp;Falcon Team</footer><a class=entry-link aria-label="post link to Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance" href=https://falcon-lm.github.io/blog/falcon-h1/></a></article><article class=post-entry><header class=entry-header><h2>Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.</h2></header><div class=entry-content><p>In this blogpost, we present the key highlights and rationales about the Falcon-Edge series - a collection of powerful, universal, and fine-tunable language models available in ternary format, based on the BitNet architecture.
Drawing from our experience with BitNet, Falcon-Edge introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.
...</p></div><footer class=entry-footer><span title='2025-05-15 12:00:00 +0000 UTC'>May 15, 2025</span>&nbsp;•&nbsp;12 min&nbsp;•&nbsp;2477 words&nbsp;•&nbsp;Falcon Team</footer><a class=entry-link aria-label="post link to Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models." href=https://falcon-lm.github.io/blog/falcon-edge/></a></article><article class=post-entry><header class=entry-header><h2>Welcome to the Falcon 3 Family of Open Models!</h2></header><div class=entry-content><p>Falcon CHAT Hugging Face DEMO DISCORD
Welcome to the Falcon 3 Family of Open Models! We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by Technology Innovation Institute (TII) in Abu Dhabi. By pushing the boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open and accessible large foundation models.
Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models’ science, math, and code capabilities.
...</p></div><footer class=entry-footer><span title='2024-12-17 12:00:00 +0000 UTC'>December 17, 2024</span>&nbsp;•&nbsp;6 min&nbsp;•&nbsp;1260 words&nbsp;•&nbsp;Falcon Team</footer><a class=entry-link aria-label="post link to Welcome to the Falcon 3 Family of Open Models!" href=https://falcon-lm.github.io/blog/falcon-3/></a></article><article class=post-entry><header class=entry-header><h2>Welcome Falcon Mamba: The first strong attention-free 7B model</h2></header><div class=entry-content><p>Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.
In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.
...</p></div><footer class=entry-footer><span title='2024-08-12 12:00:00 +0000 UTC'>August 12, 2024</span>&nbsp;•&nbsp;7 min&nbsp;•&nbsp;1463 words&nbsp;•&nbsp;Falcon Team</footer><a class=entry-link aria-label="post link to Welcome Falcon Mamba: The first strong attention-free 7B model" href=https://falcon-lm.github.io/blog/falcon-mamba/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://falcon-lm.github.io/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span><a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html rel="noopener noreferrer" target=_blank>| Terms and Conditions</a>
</span><span><a href=https://www.tii.ae/privacy-policy rel="noopener noreferrer" target=_blank>| Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>