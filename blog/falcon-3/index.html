<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Welcome to the Falcon 3 Family of Open Models! | Falcon</title>
<meta name=keywords content><meta name=description content="Falcon CHAT
Hugging Face
DEMO
DISCORD
Welcome to the Falcon 3 Family of Open Models!
We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by
Technology Innovation Institute (TII) in Abu Dhabi. By pushing the
boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open
and accessible large foundation models.
Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models&rsquo; science, math, and code capabilities."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/blog/falcon-3/><link crossorigin=anonymous href=/assets/css/stylesheet.0007972aa88c286f9b208aff0933f59a4ee385208a527e77ed36f81345f384f7.css integrity="sha256-AAeXKqiMKG+bIIr/CTP1mk7jhSCKUn537Tb4E0XzhPc=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/blog/falcon-3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Welcome to the Falcon 3 Family of Open Models!"><meta property="og:description" content="Falcon CHAT
Hugging Face
DEMO
DISCORD
Welcome to the Falcon 3 Family of Open Models!
We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by
Technology Innovation Institute (TII) in Abu Dhabi. By pushing the
boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open
and accessible large foundation models.
Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models&rsquo; science, math, and code capabilities."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/blog/falcon-3/"><meta property="og:image" content="https://falcon-lm.github.io/blog/falcon-3/falcon3-family-logo.svg"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-12-17T12:00:00+00:00"><meta property="article:modified_time" content="2024-12-17T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/blog/falcon-3/falcon3-family-logo.svg"><meta name=twitter:title content="Welcome to the Falcon 3 Family of Open Models!"><meta name=twitter:description content="Falcon CHAT
Hugging Face
DEMO
DISCORD
Welcome to the Falcon 3 Family of Open Models!
We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by
Technology Innovation Institute (TII) in Abu Dhabi. By pushing the
boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open
and accessible large foundation models.
Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models&rsquo; science, math, and code capabilities."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://falcon-lm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Welcome to the Falcon 3 Family of Open Models!","item":"https://falcon-lm.github.io/blog/falcon-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Welcome to the Falcon 3 Family of Open Models!","name":"Welcome to the Falcon 3 Family of Open Models!","description":"Falcon CHAT Hugging Face DEMO DISCORD\nWelcome to the Falcon 3 Family of Open Models! We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by Technology Innovation Institute (TII) in Abu Dhabi. By pushing the boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open and accessible large foundation models.\nFalcon3 represents a natural evolution from previous releases, emphasizing expanding the models\u0026rsquo; science, math, and code capabilities.\n","keywords":[],"articleBody":"Falcon CHAT Hugging Face DEMO DISCORD\nWelcome to the Falcon 3 Family of Open Models! We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by Technology Innovation Institute (TII) in Abu Dhabi. By pushing the boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open and accessible large foundation models.\nFalcon3 represents a natural evolution from previous releases, emphasizing expanding the models’ science, math, and code capabilities.\nThis iteration includes five base models:\nFalcon3-1B-Base Falcon3-3B-Base Falcon3-Mamba-7B-Base Falcon3-7B-Base Falcon3-10B-Base In developing these models, we incorporated several key innovations aimed at improving the models’ performances while reducing training costs:\nOne pre-training for transformer-based models: We conducted a single large-scale pretraining run on the 7B model, using 1024 H100 GPU chips, leveraging 14 trillion tokens featuring web, code, STEM, and curated high-quality and multilingual data. Depth up-scaling for improved reasoning: Building on recent studies on the effects of model depth, we upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2 trillion tokens of high-quality data. This yielded Falcon3-10B-Base which achieves state-of-the-art zero-shot and few-shot performance for models under 13B parameters. Knowledge distillation for better tiny models: To provide compact and efficient alternatives, we developed Falcon3-1B-Base and Falcon3-3B-Base by leveraging pruning and knowledge distillation techniques, using less than 100GT of curated high-quality data, thereby redefining pre-training efficiency. Pure SSM: We have further enhanced Falcon Mamba 7B by training on an additional 1.5 trillion tokens of high-quality data, resulting in Falcon3-Mamba-7B-Base. Notably, the updated model offers significantly improved reasoning and mathematical capabilities. Other variants: All models in the Falcon3 family are available in variants such as Instruct, GGUF, GPTQ-Int4, GPTQ-Int8, AWQ, and 1.58-bit, offering flexibility for a wide range of applications. Key Highlights Falcon3 featured the limits within the small and medium scales of large language models by demonstrating high performance on common benchmarks:\nFalcon3-1B-Base surpasses SmolLM2-1.7B and is on par with gemma-2-2b. Falcon3-3B-Base outperforms larger models like Llama-3.1-8B and Minitron-4B-Base, highlighting the benefits of pre-training with knowledge distillation. Falcon3-7B-Base demonstrates top performance, on par with Qwen2.5-7B, among models under the 9B scale. Falcon3-10B-Base stands as the state-of-the-art achieving strong results in the under-13B category. All the transformer-based Falcon3 models are compatible with Llama architecture allowing better integration in the AI ecosystem. Falcon3-Mamba-7B continues to lead as the top-performing State Space Language Model (SSLM), matching or even surpassing leading transformer-based LLMs at the 7B scale, along with support for a longer 32K context length. Having the same architecture as the original Falcon Mamba 7B, users can integrate Falcon3-Mamba-7B seamlessly without any additional effort. The instruct versions of our collection of base models further show remarkable performance across various benchmarks with Falcon3-7B-Instruct and Falcon3-10B-Instruct outperforming all instruct models under the 13B scale on the open leaderboard. Enhanced Capabilities We evaluated models with our internal evaluation pipeline (based on lm-evaluation-harness) and we report raw scores. Our evaluations highlight key areas where the Falcon3 family of models excel, reflecting the emphasis on enhancing performance in scientific domains, reasoning, and general knowledge capabilities:\nMath Capabilities: Falcon3-10B-Base achieves 22.9 on MATH-Lvl5 and 83.0 on GSM8K, showcasing enhanced reasoning in complex math-focused tasks. Coding Capabilities: Falcon3-10B-Base achieves 73.8 on MBPP, while Falcon3-10B-Instruct scores 45.8 on Multipl-E, reflecting their abilities to generalize across programming-related tasks. Extended Context Length: Models in the Falcon3 family support up to 32k tokens (except the 1B supporting up to 8k context), with functional improvements such as scoring 86.3 on BFCL (Falcon3-10B-Instruct). Improved Reasoning: Falcon3-7B-Base and Falcon3-10B-Base achieve 51.0 and 59.7 on BBH, reflecting enhanced reasoning capabilities, with the 10B model showing improved reasoning performance over the 7B. Scientific Knowledge Expansion: Performance on MMLU benchmarks demonstrates advances in specialized knowledge, with scores of 67.4/39.2 (MMLU/MMLU-PRO) for Falcon3-7B-Base and 73.1/42.5 (MMLU/MMLU-PRO) for Falcon3-10B-Base respectively. Models’ Specs and Benchmark Results Detailed specifications of the Falcon3 family of models are summarized in the following table. The architecture of Falcon3-7B-Base is characterized by a head dimension of 256 which yields high throughput when using FlashAttention-3 as it is optimized for this dimension. These decoder-only models span 18 to 40 layers for the transformer-based ones, and 64 layers for the mamba one, all models share the SwiGLU activation function, with vocabulary size of 131K tokens (65Kfor Mamba-7B). The Falcon3-7B-Base is trained on the largest amount of data ensuring comprehensive coverage of concepts and knowledge, the other variants require way less data.\nThe table below highlights the performances of Falcon3-7B-Base and Falcon3-10B-Base on key benchmarks showing competitive performances in general, math, reasoning, and common sense understanding domains. Feel free to take a look at models’ cards where we provide additional evaluation results (e.g. MT-Bench, Alpaca, etc). The instruct models also demonstrate competitive and super performances with equivalent and small-size models as highlighted in the tables below.\nInstruct models Falcon3-1B-Instruct and Falcon3-3B-Instruct achieve robust performance across the evaluated benchmarks. Specifically, Falcon3-1B attains competitive results in IFEval (54.4), MUSR (40.7), and SciQ (86.8), while Falcon3-3B exhibits further gains—particularly in MMLU-PRO (29.7) and MATH (19.9)—demonstrating clear scaling effects. Although they do not surpass all competing models on every metric, Falcon models show strong performances in reasoning and common-sense understanding relative to both Qwen and Llama. In our internal evaluation pipeline:\nWe use lm-evaluation harness. We report raw scores obtained by applying chat template without fewshot_as_multiturn (unlike Llama3.1). We use same batch-size across all models. Furthermore, Falcon3-7B and Falcon3-10B show robust performance across the evaluated benchmarks. Falcon3-7B achieves competitive scores on reasoning (Arc Challenge: 65.9, MUSR: 46.4) and math (GSM8K: 79.1), while Falcon3-10B demonstrates further improvements, notably in GSM8K (83.1) and IFEval (78), indicating clear scaling benefits.\rOpen Source Commitment In line with our mission to foster AI accessibility and collaboration, all models in the Falcon3 family are released under the Falcon LLM license. We hope the AI community finds these models valuable for research, application development, and further experimentation. Falcon3 is not a culmination but a continuation of our efforts to create more capable, efficient, specialized foundation models. In January 2025, we will further release other models of the Falcon3 family featuring enhanced multi-modal capabilities including image, video, and audio support, as well as a full technical report covering our methodologies. We welcome feedback and collaboration from the community as we continue to refine and advance these technologies.\nUseful links Access to our models (including GGUF and 1.58bit models) of this series through the Falcon3 HuggingFace collection. Feel free to join our discord server if you have any questions or to interact with our researchers and developers. Check out the Falcon-LLM License link for more details about the license. Refer to the official Open LLM Leaderboard for HF evaluations of our models. Acknowledgments We warmly thank the following people for their smooth support and integration within the ecosystem.\nAlina Lozovskaya and Clementine Fourrier for helping us evaluate the model on the HF leaderboard. Cyril Vallez and Arthur Zucker for the transformers documentation integration. Vaibhav Srivastav and Aritra Roy Gosthipaty for his help reviewing this blogpost. Georgi Gerganov for his help in integrating an important fix to make Falcon3 series models work in llama.cpp. Awni Hannun for helping us review necessary changes in order to integrate Falcon3 series into MLX ecosystem. BitNet.cpp team for helping us integrating 1.58bit variants of Falcon3 models into BitNet. Citation If the Falcon3 family of models were helpful to your work, feel free to give us a cite.\n@misc{Falcon3,\rtitle = {The Falcon 3 Family of Open Models},\rurl = {https://huggingface.co/blog/falcon3},\rauthor = {Falcon-LLM Team},\rmonth = {December},\ryear = {2024}\r} ","wordCount":"1260","inLanguage":"en","image":"https://falcon-lm.github.io/blog/falcon-3/falcon3-family-logo.svg","datePublished":"2024-12-17T12:00:00Z","dateModified":"2024-12-17T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/blog/falcon-3/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:url(falcon3-family-logo.svg)50%/cover no-repeat fixed"></div><div class=hero-gradient></div><div class=hero-blur></div><div class="hero text-light"><h1 class=post-title>Welcome to the Falcon 3 Family of Open Models!</h1><div class=post-meta><span title='2024-12-17 12:00:00 +0000 UTC'>December 17, 2024</span>&nbsp;•&nbsp;6 min&nbsp;•&nbsp;1260 words&nbsp;•&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><figure class=entry-cover><a href=/blog/falcon-3/falcon3-family-logo.svg target=_blank rel="noopener noreferrer"><img loading=lazy src=/blog/falcon-3/falcon3-family-logo.svg alt></a></figure><div class=post-content><p><a href=https://chat.falconllm.tii.ae class="btn external" target=_blank>Falcon CHAT</a>
<a href=https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026 class="btn external" target=_blank>Hugging Face</a>
<a href=https://huggingface.co/spaces/tiiuae/Falcon3-demo class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/vfw6k2G3 class="btn external" target=_blank>DISCORD</a></p><h1 id=welcome-to-the-falcon-3-family-of-open-models>Welcome to the Falcon 3 Family of Open Models!<a hidden class=anchor aria-hidden=true href=#welcome-to-the-falcon-3-family-of-open-models>#</a></h1><p>We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by
<a href=https://www.tii.ae/ai-and-digital-science>Technology Innovation Institute (TII)</a> in Abu Dhabi. By pushing the
boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open
and accessible large foundation models.</p><p>Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models&rsquo; science, math, and code capabilities.</p><p>This iteration includes five base models:</p><ol><li><a href=https://huggingface.co/tiiuae/Falcon3-1B-Base>Falcon3-1B-Base</a></li><li><a href=https://huggingface.co/tiiuae/Falcon3-3B-Base>Falcon3-3B-Base</a></li><li><a href=https://huggingface.co/tiiuae/Falcon3-Mamba-7B-Base>Falcon3-Mamba-7B-Base</a></li><li><a href=https://huggingface.co/tiiuae/Falcon3-7B-Base>Falcon3-7B-Base</a></li><li><a href=https://huggingface.co/tiiuae/Falcon3-10B-Base>Falcon3-10B-Base</a></li></ol><p>In developing these models, we incorporated several key innovations aimed at improving the models&rsquo; performances while reducing training costs:</p><ul><li><strong>One pre-training for transformer-based models:</strong> We conducted a single large-scale pretraining run on the 7B model, using 1024 H100 GPU chips, leveraging 14 trillion tokens featuring web, code, STEM, and curated high-quality and multilingual data.</li><li><strong>Depth up-scaling for improved reasoning:</strong> Building on recent studies on the effects of model depth, we upscaled the 7B model to a 10B parameters model by duplicating the redundant layers and continuing pre-training with 2 trillion tokens of high-quality data. This yielded Falcon3-10B-Base which achieves state-of-the-art zero-shot and few-shot performance for models under 13B parameters.</li><li><strong>Knowledge distillation for better tiny models:</strong> To provide compact and efficient alternatives, we developed Falcon3-1B-Base and Falcon3-3B-Base by leveraging pruning and knowledge distillation techniques, using less than 100GT of curated high-quality data, thereby redefining pre-training efficiency.</li><li><strong>Pure SSM:</strong> We have further enhanced <a href=https://huggingface.co/tiiuae/falcon-mamba-7b>Falcon Mamba 7B</a> by training on an additional 1.5 trillion tokens of high-quality data, resulting in Falcon3-Mamba-7B-Base. Notably, the updated model offers significantly improved reasoning and mathematical capabilities.</li><li><strong>Other variants:</strong> All models in the Falcon3 family are available in variants such as Instruct, GGUF, GPTQ-Int4, GPTQ-Int8, AWQ, and 1.58-bit, offering flexibility for a wide range of applications.</li></ul><h2 id=key-highlights>Key Highlights<a hidden class=anchor aria-hidden=true href=#key-highlights>#</a></h2><p>Falcon3 featured the limits within the small and medium scales of large language models by demonstrating high performance on common benchmarks:</p><ul><li><a href=https://huggingface.co/tiiuae/Falcon3-1B-Base>Falcon3-1B-Base</a> surpasses SmolLM2-1.7B and is on par with gemma-2-2b.</li><li><a href=https://huggingface.co/tiiuae/Falcon3-3B-Base>Falcon3-3B-Base</a> outperforms larger models like Llama-3.1-8B and Minitron-4B-Base, highlighting the benefits of pre-training with knowledge distillation.</li><li><a href=https://huggingface.co/tiiuae/Falcon3-7B-Base>Falcon3-7B-Base</a> demonstrates top performance, on par with Qwen2.5-7B, among models under the 9B scale.</li><li><a href=https://huggingface.co/tiiuae/Falcon3-10B-Base>Falcon3-10B-Base</a> stands as the state-of-the-art achieving strong results in the under-13B category.</li><li>All the transformer-based Falcon3 models are compatible with <a href=https://ai.meta.com/research/publications/the-llama-3-herd-of-models/>Llama</a> architecture allowing better integration in the AI ecosystem.</li><li><a href=https://huggingface.co/tiiuae/Falcon3-Mamba-7B-Base>Falcon3-Mamba-7B</a> continues to lead as the top-performing State Space Language Model (SSLM), matching or even surpassing leading transformer-based LLMs at the 7B scale, along with support for a longer 32K context length. Having the same architecture as the original <a href=https://huggingface.co/tiiuae/falcon-mamba-7b>Falcon Mamba 7B</a>, users can integrate Falcon3-Mamba-7B seamlessly without any additional effort.</li><li>The instruct versions of our collection of base models further show remarkable performance across various benchmarks with Falcon3-7B-Instruct and Falcon3-10B-Instruct outperforming all instruct models under the 13B scale on the open leaderboard.</li></ul><h2 id=enhanced-capabilities>Enhanced Capabilities<a hidden class=anchor aria-hidden=true href=#enhanced-capabilities>#</a></h2><p>We evaluated models with our internal evaluation pipeline (based on <a href=https://github.com/EleutherAI/lm-evaluation-harness>lm-evaluation-harness</a>) and we report raw scores.
Our evaluations highlight key areas where the Falcon3 family of models excel, reflecting the emphasis on enhancing performance in scientific domains, reasoning, and general knowledge capabilities:</p><ul><li><strong>Math Capabilities:</strong> Falcon3-10B-Base achieves 22.9 on MATH-Lvl5 and 83.0 on GSM8K, showcasing enhanced reasoning in complex math-focused tasks.</li><li><strong>Coding Capabilities:</strong> Falcon3-10B-Base achieves 73.8 on MBPP, while Falcon3-10B-Instruct scores 45.8 on Multipl-E, reflecting their abilities to generalize across programming-related tasks.</li><li><strong>Extended Context Length</strong>: Models in the Falcon3 family support up to 32k tokens (except the 1B supporting up to 8k context), with functional improvements such as scoring 86.3 on BFCL (Falcon3-10B-Instruct).</li><li><strong>Improved Reasoning:</strong> Falcon3-7B-Base and Falcon3-10B-Base achieve 51.0 and 59.7 on BBH, reflecting enhanced reasoning capabilities, with the 10B model showing improved reasoning performance over the 7B.</li><li><strong>Scientific Knowledge Expansion:</strong> Performance on MMLU benchmarks demonstrates advances in specialized knowledge, with scores of 67.4/39.2 (MMLU/MMLU-PRO) for Falcon3-7B-Base and 73.1/42.5 (MMLU/MMLU-PRO) for Falcon3-10B-Base respectively.</li></ul><h2 id=models-specs-and-benchmark-results>Models&rsquo; Specs and Benchmark Results<a hidden class=anchor aria-hidden=true href=#models-specs-and-benchmark-results>#</a></h2><p>Detailed specifications of the Falcon3 family of models are summarized in the following table. The architecture of <a href=https://huggingface.co/tiiuae/Falcon3-7B-Base>Falcon3-7B-Base</a>
is characterized by a head dimension of 256 which yields high throughput when using <a href=https://arxiv.org/abs/2407.08608>FlashAttention-3</a> as it is optimized for this dimension. These decoder-only models span 18 to 40 layers for the transformer-based ones, and 64 layers for the mamba one, all models share the SwiGLU activation function, with vocabulary size of 131K tokens (65Kfor Mamba-7B). The Falcon3-7B-Base is trained on the largest amount of data ensuring comprehensive coverage of concepts and knowledge, the other variants require way less data.</p><p><br><br></p><div style=text-align:center align=center><img src=https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/general/Falcon3-specs.png alt="Training efficiency" width=750></div><br><br><p>The table below highlights the performances of Falcon3-7B-Base and Falcon3-10B-Base on key benchmarks showing competitive performances in general, math, reasoning, and common sense understanding domains.
Feel free to take a look at models&rsquo; cards where we provide additional evaluation results (e.g. MT-Bench, Alpaca, etc).<br><br></p><div style=text-align:center align=center><img src=https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/general/medium-base-models.png alt="Training efficiency" width=800></div><br><br><p>The instruct models also demonstrate competitive and super performances with equivalent and small-size models as highlighted in the tables below.</p><h3 id=instruct-models>Instruct models<a hidden class=anchor aria-hidden=true href=#instruct-models>#</a></h3><p>Falcon3-1B-Instruct and Falcon3-3B-Instruct achieve robust performance across the evaluated benchmarks. Specifically, Falcon3-1B attains competitive results in IFEval (54.4), MUSR (40.7), and SciQ (86.8), while Falcon3-3B exhibits further gains—particularly in MMLU-PRO (29.7) and MATH (19.9)—demonstrating clear scaling effects. Although they do not surpass all competing models on every metric, Falcon models show strong performances in reasoning and common-sense understanding relative to both Qwen and Llama.
In our internal evaluation pipeline:</p><ul><li>We use <a href=https://github.com/EleutherAI/lm-evaluation-harness>lm-evaluation harness</a>.</li><li>We report <strong>raw scores</strong> obtained by applying chat template <strong>without fewshot_as_multiturn</strong> (unlike Llama3.1).</li><li>We use same batch-size across all models.</li></ul><p><br><br></p><div style=text-align:left align=center><img src=https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/general/small-instruct-models.png alt="Training efficiency" width=800></div><br><br>Furthermore, Falcon3-7B and Falcon3-10B show robust performance across the evaluated benchmarks. Falcon3-7B achieves competitive scores on reasoning (Arc Challenge: 65.9, MUSR: 46.4) and math (GSM8K: 79.1), while Falcon3-10B demonstrates further improvements, notably in GSM8K (83.1) and IFEval (78), indicating clear scaling benefits.<br><br><div style=text-align:left align=center><img src=https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/general/medium-instruct-models.png alt="Training efficiency" width=800></div><br><br><h2 id=open-source-commitment>Open Source Commitment<a hidden class=anchor aria-hidden=true href=#open-source-commitment>#</a></h2><p>In line with our mission to foster AI accessibility and collaboration, all models in the Falcon3 family are released under the <a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html><strong>Falcon LLM license</strong></a>. We hope the AI community finds these models valuable for research, application development, and further experimentation. Falcon3 is not a culmination but a continuation of our efforts to create more capable, efficient, specialized foundation models. In January 2025, we will further release other models of the Falcon3 family featuring enhanced multi-modal capabilities including image, video, and audio support, as well as a full technical report covering our methodologies. We welcome feedback and collaboration from the community as we continue to refine and advance these technologies.</p><h2 id=useful-links>Useful links<a hidden class=anchor aria-hidden=true href=#useful-links>#</a></h2><ul><li>Access to our models (including GGUF and 1.58bit models) of this series through <a href=https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026>the Falcon3 HuggingFace collection</a>.</li><li>Feel free to join <a href=https://discord.gg/vfw6k2G3>our discord server</a> if you have any questions or to interact with our researchers and developers.</li><li>Check out the <a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html>Falcon-LLM License link</a> for more details about the license.</li><li>Refer to the official <a href=https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/>Open LLM Leaderboard</a> for HF evaluations of our models.</li></ul><h2 id=acknowledgments>Acknowledgments<a hidden class=anchor aria-hidden=true href=#acknowledgments>#</a></h2><p>We warmly thank the following people for their smooth support and integration within the ecosystem.</p><ul><li><a href=https://huggingface.co/alozowski>Alina Lozovskaya</a> and <a href=https://huggingface.co/clefourrier>Clementine Fourrier</a> for helping us evaluate the model on the HF leaderboard.</li><li><a href=https://huggingface.co/cyrilvallez>Cyril Vallez</a> and <a href=https://huggingface.co/ArthurZ>Arthur Zucker</a> for the transformers documentation integration.</li><li><a href=https://huggingface.co/reach-vb>Vaibhav Srivastav</a> and <a href=https://huggingface.co/ariG23498>Aritra Roy Gosthipaty</a> for his help reviewing this blogpost.</li><li><a href=https://github.com/ggerganov>Georgi Gerganov</a> for his help in integrating an important fix to make Falcon3 series models work in <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>.</li><li><a href=https://github.com/awni>Awni Hannun</a> for helping us review necessary changes in order to integrate Falcon3 series into MLX ecosystem.</li><li><a href=https://github.com/microsoft/BitNet>BitNet.cpp team</a> for helping us integrating 1.58bit variants of Falcon3 models into BitNet.</li></ul><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If the Falcon3 family of models were helpful to your work, feel free to give us a cite.</p><pre tabindex=0><code>@misc{Falcon3,
    title = {The Falcon 3 Family of Open Models},
    url = {https://huggingface.co/blog/falcon3},
    author = {Falcon-LLM Team},
    month = {December},
    year = {2024}
}
</code></pre><div class=post-contributors><div class=contributors-section><h4>Core Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Falcon LLM team" class=contributor-image><p class=contributor-name>Falcon LLM team</p></div></div></div><br></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>