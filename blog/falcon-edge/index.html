<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script>!function(e,t){if(!e.rdt){var s,o,n=e.rdt=function(){n.sendEvent?n.sendEvent.apply(n,arguments):n.callQueue.push(arguments)};n.callQueue=[],s=t.createElement("script"),s.src="https://www.redditstatic.com/ads/pixel.js",s.async=!0,o=t.getElementsByTagName("script")[0],o.parentNode.insertBefore(s,o)}}(window,document),rdt("init","a2_f0we4sxffrnm"),rdt("track","PageVisit")</script><script async src="https://www.googletagmanager.com/gtag/js?id=AW-16575356988"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","AW-16575356988")</script><title>Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models. | Falcon</title>
<meta name=keywords content><meta name=description content="In this blogpost, we present the key highlights and rationales about the Falcon-Edge series - a collection of powerful, universal, and fine-tunable language models available in ternary format, based on the BitNet architecture.
Drawing from our experience with BitNet, Falcon-Edge introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/blog/falcon-edge/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/blog/falcon-edge/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models."><meta property="og:description" content="In this blogpost, we present the key highlights and rationales about the Falcon-Edge series - a collection of powerful, universal, and fine-tunable language models available in ternary format, based on the BitNet architecture.
Drawing from our experience with BitNet, Falcon-Edge introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/blog/falcon-edge/"><meta property="og:image" content="https://falcon-lm.github.io/blog/falcon-edge/cover.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-15T12:00:00+00:00"><meta property="article:modified_time" content="2025-05-15T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/blog/falcon-edge/cover.png"><meta name=twitter:title content="Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models."><meta name=twitter:description content="In this blogpost, we present the key highlights and rationales about the Falcon-Edge series - a collection of powerful, universal, and fine-tunable language models available in ternary format, based on the BitNet architecture.
Drawing from our experience with BitNet, Falcon-Edge introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://falcon-lm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.","item":"https://falcon-lm.github.io/blog/falcon-edge/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.","name":"Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.","description":"In this blogpost, we present the key highlights and rationales about the Falcon-Edge series - a collection of powerful, universal, and fine-tunable language models available in ternary format, based on the BitNet architecture.\nDrawing from our experience with BitNet, Falcon-Edge introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.\n","keywords":[],"articleBody":"In this blogpost, we present the key highlights and rationales about the Falcon-Edge series - a collection of powerful, universal, and fine-tunable language models available in ternary format, based on the BitNet architecture.\nDrawing from our experience with BitNet, Falcon-Edge introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.\nAvailable now in two sizes—1 Billion and 3 Billion parameters—each size comes in both base and instruction-tuned models. Discover the Falcon-Edge series on our dedicated Hugging Face collection.\nIntroduction Large Language Models (LLMs), by design, are inherently large and resource-intensive. As demand grows to deploy these models efficiently on edge devices, research into model compression has accelerated. Recent efforts, such as those by DeepSeek and Llama 4, explore training with reduced precision formats—down to FP8—to improve deployment scalability. On the other hand, many state-of-the-art methods emphasize post-training quantization. In contrast to these approaches, BitNet introduces a fundamentally different paradigm: unlike reduced-precision training which still relies on floating-point formats, and post-training quantization which adjusts weights after full-precision training, BitNet operates with the lowest possible precision — ternary weights ({-1, 0, 1}) — directly during training, enabling an end-to-end ultra-efficient model design.\nThese ternary weights are paving the way for a “matmul-free” LLM design that is notably faster and remarkably memory-efficient in practice. The primary challenge of this innovative approach is the necessity for pre-training BitNet models, which can be computationally demanding and costly for typical users.\nUnleashing the full potential of Bitnet models Six months ago, Microsoft introduced bitnet.cpp, a framework designed to accelerate CPU inference speeds by up to 5 times for certain architectures. This advancement makes BitNet models highly appealing for local deployment, significantly enhancing their readiness for production and ease of use across various applications. However, from the community’s viewpoint, BitNet is still largely regarded as a proof of concept or prototype, primarily due to two key challenges:\nModel Performance: Despite efforts by the community, including our own, to develop robust BitNet models, recent 1-bit large language models (LLMs) have struggled to match the overall performance of similarly sized models using other frameworks. This was the case at the time of writing the blogpost but Microsoft recently released a powerful BitNet model which brought a lot of interest from community. Accessibility: The process of 1-bit fine-tuning—which involves converting a non-BitNet model checkpoint into a BitNet one—has proven ineffective. Currently, pre-training appears to be the most viable approach. With this release, we aim to demonstrate that a novel pre-training approach has the potential to overcome these limitations and cater to a wide range of applications. In the subsequent sections, we will delve deeper into each of these issues and explore how our new paradigm addresses them.\nProposed architecture We adopted the architecture outlined in the paper The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, but made a key modification by eliminating the Layer Normalization layers within the BitNet layers. However, we retained the original pre-attention and pre-MLP layer norms to ensure compatibility with the Llama architecture, allowing seamless integration from the outset. Interestingly, we discovered that removing these Layer Normalization layers had no adverse effect on model performance, while also ensuring compatibility with the broader ecosystem with minimal adjustments.\nBeyond the methods described in the paper, we implemented optimized Triton kernels for both activation_quant and weight_quant, significantly lowering the pre-training costs of our models. We are making these kernels accessible to the community through the Python package onebitllms, enabling researchers and developers to leverage them for efficient BitNet pre-training and fine-tuning.\nTo further reduce the memory footprint of the final model, we intentionally opted for a smaller vocabulary size of 32678. This tokenizer was trained on a large English-focused corpus, with the most common LaTeX tokens manually added to the vocabulary.\nFalcon-Edge, a series of powerful Bitnet models Leveraging the learnings from pre-training data strategies from our center, we pre-train our model on an internal data mixture for approximately 1.5 Tera Tokens. We use the classic WSD learning rate scheduler for pre-training.\nWe evaluate our models (base and instruct versions) on the former Hugging Face leaderboard v2 benchmark and report the normalized results below in comparison with other models of similar size:\nFor 1B scale models and below Detailed results: Model Nb Params Mem Footprint IFEVAL Math-Hard GPQA MuSR BBH MMLU-Pro Avg. Qwen2.5-0.5B 0.5B 1GB 16.27 3.93 0.0 2.08 6.95 10.06 6.55 Qwen3-0.6B 0.6B 1.5GB 23.19 5.89 3.78 3.38 11.75 13.98 10.32 SmolLM2-360M 0.36B 720MB 21.15 1.21 0.0 7.73 5.54 1.88 6.25 Qwen2.5-1.5B 1.5B 3.1GB 26.74 9.14 16.66 5.27 20.61 4.7 13.85 Qwen3-1.7B 1.5B 4.06GB 26.81 13.95 6.71 13.65 20.40 26.45 17.99 SmolLM2-1.7B 1.7B 3.4GB 24.4 2.64 9.3 4.6 12.64 3.91 9.58 Falcon-3-1B-Base 1.5B 3GB 24.28 3.32 11.34 9.71 6.76 3.91 9.89 Falcon-E-1B-Base 1.8B 665MB 32.9 10.97 2.8 3.65 12.28 17.82 13.40 For 3B scale models Detailed result: Model Nb Params Mem Footprint IFEVAL Math-Hard GPQA MuSR BBH MMLU-Pro Avg. Falcon-3-3B-Base 3B 6.46GB 15.74 11.78 6.26 6.27 21.58 18.09 15.74 Qwen2.5-3B 3B 6.17GB 26.9 14.8 6.38 11.76 24.3 24.48 18.1 Falcon-E-3B-Base 3B 999MB 36.67 13.45 8.67 4.14 19.83 27.16 18.32 Below are the results for instruction fine-tuned models (for Qwen3 series, we disable the thinking mode during evaluation):\nDetailed results: Model Nb Params Mem Footprint IFEVAL Math-Hard GPQA MuSR BBH MMLU-Pro Avg. Qwen2.5-0.5B-Instruct 500M 1GB 31.53 10.35 1.23 1.37 8.17 8.00 10.11 Qwen3-0.6B 600M 1.5GB 62.15 16.15 2.81 1.86 6.13 7.71 16.13 SmolLM2-360M-Instruct 360M 720MB 38.42 1.51 0.67 2.77 4.17 1.3 8.14 Qwen2.5-1.5B-Instruct 1.5B 3.1GB 44.76 22.05 0.78 3.19 19.91 19.99 18.43 Qwen3-1.7B 1.7B 4.02GB 70.76 38.17 3.29 8.32 9.26 15.04 24.14 SmolLM2-1.7B 1.7B 3.4GB 53.68 5.82 0 4.1 10.92 11.71 15.02 Bitnet-b1.58-2B-4T 2B 1.18GB 59.11 7.23 5.25 1.74 17.94 14.82 17.70 Falcon-3-1B-Instruct 1.5B 3GB 55.57 6.34 2.24 10.56 12.96 9.32 16.16 Falcon-E-1B-Instruct 1.8B 665MB 54.35 9.12 9.64 2.51 19.42 16.5 18.59 For 3B scale models: Detailed results: Model Nb Params Mem Footprint IFEVAL Math-Hard GPQA MuSR BBH MMLU-Pro Avg. Falcon-3-3B-Instruct 3B 6.46GB 69.77 25 5.15 11.13 26.29 22.28 26.6 Qwen2.5-3B-Instruct 3B 6.17GB 64.75 36.78 3.02 7.57 25.80 25.05 27.16 Falcon-E-3B-Instruct 3B 999MB 60.97 15.3 7.45 2.12 23.59 26.45 22.65 Additional results (leaderboard v1) on comparing our instructed models with Microsoft’s new BitNet model:\nModel Nb Params Mem Footprint ARC-Challenge GSM8K HellaSwag MMLU TruthfulQA Average Bitnet-b1.58-2B-4T 2B 1.18GB 38.31 65.27 59.02 47.43 47.65 51.54 Falcon-E-1B-Instruct 1.8B 665MB 36.60 54.74 50.19 48.33 42.09 46.39 Falcon-E-3B-Instruct 3B 999MB 43.09 64.52 56.97 55.70 45.58 53.17 Falcon-Edge demonstrates on-par and better performances than models of comparable sizes on the leaderboard v2 tasks, demonstrating that it is possible to train powerful BitNet models on desired domains while being competitive enough on other tasks.\nFalcon-Edge, a series of universal models If we look closer at the formula of the BitNet linear layer for inference (in terms of Python code):\ndef activation_norm_quant(x): scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5) y = (x * scale).round().clamp_(-128, 127) return y, scale class BitLinear(nn.Linear): def post_quant_process(self, input, input_scale, weight_scale): out = input / (input_scale * weight_scale) return out def forward(self, input): w = self.weight w_quant = unpack_weights(w, dtype=self.dtype) input_quant, input_scale = self.activation_quant(input) y = F.linear(input_quant.to(self.dtype), w_quant) y = self.post_quant_process(y, self.weight_scale, input_scale) if self.bias is not None: y += self.bias.view(1, -1).expand_as(y) return y The normalization activation_norm_quant quantizes the activations in int8 format, then the activation is computed back in half precision by diving it by x_scale. Since the model has been trained with fake 8-bit activation quantization, we argue that it is possible to approximate that:\nx_quant, x_scale = activation_norm_quant(x) x ~= (x_quant / x_scale) Therefore, instead of quantizing the model post-training, injecting the weight scale after quantizing the weights should lead to a good enough “approximation” of the non-BitNet version of the model:\ndef _weight_quant(w): scale = 1.0 / w.abs().mean().clamp_(min=1e-05) u = (w * scale).round().clamp_(-1, 1) return u, scale for param_name, param_value in state_dict.items(): if _is_param_to_not_quantize(param_name): continue param_value, param_scale = _weight_quant(param_value) param_value = param_value / param_scale state_dict_quant[param_name] = param_value We confirm this by running end-to-end evaluations on the bfloat16 variant of our 1B and 3B base models and below are the results:\nModel IFEVAL Math-Hard GPQA MuSR BBH MMLU-Pro Avg. Falcon-E-1B 32.9 10.97 2.8 3.65 12.28 17.82 13.40 Falcon-E-1B-bf16 29.89 11.23 1.8 3.32 12.27 18.04 12.75 Falcon-E-3B 36.67 13.45 8.67 4.14 19.83 27.16 18.32 Falcon-E-3B-bf16 34.84 13.21 8.91 4.88 20.3 27.00 18.19 The bfloat16 counterparts of the models can be loaded directly via Hugging Face transformers by passing revision=\"bfloat16\" in the from_pretrained function:\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer from trl import SFTTrainer model_id = \"tiiuae/Falcon-E-1B-Base\" tokenizer = AutoTokenizer.from_pretrained(model_id, revision=\"prequantized\") model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=torch.bfloat16, revision=\"bfloat16\" ) Falcon-Edge, a series of fine-tunable Bitnet models To the best of our knowledge, except from the most recent release from Microsoft previous BitNet releases only focus on releasing the final quantized model, making it usable only for inference. Similarly to the release from Microsoft, we propose to extend the accessibility of research and application of BitNet models by releasing their pre-quantized weights. That way, users can either perform fine-tuning on their target domain, or do continuous pre-training of the BitNet checkpoint as long as nn.Linear layers are replaced by BitnetLinear layers, and by making sure to quantize the model post training in BitNet format. Since the weights corresponds to the pre-quantized weights, performing text generation without replacing the nn.Linear layers with BitnetLinear layers will produce gibberish output.\nThe pre-quantized weights can be downloaded via Hugging Face’s transformers library by specifying the revision argument to be prequantized:\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"tiiuae/Falcon-E-1B-Base\" tokenizer = AutoTokenizer.from_pretrained(model_id, revision=\"prequantized\") model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=torch.bfloat16, revision=\"prequantized\" ) This way, we will help fostering an ecosystem around first powerful 1-bit fine-tunes by the community. We provide to community the tools to get easily started and fine-tune their own version of powerful BitNet models by packaging all needed utility methods for performing fine-tuning on the pre-quantized weights on a Python package called onebitllms that we will cover in the next section.\nIntroducing onebitllms - a lightweight python package for 1-bit LLMs training toolkit In this release, we also introduce onebitllms - a lightweight Python package that can be plugged into your favorite LLM fine-tuning tools in order to fine-tune any pre-quantized BitNet model. At this time of writing onebitllms exposes these main functionalities:\nUtility method to convert the prequantized model checkpoints into BitNet training format in order to pass it to any of your favorite LLM fine-tuning framework. We currently tested our library with Hugging Face’s trl library. Utility method to quantize the trained checkpoint in BitNet format as well as in usual bfloat16 format. Fore more fine-grained control: Bare BitnetLinear and triton kernels that be injected and used for your pre-training framework. Currently, only full-finetuning is supported through this framework, while in this release the model sizes are relatively small, supporting Parameter-Efficient Fine-tuning (PEFT) methods for BitNet models remains an exciting and impactful open question for upcoming BitNet models.\nTo get started, simply install the package directly through pip or from source, and take a look at examples/ folders inside the source code.\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer from trl import SFTTrainer from onebitllms import replace_linear_with_bitnet_linear, quantize_to_1bit model_id = \"tiiuae/Falcon-E-1B-Base\" tokenizer = AutoTokenizer.from_pretrained(model_id, revision=\"prequantized\") model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=torch.bfloat16, revision=\"prequantized\" ) model = replace_linear_with_bitnet_linear(model) trainer = SFTTrainer( model, ... ) trainer.train() quantize_to_1bit(output_directory) With this package, we hope to accelerate research and development around ternary format LLMs, and hope to see many derivations of Falcon-Edge and other future powerful BitNet models developed by the community.\nGoing further We believe this release opens up multiple interesting directions - among all the possible follow up directions, we currently think that the following open questions will make BitNet models much more impactful in the near future:\nWriting more powerful GPU inference kernels for BitNet architecture: leveraging the same core ideas behind bitnet.cpp, we hope that this release will convince the research community to focus on developping powerful BitNet inference kernels for faster inference on GPUs - thus making them faster than native models on GPUs. Support PEFT methods for BitNet fine-tuning: This remains an unexplored research question that can open up multiple new possibilities for BitNet models. More rigourous investigation on the universality of Bitnet checkpoints: While we observe that simply injecting the weight scale leads to having a descent non-Bitnet checkpoint, we believe that more research can be done to minimize the performance degradation between the Bitnet checkpoint and its bfloat16 counterpart, thus making it fully performance degradation-free. On multi-modal Bitnet models: We hope these Bitnet foundational models together with onebitllms package can serve a as a foundational work for creating first multi-modal Bitnet VLM (Vision Language Model) etc. More optimized Bitnet training kernels: To write our kernels, we decided to take a two stages approach to first compute the global maximum to later use it block-wise for normalization. This approach can be revised to write more efficient kernels. In our tests, we estimate the overhead to be around ~20% between non-Bitnet pre-training against Bitnet pre-training. We will release soon more extensive numbers on the overhead introduced by Bitnet for training. Citation If you find this work useful for your research and work, please consider citing our work, as well as citing all the foundational work behind BitNet models:\n@misc{tiionebitllms, title = {Falcon-E, a series of powerful, universal and fine-tunable 1.58bit language models.}, author = {Falcon-LLM Team}, month = {May}, url = {https://falcon-lm.github.io/blog/falcon-edge}, year = {2025} } More References @misc{ma2025bitnetb1582b4ttechnical, title={BitNet b1.58 2B4T Technical Report}, author={Shuming Ma and Hongyu Wang and Shaohan Huang and Xingxing Zhang and Ying Hu and Ting Song and Yan Xia and Furu Wei}, year={2025}, eprint={2504.12285}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2504.12285}, } @misc{wang2025bitnetcppefficientedgeinference, title={Bitnet.cpp: Efficient Edge Inference for Ternary LLMs}, author={Jinheng Wang and Hansong Zhou and Ting Song and Shijie Cao and Yan Xia and Ting Cao and Jianyu Wei and Shuming Ma and Hongyu Wang and Furu Wei}, year={2025}, eprint={2502.11880}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2502.11880}, } @misc{, title={1.58-Bit LLM: A New Era of Extreme Quantization}, author={Mohamed Mekkouri and Marc Sun and Leandro von Werra and Thomas Wolf}, year={2024}, } @misc{ma2024era1bitllmslarge, title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei}, year={2024}, eprint={2402.17764}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2402.17764}, } @misc{wang2023bitnetscaling1bittransformers, title={BitNet: Scaling 1-bit Transformers for Large Language Models}, author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei}, year={2023}, eprint={2310.11453}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2310.11453}, } ","wordCount":"2477","inLanguage":"en","image":"https://falcon-lm.github.io/blog/falcon-edge/cover.png","datePublished":"2025-05-15T12:00:00Z","dateModified":"2025-05-15T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/blog/falcon-edge/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:url(cover.png)50%/cover no-repeat fixed"></div><div class=hero-gradient></div><div class=hero-blur></div><div class="hero text-light"><h1 class=post-title>Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.</h1><div class=post-meta><span title='2025-05-15 12:00:00 +0000 UTC'>May 15, 2025</span>&nbsp;•&nbsp;12 min&nbsp;•&nbsp;2477 words&nbsp;•&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><figure class=entry-cover><a href=/blog/falcon-edge/cover.png target=_blank rel="noopener noreferrer"><img loading=lazy srcset="https://falcon-lm.github.io/blog/falcon-edge/cover_hu_858bf5a669550532.png 360w ,https://falcon-lm.github.io/blog/falcon-edge/cover_hu_235c62a2cfc71d27.png 480w ,https://falcon-lm.github.io/blog/falcon-edge/cover_hu_70e2f7ff2f43da74.png 720w ,https://falcon-lm.github.io/blog/falcon-edge/cover_hu_443ebb01ee1dd299.png 1080w ,https://falcon-lm.github.io/blog/falcon-edge/cover_hu_aeb663f43b0f5b7f.png 1500w ,https://falcon-lm.github.io/blog/falcon-edge/cover.png 1968w" sizes="(min-width: 768px) 720px, 100vw" src=https://falcon-lm.github.io/blog/falcon-edge/cover.png alt width=1968 height=1018></a></figure><div class=post-content><p>In this blogpost, we present the key highlights and rationales about the <em>Falcon-Edge</em> series - a collection of <em>powerful</em>, <em>universal</em>, and <em>fine-tunable</em> language models available in ternary format, based on the BitNet architecture.</p><p>Drawing from our experience with BitNet, <strong>Falcon-Edge</strong> introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.</p><p>Available now in two sizes—1 Billion and 3 Billion parameters—each size comes in both base and instruction-tuned models. Discover the Falcon-Edge series on <a href=https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130>our dedicated Hugging Face collection</a>.</p><p><a id=pull-figures></a></p><div style=display:flex;justify-content:center;flex-wrap:wrap;gap:20px><img src=./figure-perf-instruct.png alt="Instruct models performance" style=width:100%;max-width:600px;height:auto></div><p><a id=pull-figures></a></p><div style=display:flex;justify-content:center;flex-wrap:wrap;gap:20px><img src=./figure-perf-base.png alt="Base models performance" style=width:100%;max-width:600px;height:auto></div><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Large Language Models (LLMs), by design, are inherently large and resource-intensive. As demand grows to deploy these models efficiently on edge devices, research into model compression has accelerated. Recent efforts, such as those by DeepSeek and Llama 4, explore training with reduced precision formats—down to FP8—to improve deployment scalability. On the other hand, many state-of-the-art methods emphasize post-training quantization. In contrast to these approaches, BitNet introduces a fundamentally different paradigm: unlike reduced-precision training which still relies on floating-point formats, and post-training quantization which adjusts weights after full-precision training, BitNet operates with the lowest possible precision — ternary weights ({-1, 0, 1}) — directly during training, enabling an end-to-end ultra-efficient model design.</p><p>These ternary weights are paving the way for a &ldquo;matmul-free&rdquo; LLM design that is notably faster and remarkably memory-efficient in practice. The primary challenge of this innovative approach is the necessity for pre-training BitNet models, which can be computationally demanding and costly for typical users.</p><h3 id=unleashing-the-full-potential-of-bitnet-models>Unleashing the full potential of Bitnet models<a hidden class=anchor aria-hidden=true href=#unleashing-the-full-potential-of-bitnet-models>#</a></h3><p>Six months ago, Microsoft introduced <a href=https://github.com/microsoft/BitNet>bitnet.cpp</a>, a framework designed to accelerate CPU inference speeds by up to 5 times for certain architectures. This advancement makes BitNet models highly appealing for local deployment, significantly enhancing their readiness for production and ease of use across various applications. However, from the community&rsquo;s viewpoint, BitNet is still largely regarded as a proof of concept or prototype, primarily due to two key challenges:</p><ul><li>Model Performance: Despite efforts by the community, including our own, to develop robust BitNet models, recent 1-bit large language models (LLMs) have struggled to match the overall performance of similarly sized models using other frameworks. This was the case at the time of writing the blogpost but Microsoft recently released <a href=https://huggingface.co/collections/microsoft/bitnet-67fddfe39a03686367734550>a powerful BitNet model</a> which brought a lot of interest from community.</li><li>Accessibility: The process of 1-bit fine-tuning—which involves converting a non-BitNet model checkpoint into a BitNet one—has proven ineffective. Currently, pre-training appears to be the most viable approach.</li></ul><p>With this release, we aim to demonstrate that a novel pre-training approach has the potential to overcome these limitations and cater to a wide range of applications. In the subsequent sections, we will delve deeper into each of these issues and explore how our new paradigm addresses them.</p><h3 id=proposed-architecture>Proposed architecture<a hidden class=anchor aria-hidden=true href=#proposed-architecture>#</a></h3><p>We adopted the architecture outlined in the paper <a href=https://arxiv.org/abs/2402.17764><em>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</em></a>, but made a key modification by eliminating the Layer Normalization layers within the BitNet layers. However, we retained the original pre-attention and pre-MLP layer norms to ensure compatibility with the Llama architecture, allowing seamless integration from the outset. Interestingly, we discovered that removing these Layer Normalization layers had no adverse effect on model performance, while also ensuring compatibility with the broader ecosystem with minimal adjustments.</p><p>Beyond the methods described in the paper, we implemented optimized Triton kernels for both <code>activation_quant</code> and <code>weight_quant</code>, significantly lowering the pre-training costs of our models. We are making these kernels accessible to the community through the Python package <code>onebitllms</code>, enabling researchers and developers to leverage them for efficient BitNet pre-training and fine-tuning.</p><p>To further reduce the memory footprint of the final model, we intentionally opted for a smaller vocabulary size of <code>32678</code>. This tokenizer was trained on a large English-focused corpus, with the most common LaTeX tokens manually added to the vocabulary.</p><h3 id=falcon-edge-a-series-of-powerful-bitnet-models><em>Falcon-Edge</em>, a series of powerful Bitnet models<a hidden class=anchor aria-hidden=true href=#falcon-edge-a-series-of-powerful-bitnet-models>#</a></h3><p>Leveraging the learnings from pre-training data strategies from our center, we pre-train our model on an internal data mixture for approximately 1.5 Tera Tokens. We use the classic WSD learning rate scheduler for pre-training.</p><p>We evaluate our models (base and instruct versions) on the former Hugging Face leaderboard v2 benchmark and report the normalized results below in comparison with other models of similar size:</p><h4 id=for-1b-scale-models-and-below>For 1B scale models and below<a hidden class=anchor aria-hidden=true href=#for-1b-scale-models-and-below>#</a></h4><div id=chart-general-plt></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`
[
    { "category": "IFEVAL", "model": "Qwen2.5-0.5B", "value": 0.1627 },
    { "category": "IFEVAL", "model": "Qwen3-0.6B", "value": 0.2319 },
    { "category": "IFEVAL", "model": "SmolLM2-360M", "value": 0.2115 },
    { "category": "IFEVAL", "model": "Qwen2.5-1.5B", "value": 0.2674 },
    { "category": "IFEVAL", "model": "Qwen3-1.7B", "value": 0.2681 },
    { "category": "IFEVAL", "model": "SmolLM2-1.7B", "value": 0.244 },
    { "category": "IFEVAL", "model": "Falcon-3-1B-Base", "value": 0.2428 },
    { "category": "IFEVAL", "model": "Falcon-E-1B-Base", "value": 0.3290 },

    { "category": "Math-Hard", "model": "Qwen2.5-0.5B", "value": 0.0393 },
    { "category": "Math-Hard", "model": "Qwen3-0.6B", "value": 0.0589 },
    { "category": "Math-Hard", "model": "SmolLM2-360M", "value": 0.0121 },
    { "category": "Math-Hard", "model": "Qwen2.5-1.5B", "value": 0.0914 },
    { "category": "Math-Hard", "model": "Qwen3-1.7B", "value": 0.1395 },
    { "category": "Math-Hard", "model": "SmolLM2-1.7B", "value": 0.0264 },
    { "category": "Math-Hard", "model": "Falcon-3-1B-Base", "value": 0.0332 },
    { "category": "Math-Hard", "model": "Falcon-E-1B-Base", "value": 0.1097 },

    { "category": "GPQA", "model": "Qwen2.5-0.5B", "value": 0.00 },
    { "category": "GPQA", "model": "Qwen3-0.6B", "value": 0.0378 },
    { "category": "GPQA", "model": "SmolLM2-360M", "value": 0.00 },
    { "category": "GPQA", "model": "Qwen2.5-1.5B", "value": 0.0470 },
    { "category": "GPQA", "model": "Qwen3-1.7B", "value": 0.0671 },
    { "category": "GPQA", "model": "SmolLM2-1.7B", "value": 0.0391 },
    { "category": "GPQA", "model": "Falcon-3-1B-Base", "value": 0.0391 },
    { "category": "GPQA", "model": "Falcon-E-1B-Base", "value": 0.0280 },

    { "category": "MuSR", "model": "Qwen2.5-0.5B", "value": 0.0208 },
    { "category": "MuSR", "model": "Qwen3-0.6B", "value": 0.0338 },
    { "category": "MuSR", "model": "SmolLM2-360M", "value": 0.0773 },
    { "category": "MuSR", "model": "Qwen2.5-1.5B", "value": 0.0527 },
    { "category": "MuSR", "model": "Qwen3-1.7B", "value": 0.1364 },
    { "category": "MuSR", "model": "SmolLM2-1.7B", "value": 0.0460 },
    { "category": "MuSR", "model": "Falcon-3-1B-Base", "value": 0.0971 },
    { "category": "MuSR", "model": "Falcon-E-1B-Base", "value": 0.0365 },

    { "category": "BBH", "model": "Qwen2.5-0.5B", "value": 0.0695 },
    { "category": "BBH", "model": "Qwen3-0.6B", "value": 0.1175 },
    { "category": "BBH", "model": "SmolLM2-360M", "value": 0.0554 },
    { "category": "BBH", "model": "Qwen2.5-1.5B", "value": 0.1666 },
    { "category": "BBH", "model": "Qwen3-1.7B", "value": 0.2040 },
    { "category": "BBH", "model": "SmolLM2-1.7B", "value": 0.0930 },
    { "category": "BBH", "model": "Falcon-3-1B-Base", "value": 0.1134 },
    { "category": "BBH", "model": "Falcon-E-1B-Base", "value": 0.1228 },

    { "category": "MMLU-Pro", "model": "Qwen2.5-0.5B", "value": 0.1006 },
    { "category": "MMLU-Pro", "model": "Qwen3-0.6B", "value": 0.1398 },
    { "category": "MMLU-Pro", "model": "SmolLM2-360M", "value": 0.0188 },
    { "category": "MMLU-Pro", "model": "Qwen2.5-1.5B", "value": 0.2061 },
    { "category": "MMLU-Pro", "model": "Qwen3-1.7B", "value": 0.2645 },
    { "category": "MMLU-Pro", "model": "SmolLM2-1.7B", "value": 0.1264 },
    { "category": "MMLU-Pro", "model": "Falcon-3-1B-Base", "value": 0.0676 },
    { "category": "MMLU-Pro", "model": "Falcon-E-1B-Base", "value": 0.1782 },

    { "category": "Avg", "model": "Qwen2.5-0.5B", "value": 0.0655 },
    { "category": "Avg", "model": "Qwen3-0.6B", "value": 0.1032 },
    { "category": "Avg", "model": "SmolLM2-360M", "value": 0.0625 },
    { "category": "Avg", "model": "Qwen2.5-1.5B", "value": 0.1385 },
    { "category": "Avg", "model": "Qwen3-1.7B", "value": 0.1799 },
    { "category": "Avg", "model": "SmolLM2-1.7B", "value": 0.0958 },
    { "category": "Avg", "model": "Falcon-3-1B-Base", "value": 0.0989 },
    { "category": "Avg", "model": "Falcon-E-1B-Base", "value": 0.1340 }
]
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const p=[],t=p.length>0?p:Array.from(new Set(n.map(e=>e.model))),u=[],g=u.length>0?u:Array.from(new Set(n.map(e=>e.category))),b="Falcon-E-1B-Base",r=b||t[0],v=!0,e={top:40,right:30,bottom:40,left:60},m=800-e.left-e.right,i=430-e.top-e.bottom,a=d3.select("#chart-general-plt").append("svg").attr("width",m+e.left+e.right).attr("height",i+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-general-plt")||d3.select("body").append("div").attr("id","tooltip-general-plt").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const h=d3.select("#tooltip-general-plt"),d=d3.scaleBand().domain(g).range([0,m]).padding(.3);a.append("g").attr("transform",`translate(0,${i})`).call(d3.axisBottom(d).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove();const t=d3.select(this).append("foreignObject").attr("x",-50).attr("y",0).attr("width",100).attr("height",50);t.append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","center").style("width","100%").text(e)});const f=d3.scaleBand().domain(t).range([0,d.bandwidth()]).padding(.05);let s="0",o="0.50";if(s===null||o===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;s=s===null?Math.max(0,Math.min(...e)-t):s,o=o===null?Math.max(...e)+t:o}const l=d3.scaleLinear().domain([s,o]).range([i,0]);a.append("g").call(d3.axisLeft(l).tickFormat(e=>v?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");const c=e=>{if(e.model===r)return"#b987ff";const s=t.indexOf(e.model),o=t.length,n=128-s*(0-80)/(o-1||1);return`rgb(${n}, ${n+8}, ${n+16})`},j=e=>{if(e===r)return"#b088ff";const s=t.indexOf(e),o=t.length,i=128-s*(0-80)/(o-1||1),n=Math.min(i+30,230);return`rgb(${n}, ${n+8}, ${n+16})`};g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`category-group-${e}`);s.selectAll(`.bar-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",t=>d(e)+f(t.model)).attr("y",e=>l(e.value)).attr("width",f.bandwidth()).attr("height",e=>i-l(e.value)).attr("fill",e=>c(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){const e=d3.select(this).attr("data-model");return j(e)});const s=`<strong style="font-size: 16px;">${e}</strong><br/><span style="font-size: 12px;">`+t.map(e=>v?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>";h.style("opacity",1).html(s).style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",function(){const e=d3.select(this).attr("data-model");return c({model:e})}),h.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("transform","rotate(-90)").attr("y",-e.left+15).attr("x",-i/2).text("Performance %").style("font-family","inherit");const y=d3.select("#chart-general-plt").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","-10px"),_=[...t].sort((e,n)=>e===r?-1:n===r?1:t.indexOf(e)-t.indexOf(n));_.forEach(e=>{const n=c({model:e}),t=y.append("div").style("display","flex").style("align-items","center").style("margin","0 10px");t.append("div").style("width","20px").style("height","20px").style("margin-right","5px").style("background-color",n),t.append("div").text(e)})})()</script><div id=chart-memory-ply></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`
[
    { "category": "Memory Footprint", "model": "Qwen2.5-0.5B", "value": 1.0 },
    { "category": "Memory Footprint", "model": "Qwen3-0.6B", "value": 1.5 },
    { "category": "Memory Footprint", "model": "SmolLM2-360M", "value": 0.720 },
    { "category": "Memory Footprint", "model": "Qwen2.5-1.5B", "value": 3.1 },
    { "category": "Memory Footprint", "model": "Qwen3-1.7B", "value": 4.06 },
    { "category": "Memory Footprint", "model": "SmolLM2-1.7B", "value": 3.4 },
    { "category": "Memory Footprint", "model": "Falcon-3-1B-Base", "value": 3.0 },
    { "category": "Memory Footprint", "model": "Falcon-E-1B-Base", "value": 0.665 }
]
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const p=[],t=p.length>0?p:Array.from(new Set(n.map(e=>e.model))),u=[],g=u.length>0?u:Array.from(new Set(n.map(e=>e.category))),b="Falcon-E-1B-Base",r=b||t[0],v=!1,e={top:40,right:30,bottom:40,left:60},m=800-e.left-e.right,i=430-e.top-e.bottom,a=d3.select("#chart-memory-ply").append("svg").attr("width",m+e.left+e.right).attr("height",i+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-memory-ply")||d3.select("body").append("div").attr("id","tooltip-memory-ply").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const h=d3.select("#tooltip-memory-ply"),d=d3.scaleBand().domain(g).range([0,m]).padding(.3);a.append("g").attr("transform",`translate(0,${i})`).call(d3.axisBottom(d).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove();const t=d3.select(this).append("foreignObject").attr("x",-50).attr("y",0).attr("width",100).attr("height",50);t.append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","center").style("width","100%").text(e)});const f=d3.scaleBand().domain(t).range([0,d.bandwidth()]).padding(.05);let s="0",o="4";if(s===null||o===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;s=s===null?Math.max(0,Math.min(...e)-t):s,o=o===null?Math.max(...e)+t:o}const l=d3.scaleLinear().domain([s,o]).range([i,0]);a.append("g").call(d3.axisLeft(l).tickFormat(e=>v?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");const c=e=>{if(e.model===r)return"#b987ff";const s=t.indexOf(e.model),o=t.length,n=128-s*(0-80)/(o-1||1);return`rgb(${n}, ${n+8}, ${n+16})`},j=e=>{if(e===r)return"#b088ff";const s=t.indexOf(e),o=t.length,i=128-s*(0-80)/(o-1||1),n=Math.min(i+30,230);return`rgb(${n}, ${n+8}, ${n+16})`};g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`category-group-${e}`);s.selectAll(`.bar-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",t=>d(e)+f(t.model)).attr("y",e=>l(e.value)).attr("width",f.bandwidth()).attr("height",e=>i-l(e.value)).attr("fill",e=>c(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){const e=d3.select(this).attr("data-model");return j(e)});const s=`<strong style="font-size: 16px;">${e}</strong><br/><span style="font-size: 12px;">`+t.map(e=>v?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>";h.style("opacity",1).html(s).style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",function(){const e=d3.select(this).attr("data-model");return c({model:e})}),h.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("transform","rotate(-90)").attr("y",-e.left+15).attr("x",-i/2).text("Memory Footprint (GB)").style("font-family","inherit");const y=d3.select("#chart-memory-ply").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","-10px"),_=[...t].sort((e,n)=>e===r?-1:n===r?1:t.indexOf(e)-t.indexOf(n));_.forEach(e=>{const n=c({model:e}),t=y.append("div").style("display","flex").style("align-items","center").style("margin","0 10px");t.append("div").style("width","20px").style("height","20px").style("margin-right","5px").style("background-color",n),t.append("div").text(e)})})()</script><details><summary class=bold>Detailed results:</summary><table><thead><tr><th>Model</th><th>Nb Params</th><th>Mem Footprint</th><th>IFEVAL</th><th>Math-Hard</th><th>GPQA</th><th>MuSR</th><th>BBH</th><th>MMLU-Pro</th><th>Avg.</th></tr></thead><tbody><tr><td>Qwen2.5-0.5B</td><td>0.5B</td><td>1GB</td><td>16.27</td><td>3.93</td><td>0.0</td><td>2.08</td><td>6.95</td><td>10.06</td><td>6.55</td></tr><tr><td>Qwen3-0.6B</td><td>0.6B</td><td>1.5GB</td><td>23.19</td><td>5.89</td><td>3.78</td><td>3.38</td><td>11.75</td><td>13.98</td><td>10.32</td></tr><tr><td>SmolLM2-360M</td><td>0.36B</td><td>720MB</td><td>21.15</td><td>1.21</td><td>0.0</td><td>7.73</td><td>5.54</td><td>1.88</td><td>6.25</td></tr><tr><td>Qwen2.5-1.5B</td><td>1.5B</td><td>3.1GB</td><td>26.74</td><td>9.14</td><td>16.66</td><td>5.27</td><td>20.61</td><td>4.7</td><td>13.85</td></tr><tr><td>Qwen3-1.7B</td><td>1.5B</td><td>4.06GB</td><td>26.81</td><td>13.95</td><td>6.71</td><td>13.65</td><td>20.40</td><td>26.45</td><td>17.99</td></tr><tr><td>SmolLM2-1.7B</td><td>1.7B</td><td>3.4GB</td><td>24.4</td><td>2.64</td><td>9.3</td><td>4.6</td><td>12.64</td><td>3.91</td><td>9.58</td></tr><tr><td>Falcon-3-1B-Base</td><td>1.5B</td><td>3GB</td><td>24.28</td><td>3.32</td><td>11.34</td><td>9.71</td><td>6.76</td><td>3.91</td><td>9.89</td></tr><tr><td>Falcon-E-1B-Base</td><td>1.8B</td><td><strong>665MB</strong></td><td>32.9</td><td>10.97</td><td>2.8</td><td>3.65</td><td>12.28</td><td>17.82</td><td>13.40</td></tr></tbody></table></details><h4 id=for-3b-scale-models>For 3B scale models<a hidden class=anchor aria-hidden=true href=#for-3b-scale-models>#</a></h4><div id=chart-general-plt-3b></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`
[
    { "category": "IFEVAL", "model": "Qwen2.5-3B", "value": 0.2690 },
    { "category": "IFEVAL", "model": "Falcon-3-3B-Base", "value": 0.1574 },
    { "category": "IFEVAL", "model": "Falcon-E-3B-Base", "value": 0.3667 },

    { "category": "Math-Hard", "model": "Qwen2.5-3B", "value": 0.1480 },
    { "category": "Math-Hard", "model": "Falcon-3-3B-Base", "value": 0.1178 },
    { "category": "Math-Hard", "model": "Falcon-E-3B-Base", "value": 0.1345 },

    { "category": "GPQA", "model": "Qwen2.5-3B", "value": 0.0638 },
    { "category": "GPQA", "model": "Falcon-3-3B-Base", "value": 0.0626 },
    { "category": "GPQA", "model": "Falcon-E-3B-Base", "value": 0.0867 },

    { "category": "MuSR", "model": "Qwen2.5-3B", "value": 0.1176 },
    { "category": "MuSR", "model": "Falcon-3-3B-Base", "value": 0.0627 },
    { "category": "MuSR", "model": "Falcon-E-3B-Base", "value": 0.0414 },

    { "category": "BBH", "model": "Qwen2.5-3B", "value": 0.2430 },
    { "category": "BBH", "model": "Falcon-3-3B-Base", "value": 0.2158 },
    { "category": "BBH", "model": "Falcon-E-3B-Base", "value": 0.1983 },

    { "category": "MMLU-Pro", "model": "Qwen2.5-3B", "value": 0.2448 },
    { "category": "MMLU-Pro", "model": "Falcon-3-3B-Base", "value": 0.2088 },
    { "category": "MMLU-Pro", "model": "Falcon-E-3B-Base", "value": 0.2716 },

    { "category": "Avg", "model": "Qwen2.5-3B", "value": 0.1810 },
    { "category": "Avg", "model": "Falcon-3-3B-Base", "value": 0.1574 },
    { "category": "Avg", "model": "Falcon-E-3B-Base", "value": 0.1832 }
]
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const p=[],t=p.length>0?p:Array.from(new Set(n.map(e=>e.model))),u=[],g=u.length>0?u:Array.from(new Set(n.map(e=>e.category))),b="Falcon-E-3B-Base",r=b||t[0],v=!0,e={top:40,right:30,bottom:40,left:60},m=800-e.left-e.right,i=430-e.top-e.bottom,a=d3.select("#chart-general-plt-3b").append("svg").attr("width",m+e.left+e.right).attr("height",i+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-general-plt-3b")||d3.select("body").append("div").attr("id","tooltip-general-plt-3b").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const h=d3.select("#tooltip-general-plt-3b"),d=d3.scaleBand().domain(g).range([0,m]).padding(.3);a.append("g").attr("transform",`translate(0,${i})`).call(d3.axisBottom(d).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove();const t=d3.select(this).append("foreignObject").attr("x",-50).attr("y",0).attr("width",100).attr("height",50);t.append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","center").style("width","100%").text(e)});const f=d3.scaleBand().domain(t).range([0,d.bandwidth()]).padding(.05);let s="0",o="0.50";if(s===null||o===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;s=s===null?Math.max(0,Math.min(...e)-t):s,o=o===null?Math.max(...e)+t:o}const l=d3.scaleLinear().domain([s,o]).range([i,0]);a.append("g").call(d3.axisLeft(l).tickFormat(e=>v?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");const c=e=>{if(e.model===r)return"#b987ff";const s=t.indexOf(e.model),o=t.length,n=128-s*(0-80)/(o-1||1);return`rgb(${n}, ${n+8}, ${n+16})`},j=e=>{if(e===r)return"#b088ff";const s=t.indexOf(e),o=t.length,i=128-s*(0-80)/(o-1||1),n=Math.min(i+30,230);return`rgb(${n}, ${n+8}, ${n+16})`};g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`category-group-${e}`);s.selectAll(`.bar-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",t=>d(e)+f(t.model)).attr("y",e=>l(e.value)).attr("width",f.bandwidth()).attr("height",e=>i-l(e.value)).attr("fill",e=>c(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){const e=d3.select(this).attr("data-model");return j(e)});const s=`<strong style="font-size: 16px;">${e}</strong><br/><span style="font-size: 12px;">`+t.map(e=>v?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>";h.style("opacity",1).html(s).style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",function(){const e=d3.select(this).attr("data-model");return c({model:e})}),h.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("transform","rotate(-90)").attr("y",-e.left+15).attr("x",-i/2).text("Performance %").style("font-family","inherit");const y=d3.select("#chart-general-plt-3b").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","-10px"),_=[...t].sort((e,n)=>e===r?-1:n===r?1:t.indexOf(e)-t.indexOf(n));_.forEach(e=>{const n=c({model:e}),t=y.append("div").style("display","flex").style("align-items","center").style("margin","0 10px");t.append("div").style("width","20px").style("height","20px").style("margin-right","5px").style("background-color",n),t.append("div").text(e)})})()</script><div id=chart-memory-plt-3b></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`
[
    { "category": "Memory Footprint", "model": "Qwen2.5-3B", "value": 6.46 },
    { "category": "Memory Footprint", "model": "Falcon-3-3B-Base", "value": 6.17 },
    { "category": "Memory Footprint", "model": "Falcon-E-3B-Base", "value": 0.999 }
]
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const p=[],t=p.length>0?p:Array.from(new Set(n.map(e=>e.model))),u=[],g=u.length>0?u:Array.from(new Set(n.map(e=>e.category))),b="Falcon-E-3B-Base",r=b||t[0],v=!1,e={top:40,right:30,bottom:40,left:60},m=800-e.left-e.right,i=430-e.top-e.bottom,a=d3.select("#chart-memory-plt-3b").append("svg").attr("width",m+e.left+e.right).attr("height",i+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-memory-plt-3b")||d3.select("body").append("div").attr("id","tooltip-memory-plt-3b").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const h=d3.select("#tooltip-memory-plt-3b"),d=d3.scaleBand().domain(g).range([0,m]).padding(.3);a.append("g").attr("transform",`translate(0,${i})`).call(d3.axisBottom(d).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove();const t=d3.select(this).append("foreignObject").attr("x",-50).attr("y",0).attr("width",100).attr("height",50);t.append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","center").style("width","100%").text(e)});const f=d3.scaleBand().domain(t).range([0,d.bandwidth()]).padding(.05);let s="0",o="7";if(s===null||o===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;s=s===null?Math.max(0,Math.min(...e)-t):s,o=o===null?Math.max(...e)+t:o}const l=d3.scaleLinear().domain([s,o]).range([i,0]);a.append("g").call(d3.axisLeft(l).tickFormat(e=>v?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");const c=e=>{if(e.model===r)return"#b987ff";const s=t.indexOf(e.model),o=t.length,n=128-s*(0-80)/(o-1||1);return`rgb(${n}, ${n+8}, ${n+16})`},j=e=>{if(e===r)return"#b088ff";const s=t.indexOf(e),o=t.length,i=128-s*(0-80)/(o-1||1),n=Math.min(i+30,230);return`rgb(${n}, ${n+8}, ${n+16})`};g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`category-group-${e}`);s.selectAll(`.bar-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",t=>d(e)+f(t.model)).attr("y",e=>l(e.value)).attr("width",f.bandwidth()).attr("height",e=>i-l(e.value)).attr("fill",e=>c(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){const e=d3.select(this).attr("data-model");return j(e)});const s=`<strong style="font-size: 16px;">${e}</strong><br/><span style="font-size: 12px;">`+t.map(e=>v?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>";h.style("opacity",1).html(s).style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",function(){const e=d3.select(this).attr("data-model");return c({model:e})}),h.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("transform","rotate(-90)").attr("y",-e.left+15).attr("x",-i/2).text("Memory Footprint (GB)").style("font-family","inherit");const y=d3.select("#chart-memory-plt-3b").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","-10px"),_=[...t].sort((e,n)=>e===r?-1:n===r?1:t.indexOf(e)-t.indexOf(n));_.forEach(e=>{const n=c({model:e}),t=y.append("div").style("display","flex").style("align-items","center").style("margin","0 10px");t.append("div").style("width","20px").style("height","20px").style("margin-right","5px").style("background-color",n),t.append("div").text(e)})})()</script><details><summary class=bold>Detailed result:</summary><table><thead><tr><th>Model</th><th>Nb Params</th><th>Mem Footprint</th><th>IFEVAL</th><th>Math-Hard</th><th>GPQA</th><th>MuSR</th><th>BBH</th><th>MMLU-Pro</th><th>Avg.</th></tr></thead><tbody><tr><td>Falcon-3-3B-Base</td><td>3B</td><td>6.46GB</td><td>15.74</td><td>11.78</td><td>6.26</td><td>6.27</td><td>21.58</td><td>18.09</td><td>15.74</td></tr><tr><td>Qwen2.5-3B</td><td>3B</td><td>6.17GB</td><td>26.9</td><td>14.8</td><td>6.38</td><td>11.76</td><td>24.3</td><td>24.48</td><td>18.1</td></tr><tr><td>Falcon-E-3B-Base</td><td>3B</td><td><strong>999MB</strong></td><td>36.67</td><td>13.45</td><td>8.67</td><td>4.14</td><td>19.83</td><td>27.16</td><td>18.32</td></tr></tbody></table></details><p>Below are the results for instruction fine-tuned models (for Qwen3 series, we disable the thinking mode during evaluation):</p><div id=chart-instruct-plt></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`
[
    { "category": "IFEVAL", "model": "Qwen2.5-0.5B", "value": 0.3153 },
    { "category": "IFEVAL", "model": "SmolLM2-360M", "value": 0.3842 },
    { "category": "IFEVAL", "model": "Qwen2.5-1.5B", "value": 0.4476 },
    { "category": "IFEVAL", "model": "Qwen3-0.6B", "value": 0.6215 },
    { "category": "IFEVAL", "model": "Qwen3-1.7B", "value": 0.7077 },
    { "category": "IFEVAL", "model": "SmolLM2-1.7B", "value": 0.5368 },
    { "category": "IFEVAL", "model": "Bitnet-b1.58-2B-4T", "value": 0.5911 },
    { "category": "IFEVAL", "model": "Falcon-3-1B-Instruct", "value": 0.5557 },
    { "category": "IFEVAL", "model": "Falcon-E-1B-Instruct", "value": 0.5435 },


    { "category": "Math-Hard", "model": "Qwen2.5-0.5B", "value": 0.1035 },
    { "category": "Math-Hard", "model": "Qwen3-0.6B", "value": 0.1615 },
    { "category": "Math-Hard", "model": "SmolLM2-360M", "value": 0.0151 },
    { "category": "Math-Hard", "model": "Qwen2.5-1.5B", "value": 0.2205 },
    { "category": "Math-Hard", "model": "Qwen3-1.7B", "value": 0.3817 },
    { "category": "Math-Hard", "model": "SmolLM2-1.7B", "value": 0.0582 },
    { "category": "Math-Hard", "model": "Bitnet-b1.58-2B-4T", "value": 0.07233 },
    { "category": "Math-Hard", "model": "Falcon-3-1B-Instruct", "value": 0.0634 },
    { "category": "Math-Hard", "model": "Falcon-E-1B-Instruct", "value": 0.0912 },

    { "category": "BBH", "model": "Qwen2.5-0.5B", "value": 0.0817 },
    { "category": "BBH", "model": "Qwen3-0.6B", "value": 0.0613 },
    { "category": "BBH", "model": "SmolLM2-360M", "value": 0.0417 },
    { "category": "BBH", "model": "Qwen2.5-1.5B", "value": 0.1981 },
    { "category": "BBH", "model": "Qwen3-1.7B", "value": 0.0926 },
    { "category": "BBH", "model": "SmolLM2-1.7B", "value": 0.1092 },
    { "category": "BBH", "model": "Bitnet-b1.58-2B-4T", "value": 0.1794},
    { "category": "BBH", "model": "Falcon-3-1B-Instruct", "value": 0.0932 },
    { "category": "BBH", "model": "Falcon-E-1B-Instruct", "value": 0.1650 },

    { "category": "MuSR", "model": "Qwen2.5-0.5B", "value": 0.0137 },
    { "category": "MuSR", "model": "Qwen3-0.6B", "value": 0.0186 },
    { "category": "MuSR", "model": "SmolLM2-360M", "value": 0.0277 },
    { "category": "MuSR", "model": "Qwen2.5-1.5B", "value": 0.0319 },
    { "category": "MuSR", "model": "Qwen3-1.7B", "value": 0.0831 },
    { "category": "MuSR", "model": "SmolLM2-1.7B", "value": 0.041 },
    { "category": "MuSR", "model": "Bitnet-b1.58-2B-4T", "value": 0.0174 },
    { "category": "MuSR", "model": "Falcon-3-1B-Instruct", "value": 0.1056 },
    { "category": "MuSR", "model": "Falcon-E-1B-Instruct", "value": 0.0251 },

    { "category": "MMLU-Pro", "model": "Qwen2.5-0.5B", "value": 0.08 },
    { "category": "MMLU-Pro", "model": "Qwen3-0.6B", "value": 0.0771 },
    { "category": "MMLU-Pro", "model": "SmolLM2-360M", "value": 0.013 },
    { "category": "MMLU-Pro", "model": "Qwen2.5-1.5B", "value": 0.1999 },
    { "category": "MMLU-Pro", "model": "Qwen3-1.7B", "value": 0.1504 },
    { "category": "MMLU-Pro", "model": "SmolLM2-1.7B", "value": 0.1171 },
    { "category": "MMLU-Pro", "model": "Bitnet-b1.58-2B-4T", "value": 0.1482 },
    { "category": "MMLU-Pro", "model": "Falcon-3-1B-Instruct", "value": 0.1296 },
    { "category": "MMLU-Pro", "model": "Falcon-E-1B-Instruct", "value": 0.1942 },

    { "category": "GPQA", "model": "Qwen2.5-0.5B", "value": 0.0123 },
    { "category": "GPQA", "model": "Qwen3-0.6B", "value": 0.028 },
    { "category": "GPQA", "model": "SmolLM2-360M", "value": 0.0067 },
    { "category": "GPQA", "model": "Qwen2.5-1.5B", "value": 0.0078 },
    { "category": "GPQA", "model": "Qwen3-1.7B", "value": 0.0329 },
    { "category": "GPQA", "model": "SmolLM2-1.7B", "value": 0.0 },
    { "category": "GPQA", "model": "Bitnet-b1.58-2B-4T", "value": 0.0525},
    { "category": "GPQA", "model": "Falcon-3-1B-Instruct", "value": 0.0932 },
    { "category": "GPQA", "model": "Falcon-E-1B-Instruct", "value": 0.0964 },

    { "category": "Avg", "model": "Qwen2.5-0.5B", "value": 0.1011 },
    { "category": "Avg", "model": "Qwen3-0.6B", "value": 0.1613 },
    { "category": "Avg", "model": "SmolLM2-360M", "value": 0.0814 },
    { "category": "Avg", "model": "Qwen2.5-1.5B", "value": 0.1843 },
    { "category": "Avg", "model": "Qwen3-1.7B", "value": 0.2414 },
    { "category": "Avg", "model": "SmolLM2-1.7B", "value": 0.1502 },
    { "category": "Avg", "model": "Bitnet-b1.58-2B-4T", "value": 0.1770},
    { "category": "Avg", "model": "Falcon-3-1B-Instruct", "value": 0.1616 },
    { "category": "Avg", "model": "Falcon-E-1B-Instruct", "value": 0.1859 }
]
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const p=[],t=p.length>0?p:Array.from(new Set(n.map(e=>e.model))),u=[],g=u.length>0?u:Array.from(new Set(n.map(e=>e.category))),b="Falcon-E-1B-Instruct",r=b||t[0],v=!0,e={top:40,right:30,bottom:40,left:60},m=800-e.left-e.right,i=430-e.top-e.bottom,a=d3.select("#chart-instruct-plt").append("svg").attr("width",m+e.left+e.right).attr("height",i+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-instruct-plt")||d3.select("body").append("div").attr("id","tooltip-instruct-plt").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const h=d3.select("#tooltip-instruct-plt"),d=d3.scaleBand().domain(g).range([0,m]).padding(.3);a.append("g").attr("transform",`translate(0,${i})`).call(d3.axisBottom(d).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove();const t=d3.select(this).append("foreignObject").attr("x",-50).attr("y",0).attr("width",100).attr("height",50);t.append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","center").style("width","100%").text(e)});const f=d3.scaleBand().domain(t).range([0,d.bandwidth()]).padding(.05);let s="0",o="0.50";if(s===null||o===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;s=s===null?Math.max(0,Math.min(...e)-t):s,o=o===null?Math.max(...e)+t:o}const l=d3.scaleLinear().domain([s,o]).range([i,0]);a.append("g").call(d3.axisLeft(l).tickFormat(e=>v?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");const c=e=>{if(e.model===r)return"#b987ff";const s=t.indexOf(e.model),o=t.length,n=128-s*(0-80)/(o-1||1);return`rgb(${n}, ${n+8}, ${n+16})`},j=e=>{if(e===r)return"#b088ff";const s=t.indexOf(e),o=t.length,i=128-s*(0-80)/(o-1||1),n=Math.min(i+30,230);return`rgb(${n}, ${n+8}, ${n+16})`};g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`category-group-${e}`);s.selectAll(`.bar-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",t=>d(e)+f(t.model)).attr("y",e=>l(e.value)).attr("width",f.bandwidth()).attr("height",e=>i-l(e.value)).attr("fill",e=>c(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){const e=d3.select(this).attr("data-model");return j(e)});const s=`<strong style="font-size: 16px;">${e}</strong><br/><span style="font-size: 12px;">`+t.map(e=>v?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>";h.style("opacity",1).html(s).style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",function(){const e=d3.select(this).attr("data-model");return c({model:e})}),h.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("transform","rotate(-90)").attr("y",-e.left+15).attr("x",-i/2).text("Performance %").style("font-family","inherit");const y=d3.select("#chart-instruct-plt").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","-10px"),_=[...t].sort((e,n)=>e===r?-1:n===r?1:t.indexOf(e)-t.indexOf(n));_.forEach(e=>{const n=c({model:e}),t=y.append("div").style("display","flex").style("align-items","center").style("margin","0 10px");t.append("div").style("width","20px").style("height","20px").style("margin-right","5px").style("background-color",n),t.append("div").text(e)})})()</script><details><summary class=bold>Detailed results:</summary><table><thead><tr><th>Model</th><th>Nb Params</th><th>Mem Footprint</th><th>IFEVAL</th><th>Math-Hard</th><th>GPQA</th><th>MuSR</th><th>BBH</th><th>MMLU-Pro</th><th>Avg.</th></tr></thead><tbody><tr><td>Qwen2.5-0.5B-Instruct</td><td>500M</td><td>1GB</td><td>31.53</td><td>10.35</td><td>1.23</td><td>1.37</td><td>8.17</td><td>8.00</td><td>10.11</td></tr><tr><td>Qwen3-0.6B</td><td>600M</td><td>1.5GB</td><td>62.15</td><td>16.15</td><td>2.81</td><td>1.86</td><td>6.13</td><td>7.71</td><td>16.13</td></tr><tr><td>SmolLM2-360M-Instruct</td><td>360M</td><td>720MB</td><td>38.42</td><td>1.51</td><td>0.67</td><td>2.77</td><td>4.17</td><td>1.3</td><td>8.14</td></tr><tr><td>Qwen2.5-1.5B-Instruct</td><td>1.5B</td><td>3.1GB</td><td>44.76</td><td>22.05</td><td>0.78</td><td>3.19</td><td>19.91</td><td>19.99</td><td>18.43</td></tr><tr><td>Qwen3-1.7B</td><td>1.7B</td><td>4.02GB</td><td>70.76</td><td>38.17</td><td>3.29</td><td>8.32</td><td>9.26</td><td>15.04</td><td>24.14</td></tr><tr><td>SmolLM2-1.7B</td><td>1.7B</td><td>3.4GB</td><td>53.68</td><td>5.82</td><td>0</td><td>4.1</td><td>10.92</td><td>11.71</td><td>15.02</td></tr><tr><td>Bitnet-b1.58-2B-4T</td><td>2B</td><td>1.18GB</td><td>59.11</td><td>7.23</td><td>5.25</td><td>1.74</td><td>17.94</td><td>14.82</td><td>17.70</td></tr><tr><td>Falcon-3-1B-Instruct</td><td>1.5B</td><td>3GB</td><td>55.57</td><td>6.34</td><td>2.24</td><td>10.56</td><td>12.96</td><td>9.32</td><td>16.16</td></tr><tr><td>Falcon-E-1B-Instruct</td><td>1.8B</td><td><strong>665MB</strong></td><td>54.35</td><td>9.12</td><td>9.64</td><td>2.51</td><td>19.42</td><td>16.5</td><td>18.59</td></tr></tbody></table></details><h4 id=for-3b-scale-models-1>For 3B scale models:<a hidden class=anchor aria-hidden=true href=#for-3b-scale-models-1>#</a></h4><div id=chart-instruct-plt-3b></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`
[
    { "category": "IFEVAL", "model": "Falcon-3-3B-Instruct", "value": 0.6977 },
    { "category": "IFEVAL", "model": "Qwen2.5-3B-Instruct", "value": 0.6475 },
    { "category": "IFEVAL", "model": "Falcon-E-3B-Instruct", "value": 0.6097 },

    { "category": "Math-Hard", "model": "Falcon-3-3B-Instruct", "value": 0.25 },
    { "category": "Math-Hard", "model": "Qwen2.5-3B-Instruct", "value": 0.3678 },
    { "category": "Math-Hard", "model": "Falcon-E-3B-Instruct", "value": 0.1530 },

    { "category": "MMLU-Pro", "model": "Falcon-3-3B-Instruct", "value": 0.2228 },
    { "category": "MMLU-Pro", "model": "Qwen2.5-3B-Instruct", "value": 0.258 },
    { "category": "MMLU-Pro", "model": "Falcon-E-3B-Instruct", "value": 0.2359 },

    { "category": "MuSR", "model": "Falcon-3-3B-Instruct", "value": 0.1113 },
    { "category": "MuSR", "model": "Qwen2.5-3B-Instruct", "value": 0.0757 },
    { "category": "MuSR", "model": "Falcon-E-3B-Instruct", "value": 0.0212 },

    { "category": "BBH", "model": "Falcon-3-3B-Instruct", "value": 0.2629 },
    { "category": "BBH", "model": "Qwen2.5-3B-Instruct", "value": 0.258 },
    { "category": "BBH", "model": "Falcon-E-3B-Instruct", "value": 0.2359 },

    { "category": "GPQA", "model": "Falcon-3-3B-Instruct", "value": 0.0515 },
    { "category": "GPQA", "model": "Qwen2.5-3B-Instruct", "value": 0.0302 },
    { "category": "GPQA", "model": "Falcon-E-3B-Instruct", "value": 0.0745 },

    { "category": "Avg", "model": "Falcon-3-3B-Instruct", "value": 0.266 },
    { "category": "Avg", "model": "Qwen2.5-3B-Instruct", "value": 0.2716 },
    { "category": "Avg", "model": "Falcon-E-3B-Instruct", "value": 0.2265 }
]
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const p=[],t=p.length>0?p:Array.from(new Set(n.map(e=>e.model))),u=[],g=u.length>0?u:Array.from(new Set(n.map(e=>e.category))),b="Falcon-E-3B-Instruct",r=b||t[0],v=!0,e={top:40,right:30,bottom:40,left:60},m=800-e.left-e.right,i=430-e.top-e.bottom,a=d3.select("#chart-instruct-plt-3b").append("svg").attr("width",m+e.left+e.right).attr("height",i+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-instruct-plt-3b")||d3.select("body").append("div").attr("id","tooltip-instruct-plt-3b").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const h=d3.select("#tooltip-instruct-plt-3b"),d=d3.scaleBand().domain(g).range([0,m]).padding(.3);a.append("g").attr("transform",`translate(0,${i})`).call(d3.axisBottom(d).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove();const t=d3.select(this).append("foreignObject").attr("x",-50).attr("y",0).attr("width",100).attr("height",50);t.append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","center").style("width","100%").text(e)});const f=d3.scaleBand().domain(t).range([0,d.bandwidth()]).padding(.05);let s="0",o="0.70";if(s===null||o===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;s=s===null?Math.max(0,Math.min(...e)-t):s,o=o===null?Math.max(...e)+t:o}const l=d3.scaleLinear().domain([s,o]).range([i,0]);a.append("g").call(d3.axisLeft(l).tickFormat(e=>v?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");const c=e=>{if(e.model===r)return"#b987ff";const s=t.indexOf(e.model),o=t.length,n=128-s*(0-80)/(o-1||1);return`rgb(${n}, ${n+8}, ${n+16})`},j=e=>{if(e===r)return"#b088ff";const s=t.indexOf(e),o=t.length,i=128-s*(0-80)/(o-1||1),n=Math.min(i+30,230);return`rgb(${n}, ${n+8}, ${n+16})`};g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`category-group-${e}`);s.selectAll(`.bar-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",t=>d(e)+f(t.model)).attr("y",e=>l(e.value)).attr("width",f.bandwidth()).attr("height",e=>i-l(e.value)).attr("fill",e=>c(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){const e=d3.select(this).attr("data-model");return j(e)});const s=`<strong style="font-size: 16px;">${e}</strong><br/><span style="font-size: 12px;">`+t.map(e=>v?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>";h.style("opacity",1).html(s).style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",function(){const e=d3.select(this).attr("data-model");return c({model:e})}),h.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("transform","rotate(-90)").attr("y",-e.left+15).attr("x",-i/2).text("Performance %").style("font-family","inherit");const y=d3.select("#chart-instruct-plt-3b").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","-10px"),_=[...t].sort((e,n)=>e===r?-1:n===r?1:t.indexOf(e)-t.indexOf(n));_.forEach(e=>{const n=c({model:e}),t=y.append("div").style("display","flex").style("align-items","center").style("margin","0 10px");t.append("div").style("width","20px").style("height","20px").style("margin-right","5px").style("background-color",n),t.append("div").text(e)})})()</script><details><summary class=bold>Detailed results:</summary><table><thead><tr><th>Model</th><th>Nb Params</th><th>Mem Footprint</th><th>IFEVAL</th><th>Math-Hard</th><th>GPQA</th><th>MuSR</th><th>BBH</th><th>MMLU-Pro</th><th>Avg.</th></tr></thead><tbody><tr><td>Falcon-3-3B-Instruct</td><td>3B</td><td>6.46GB</td><td>69.77</td><td>25</td><td>5.15</td><td>11.13</td><td>26.29</td><td>22.28</td><td>26.6</td></tr><tr><td>Qwen2.5-3B-Instruct</td><td>3B</td><td>6.17GB</td><td>64.75</td><td>36.78</td><td>3.02</td><td>7.57</td><td>25.80</td><td>25.05</td><td>27.16</td></tr><tr><td>Falcon-E-3B-Instruct</td><td>3B</td><td><strong>999MB</strong></td><td>60.97</td><td>15.3</td><td>7.45</td><td>2.12</td><td>23.59</td><td>26.45</td><td>22.65</td></tr></tbody></table></details><p>Additional results (leaderboard v1) on comparing our instructed models with Microsoft&rsquo;s new BitNet model:</p><table><thead><tr><th>Model</th><th>Nb Params</th><th>Mem Footprint</th><th>ARC-Challenge</th><th>GSM8K</th><th>HellaSwag</th><th>MMLU</th><th>TruthfulQA</th><th>Average</th></tr></thead><tbody><tr><td>Bitnet-b1.58-2B-4T</td><td>2B</td><td>1.18GB</td><td>38.31</td><td>65.27</td><td>59.02</td><td>47.43</td><td>47.65</td><td>51.54</td></tr><tr><td>Falcon-E-1B-Instruct</td><td>1.8B</td><td>665MB</td><td>36.60</td><td>54.74</td><td>50.19</td><td>48.33</td><td>42.09</td><td>46.39</td></tr><tr><td>Falcon-E-3B-Instruct</td><td>3B</td><td>999MB</td><td>43.09</td><td>64.52</td><td>56.97</td><td>55.70</td><td>45.58</td><td>53.17</td></tr></tbody></table><p><em>Falcon-Edge</em> demonstrates on-par and better performances than models of comparable sizes on the leaderboard v2 tasks, demonstrating that it is possible to train powerful BitNet models on desired domains while being competitive enough on other tasks.</p><h3 id=falcon-edge-a-series-of-universal-models><em>Falcon-Edge</em>, a series of universal models<a hidden class=anchor aria-hidden=true href=#falcon-edge-a-series-of-universal-models>#</a></h3><p>If we look closer at the formula of the BitNet linear layer for inference (in terms of Python code):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>activation_norm_quant</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=mf>127.0</span> <span class=o>/</span> <span class=n>x</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>values</span><span class=o>.</span><span class=n>clamp_</span><span class=p>(</span><span class=nb>min</span><span class=o>=</span><span class=mf>1e-5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>*</span> <span class=n>scale</span><span class=p>)</span><span class=o>.</span><span class=n>round</span><span class=p>()</span><span class=o>.</span><span class=n>clamp_</span><span class=p>(</span><span class=o>-</span><span class=mi>128</span><span class=p>,</span> <span class=mi>127</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>y</span><span class=p>,</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BitLinear</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>post_quant_process</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=nb>input</span><span class=p>,</span> <span class=n>input_scale</span><span class=p>,</span> <span class=n>weight_scale</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=nb>input</span> <span class=o>/</span> <span class=p>(</span><span class=n>input_scale</span> <span class=o>*</span> <span class=n>weight_scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=nb>input</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>w</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl>        <span class=n>w_quant</span> <span class=o>=</span> <span class=n>unpack_weights</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>input_quant</span><span class=p>,</span> <span class=n>input_scale</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>activation_quant</span><span class=p>(</span><span class=nb>input</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>input_quant</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dtype</span><span class=p>),</span> <span class=n>w_quant</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>post_quant_process</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight_scale</span><span class=p>,</span> <span class=n>input_scale</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>y</span> <span class=o>+=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>y</span>
</span></span></code></pre></div><p>The normalization <code>activation_norm_quant</code> quantizes the activations in <code>int8</code> format, then the activation is computed back in half precision by diving it by <code>x_scale</code>. Since the model has been trained with fake 8-bit activation quantization, we argue that it is possible to approximate that:</p><pre tabindex=0><code>x_quant, x_scale = activation_norm_quant(x)
x ~= (x_quant / x_scale)
</code></pre><p>Therefore, instead of quantizing the model post-training, injecting the weight scale after quantizing the weights should lead to a good enough &ldquo;approximation&rdquo; of the non-BitNet version of the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>_weight_quant</span><span class=p>(</span><span class=n>w</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>scale</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=n>w</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>clamp_</span><span class=p>(</span><span class=nb>min</span><span class=o>=</span><span class=mf>1e-05</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>u</span> <span class=o>=</span> <span class=p>(</span><span class=n>w</span> <span class=o>*</span> <span class=n>scale</span><span class=p>)</span><span class=o>.</span><span class=n>round</span><span class=p>()</span><span class=o>.</span><span class=n>clamp_</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>u</span><span class=p>,</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>param_name</span><span class=p>,</span> <span class=n>param_value</span> <span class=ow>in</span> <span class=n>state_dict</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>_is_param_to_not_quantize</span><span class=p>(</span><span class=n>param_name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>continue</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>param_value</span><span class=p>,</span> <span class=n>param_scale</span> <span class=o>=</span> <span class=n>_weight_quant</span><span class=p>(</span><span class=n>param_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>param_value</span> <span class=o>=</span> <span class=n>param_value</span> <span class=o>/</span> <span class=n>param_scale</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>state_dict_quant</span><span class=p>[</span><span class=n>param_name</span><span class=p>]</span> <span class=o>=</span> <span class=n>param_value</span>
</span></span></code></pre></div><p>We confirm this by running end-to-end evaluations on the bfloat16 variant of our 1B and 3B base models and below are the results:</p><table><thead><tr><th>Model</th><th>IFEVAL</th><th>Math-Hard</th><th>GPQA</th><th>MuSR</th><th>BBH</th><th>MMLU-Pro</th><th>Avg.</th></tr></thead><tbody><tr><td>Falcon-E-1B</td><td>32.9</td><td>10.97</td><td>2.8</td><td>3.65</td><td>12.28</td><td>17.82</td><td>13.40</td></tr><tr><td>Falcon-E-1B-bf16</td><td>29.89</td><td>11.23</td><td>1.8</td><td>3.32</td><td>12.27</td><td>18.04</td><td>12.75</td></tr><tr><td>Falcon-E-3B</td><td>36.67</td><td>13.45</td><td>8.67</td><td>4.14</td><td>19.83</td><td>27.16</td><td>18.32</td></tr><tr><td>Falcon-E-3B-bf16</td><td>34.84</td><td>13.21</td><td>8.91</td><td>4.88</td><td>20.3</td><td>27.00</td><td>18.19</td></tr></tbody></table><p>The <code>bfloat16</code> counterparts of the models can be loaded directly via Hugging Face transformers by passing <code>revision="bfloat16"</code> in the <code>from_pretrained</code> function:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>SFTTrainer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;tiiuae/Falcon-E-1B-Base&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>,</span> <span class=n>revision</span><span class=o>=</span><span class=s2>&#34;prequantized&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>revision</span><span class=o>=</span><span class=s2>&#34;bfloat16&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h3 id=falcon-edge-a-series-of-fine-tunable-bitnet-models><em>Falcon-Edge</em>, a series of fine-tunable Bitnet models<a hidden class=anchor aria-hidden=true href=#falcon-edge-a-series-of-fine-tunable-bitnet-models>#</a></h3><p>To the best of our knowledge, <a href=https://huggingface.co/collections/microsoft/bitnet-67fddfe39a03686367734550>except from the most recent release from Microsoft</a> previous BitNet releases only focus on releasing the final quantized model, making it usable only for inference. Similarly to the release from Microsoft, we propose to extend the accessibility of research and application of BitNet models by releasing their pre-quantized weights. That way, users can either perform fine-tuning on their target domain, or do continuous pre-training of the BitNet checkpoint as long as <code>nn.Linear</code> layers are replaced by <code>BitnetLinear</code> layers, and by making sure to quantize the model post training in BitNet format. Since the weights corresponds to the pre-quantized weights, performing text generation without replacing the <code>nn.Linear</code> layers with <code>BitnetLinear</code> layers will produce gibberish output.</p><p>The pre-quantized weights can be downloaded via Hugging Face&rsquo;s transformers library by specifying the <code>revision</code> argument to be <code>prequantized</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;tiiuae/Falcon-E-1B-Base&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>,</span> <span class=n>revision</span><span class=o>=</span><span class=s2>&#34;prequantized&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>revision</span><span class=o>=</span><span class=s2>&#34;prequantized&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>This way, we will help fostering an ecosystem around first powerful 1-bit fine-tunes by the community. We provide to community the tools to get easily started and fine-tune their own version of powerful BitNet models by packaging all needed utility methods for performing fine-tuning on the pre-quantized weights on a Python package called <code>onebitllms</code> that we will cover in the next section.</p><h2 id=introducing-onebitllms---a-lightweight-python-package-for-1-bit-llms-training-toolkit>Introducing <code>onebitllms</code> - a lightweight python package for 1-bit LLMs training toolkit<a hidden class=anchor aria-hidden=true href=#introducing-onebitllms---a-lightweight-python-package-for-1-bit-llms-training-toolkit>#</a></h2><p><a id=lib-logo></a></p><div style=display:flex;justify-content:center><img src=./onebitllms-logo.png alt="Library logo" style=width:20%;max-width:1000px;min-width:400px;height:auto></div><p>In this release, we also introduce <code>onebitllms</code> - a lightweight Python package that can be plugged into your favorite LLM fine-tuning tools in order to fine-tune any pre-quantized BitNet model. At this time of writing <code>onebitllms</code> exposes these main functionalities:</p><ul><li>Utility method to convert the prequantized model checkpoints into BitNet training format in order to pass it to any of your favorite LLM fine-tuning framework. We currently tested our library with Hugging Face&rsquo;s <code>trl</code> library.</li><li>Utility method to quantize the trained checkpoint in BitNet format as well as in usual <code>bfloat16</code> format.</li><li>Fore more fine-grained control: Bare <code>BitnetLinear</code> and triton kernels that be injected and used for your pre-training framework.</li></ul><p>Currently, only full-finetuning is supported through this framework, while in this release the model sizes are relatively small, supporting Parameter-Efficient Fine-tuning (PEFT) methods for BitNet models remains an exciting and impactful open question for upcoming BitNet models.</p><p>To get started, simply install the package directly through <code>pip</code> or from source, and take a look at <code>examples/</code> folders inside the <a href=https://github.com/tiiuae/onebitllms>source code</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>trl</span> <span class=kn>import</span> <span class=n>SFTTrainer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>onebitllms</span> <span class=kn>import</span> <span class=n>replace_linear_with_bitnet_linear</span><span class=p>,</span> <span class=n>quantize_to_1bit</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;tiiuae/Falcon-E-1B-Base&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>,</span> <span class=n>revision</span><span class=o>=</span><span class=s2>&#34;prequantized&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>revision</span><span class=o>=</span><span class=s2>&#34;prequantized&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>replace_linear_with_bitnet_linear</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>SFTTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>quantize_to_1bit</span><span class=p>(</span><span class=n>output_directory</span><span class=p>)</span>
</span></span></code></pre></div><p>With this package, we hope to accelerate research and development around ternary format LLMs, and hope to see many derivations of <em>Falcon-Edge</em> and other future powerful BitNet models developed by the community.</p><h2 id=going-further>Going further<a hidden class=anchor aria-hidden=true href=#going-further>#</a></h2><p>We believe this release opens up multiple interesting directions - among all the possible follow up directions, we currently think that the following open questions will make BitNet models much more impactful in the near future:</p><ul><li><em>Writing more powerful GPU inference kernels for BitNet architecture</em>: leveraging the same core ideas behind <a href=https://github.com/microsoft/BitNet><code>bitnet.cpp</code></a>, we hope that this release will convince the research community to focus on developping powerful BitNet inference kernels for faster inference on GPUs - thus making them faster than native models on GPUs.</li><li><em>Support PEFT methods for BitNet fine-tuning</em>: This remains an unexplored research question that can open up multiple new possibilities for BitNet models.</li><li><em>More rigourous investigation on the universality of Bitnet checkpoints</em>: While we observe that simply injecting the weight scale leads to having a descent non-Bitnet checkpoint, we believe that more research can be done to minimize the performance degradation between the Bitnet checkpoint and its <code>bfloat16</code> counterpart, thus making it fully performance degradation-free.</li><li><em>On multi-modal Bitnet models</em>: We hope these Bitnet foundational models together with <code>onebitllms</code> package can serve a as a foundational work for creating first multi-modal Bitnet VLM (Vision Language Model) etc.</li><li><em>More optimized Bitnet training kernels</em>: To write our kernels, we decided to take a two stages approach to first compute the global maximum to later use it block-wise for normalization. This approach can be revised to write more efficient kernels. In our tests, we estimate the overhead to be around ~20% between non-Bitnet pre-training against Bitnet pre-training. We will release soon more extensive numbers on the overhead introduced by Bitnet for training.</li></ul><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you find this work useful for your research and work, please consider citing our work, as well as citing all the foundational work behind BitNet models:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-latex data-lang=latex><span class=line><span class=cl>@misc<span class=nb>{</span>tiionebitllms,
</span></span><span class=line><span class=cl>    title = <span class=nb>{</span>Falcon-E, a series of powerful, universal and fine-tunable 1.58bit language models.<span class=nb>}</span>,
</span></span><span class=line><span class=cl>    author = <span class=nb>{</span>Falcon-LLM Team<span class=nb>}</span>,
</span></span><span class=line><span class=cl>    month = <span class=nb>{</span>May<span class=nb>}</span>,
</span></span><span class=line><span class=cl>    url = <span class=nb>{</span>https://falcon-lm.github.io/blog/falcon-edge<span class=nb>}</span>,
</span></span><span class=line><span class=cl>    year = <span class=nb>{</span>2025<span class=nb>}</span>
</span></span><span class=line><span class=cl><span class=nb>}</span>
</span></span></code></pre></div><details><summary class=bold>More References</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-latex data-lang=latex><span class=line><span class=cl>@misc<span class=nb>{</span>ma2025bitnetb1582b4ttechnical,
</span></span><span class=line><span class=cl>      title=<span class=nb>{</span>BitNet b1.58 2B4T Technical Report<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      author=<span class=nb>{</span>Shuming Ma and Hongyu Wang and Shaohan Huang and Xingxing Zhang and Ying Hu and Ting Song and Yan Xia and Furu Wei<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      year=<span class=nb>{</span>2025<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      eprint=<span class=nb>{</span>2504.12285<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      archivePrefix=<span class=nb>{</span>arXiv<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      primaryClass=<span class=nb>{</span>cs.CL<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      url=<span class=nb>{</span>https://arxiv.org/abs/2504.12285<span class=nb>}</span>,
</span></span><span class=line><span class=cl><span class=nb>}</span>
</span></span><span class=line><span class=cl>@misc<span class=nb>{</span>wang2025bitnetcppefficientedgeinference,
</span></span><span class=line><span class=cl>      title=<span class=nb>{</span>Bitnet.cpp: Efficient Edge Inference for Ternary LLMs<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      author=<span class=nb>{</span>Jinheng Wang and Hansong Zhou and Ting Song and Shijie Cao and Yan Xia and Ting Cao and Jianyu Wei and Shuming Ma and Hongyu Wang and Furu Wei<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      year=<span class=nb>{</span>2025<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      eprint=<span class=nb>{</span>2502.11880<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      archivePrefix=<span class=nb>{</span>arXiv<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      primaryClass=<span class=nb>{</span>cs.LG<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      url=<span class=nb>{</span>https://arxiv.org/abs/2502.11880<span class=nb>}</span>,
</span></span><span class=line><span class=cl><span class=nb>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-latex data-lang=latex><span class=line><span class=cl>@misc<span class=nb>{</span>,
</span></span><span class=line><span class=cl>      title=<span class=nb>{</span>1.58-Bit LLM: A New Era of Extreme Quantization<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      author=<span class=nb>{</span>Mohamed Mekkouri and Marc Sun and Leandro von Werra and Thomas Wolf<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      year=<span class=nb>{</span>2024<span class=nb>}</span>,
</span></span><span class=line><span class=cl><span class=nb>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-latex data-lang=latex><span class=line><span class=cl>@misc<span class=nb>{</span>ma2024era1bitllmslarge,
</span></span><span class=line><span class=cl>      title=<span class=nb>{</span>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      author=<span class=nb>{</span>Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      year=<span class=nb>{</span>2024<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      eprint=<span class=nb>{</span>2402.17764<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      archivePrefix=<span class=nb>{</span>arXiv<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      primaryClass=<span class=nb>{</span>cs.CL<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      url=<span class=nb>{</span>https://arxiv.org/abs/2402.17764<span class=nb>}</span>,
</span></span><span class=line><span class=cl><span class=nb>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-latex data-lang=latex><span class=line><span class=cl>@misc<span class=nb>{</span>wang2023bitnetscaling1bittransformers,
</span></span><span class=line><span class=cl>      title=<span class=nb>{</span>BitNet: Scaling 1-bit Transformers for Large Language Models<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      author=<span class=nb>{</span>Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      year=<span class=nb>{</span>2023<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      eprint=<span class=nb>{</span>2310.11453<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      archivePrefix=<span class=nb>{</span>arXiv<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      primaryClass=<span class=nb>{</span>cs.CL<span class=nb>}</span>,
</span></span><span class=line><span class=cl>      url=<span class=nb>{</span>https://arxiv.org/abs/2310.11453<span class=nb>}</span>,
</span></span><span class=line><span class=cl><span class=nb>}</span>
</span></span></code></pre></div></details><div class=post-contributors><div class=contributors-section><h4>Core Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/younes_belkada.jpg alt="Younes Belkada" class=contributor-image><p class=contributor-name>Younes Belkada</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/qiyang_zhao.jpg alt="Qiyang Zhao" class=contributor-image><p class=contributor-name>Qiyang Zhao</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/hang_zou.png alt="Hang Zou" class=contributor-image><p class=contributor-name>Hang Zou</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/dhia_eddine_rhaiem.jpg alt="DhiaEddine Rhaiem" class=contributor-image><p class=contributor-name>DhiaEddine Rhaiem</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/ilyas_chahed.jpg alt="Ilyas Chahed" class=contributor-image><p class=contributor-name>Ilyas Chahed</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/maksim_velikanov.jpg alt="Maksim Velikanov" class=contributor-image><p class=contributor-name>Maksim Velikanov</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/jingwei_zuo.jpg alt="Jingwei Zuo" class=contributor-image><p class=contributor-name>Jingwei Zuo</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/mikhail_lubinets.jpg alt="Mikhail Lubinets" class=contributor-image><p class=contributor-name>Mikhail Lubinets</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/hakim_hacid.png alt="Hakim Hacid" class=contributor-image><p class=contributor-name>Hakim Hacid</p></div></div></div></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span><a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html rel="noopener noreferrer" target=_blank>| Terms and Conditions</a>
</span><span><a href=https://www.tii.ae/privacy-policy rel="noopener noreferrer" target=_blank>| Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>