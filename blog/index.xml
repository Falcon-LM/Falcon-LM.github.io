<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on Falcon</title>
    <link>https://falcon-lm.github.io/blog/</link>
    <description>Recent content in Blog on Falcon</description>
    <image>
      <url>https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 04 Feb 2026 08:00:00 +0000</lastBuildDate><atom:link href="https://falcon-lm.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Falcon‑H1R-FP8: Accelerating Inference with Quantized Precision</title>
      <link>https://falcon-lm.github.io/blog/falcon-h1r-7b-fp8/</link>
      <pubDate>Wed, 04 Feb 2026 08:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-h1r-7b-fp8/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chat.falconllm.tii.ae&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Falcon CHAT&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon-h1r&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt;
&lt;a href=&#34;https://discord.gg/Cbek57PrZE&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DISCORD&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Introducing &lt;span class=&#34;bold&#34;&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/Falcon-H1R-7B-FP8&#34;&gt;Falcon H1R 7B FP8&lt;/a&gt;&lt;/span&gt;, a fully quantized version of the &lt;a href=&#34;https://huggingface.co/tiiuae/Falcon-H1R-7B&#34;&gt;Falcon H1R 7B&lt;/a&gt; model that packs both weights and activations into FP8 format. Using &lt;a href=&#34;https://github.com/NVIDIA/Model-Optimizer&#34;&gt;NVIDIA Model Optimizer&lt;/a&gt; and post-training quantization (PTQ) workflow, the FP8 quantized model preserves the original BF16 quality performance while delivering a 1.2×–1.5× throughput boost and halving GPU memory footprint.&lt;/p&gt;
&lt;h1 id=&#34;evaluations&#34;&gt;Evaluations&lt;/h1&gt;
&lt;p&gt;The FP8 variant retains essentially the same accuracy as BF16 across all three major reasoning tasks: AIME25 drops only 0.8 % (from 83.1 % to 82.3 %), LCB‑v6 falls by 1 % (68.6 % → 67.6 %), and GPQA‑D shows a negligible 0.1 % difference (61.3 % → 61.2 %). These results confirm that the FP8 PTQ preserves benchmark performance while delivering substantial memory and throughput gains.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling</title>
      <link>https://falcon-lm.github.io/blog/falcon-h1r-7b/</link>
      <pubDate>Mon, 05 Jan 2026 08:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-h1r-7b/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chat.falconllm.tii.ae&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Falcon CHAT&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon-h1r&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/spaces/tiiuae/Falcon-H1R-playground&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DEMO&lt;/a&gt;
&lt;a href=&#34;https://discord.gg/Cbek57PrZE&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DISCORD&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;introducing-falcon-h1r-7b&#34;&gt;Introducing Falcon H1R 7B&lt;/h1&gt;
&lt;p&gt;We’re excited to unveil &lt;span class=&#34;bold&#34;&gt;Falcon H1R 7B&lt;/span&gt;, a decoder-only large language model, developed by the &lt;a href=&#34;https://www.tii.ae/ai-and-digital-science&#34;&gt;Technology Innovation Institute (TII)&lt;/a&gt; in Abu Dhabi. Building upon the robust foundation of Falcon-H1 Base model, &lt;span class=&#34;bold&#34;&gt;Falcon H1R 7B&lt;/span&gt; takes a major leap forward in reasoning capabilities.&lt;/p&gt;
&lt;p&gt;Despite its modest 7 billion‑parameter size, &lt;span class=&#34;bold&#34;&gt;Falcon H1R 7B&lt;/span&gt; matches or outperforms state‑of‑the‑art reasoning models that are 2–7× larger, proving its exceptional parameter efficiency and does so consistently across a wide range of reasoning‑intensive benchmarks.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture</title>
      <link>https://falcon-lm.github.io/blog/falcon-h1-arabic/</link>
      <pubDate>Mon, 05 Jan 2026 01:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-h1-arabic/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Check out the &lt;a href=&#34;https://falcon-lm.github.io/ar/blog/falcon-h1-arabic/&#34;&gt;Arabic version&lt;/a&gt; translated by &lt;strong&gt;Falcon-H1-Arabic&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we&amp;rsquo;re excited to announce &lt;strong&gt;Falcon-H1-Arabic&lt;/strong&gt;, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in &lt;strong&gt;three&lt;/strong&gt; powerful models that set new standards for Arabic natural language processing.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Falcon-Arabic: A Breakthrough in Arabic Language Models</title>
      <link>https://falcon-lm.github.io/blog/falcon-arabic/</link>
      <pubDate>Wed, 21 May 2025 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-arabic/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Check out the &lt;a href=&#34;https://falcon-lm.github.io/ar/blog/falcon-arabic/&#34;&gt;Arabic version&lt;/a&gt; translated by &lt;strong&gt;Falcon-Arabic&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;We are excited to introduce &lt;strong&gt;Falcon-Arabic&lt;/strong&gt;, a 7B parameter Language Model that sets a new benchmark for Arabic NLP. Built on the Falcon 3 architecture, Falcon-Arabic is a multilingual model that supports Arabic, English, and several other languages. It excels in general knowledge, Arabic grammar, mathematical reasoning, complex problem solving, and understanding the rich diversity of Arabic dialects. Falcon-Arabic supports a context length of 32,000 tokens, allowing it to handle long documents and enabling advanced applications like retrieval-augmented generation (RAG), in-depth content creation, and knowledge-intensive tasks.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance</title>
      <link>https://falcon-lm.github.io/blog/falcon-h1/</link>
      <pubDate>Tue, 20 May 2025 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-h1/</guid>
      <description>&lt;style&gt;
  html, body {
    background: #f0f2f9;
  }
&lt;/style&gt;
&lt;p&gt;&lt;a href=&#34;https://chat.falconllm.tii.ae&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Falcon CHAT&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt;
&lt;a href=&#34;https://arxiv.org/abs/2507.22448&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Paper&lt;/a&gt;
&lt;a href=&#34;https://github.com/tiiuae/falcon-h1&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/spaces/tiiuae/Falcon-H1-Playground&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DEMO&lt;/a&gt;
&lt;a href=&#34;https://discord.gg/Cbek57PrZE&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DISCORD&lt;/a&gt;&lt;/p&gt;
&lt;div style=&#34;display: flex; justify-content: center;&#34;&gt;
  &lt;div style=&#34;position: relative; width: 100%; max-width: 700px; aspect-ratio: 700 /600;&#34;&gt;
    &lt;iframe
      src=&#34;https://falcon-lm.github.io/plots_h1/falcon_h1_performance_scatter_2.html&#34;
      style=&#34;border: none; position: absolute; width: 100%; height: 100%; left: 0; top: 0;&#34;
      allowfullscreen
    &gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.</title>
      <link>https://falcon-lm.github.io/blog/falcon-edge/</link>
      <pubDate>Thu, 15 May 2025 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-edge/</guid>
      <description>&lt;p&gt;In this blogpost, we present the key highlights and rationales about the &lt;em&gt;Falcon-Edge&lt;/em&gt; series - a collection of &lt;em&gt;powerful&lt;/em&gt;, &lt;em&gt;universal&lt;/em&gt;, and &lt;em&gt;fine-tunable&lt;/em&gt; language models available in ternary format, based on the BitNet architecture.&lt;/p&gt;
&lt;p&gt;Drawing from our experience with BitNet, &lt;strong&gt;Falcon-Edge&lt;/strong&gt; introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Welcome to the Falcon 3 Family of Open Models!</title>
      <link>https://falcon-lm.github.io/blog/falcon-3/</link>
      <pubDate>Tue, 17 Dec 2024 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-3/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chat.falconllm.tii.ae&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Falcon CHAT&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/spaces/tiiuae/Falcon3-demo&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DEMO&lt;/a&gt;
&lt;a href=&#34;https://discord.gg/Cbek57PrZE&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DISCORD&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;welcome-to-the-falcon-3-family-of-open-models&#34;&gt;Welcome to the Falcon 3 Family of Open Models!&lt;/h1&gt;
&lt;p&gt;We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by
&lt;a href=&#34;https://www.tii.ae/ai-and-digital-science&#34;&gt;Technology Innovation Institute (TII)&lt;/a&gt; in Abu Dhabi. By pushing the
boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open
and accessible large foundation models.&lt;/p&gt;
&lt;p&gt;Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models&amp;rsquo; science, math, and code capabilities.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Welcome Falcon Mamba: The first strong attention-free 7B model</title>
      <link>https://falcon-lm.github.io/blog/falcon-mamba/</link>
      <pubDate>Mon, 12 Aug 2024 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-mamba/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html&#34;&gt;Falcon Mamba&lt;/a&gt; is a new model by &lt;a href=&#34;https://www.tii.ae/ai-and-digital-science&#34;&gt;Technology Innovation Institute (TII)&lt;/a&gt; in Abu Dhabi released under the &lt;a href=&#34;https://falconllm.tii.ae/falcon-mamba-7b-terms-and-conditions.html&#34;&gt;TII Falcon Mamba 7B License 1.0&lt;/a&gt;. The model is open access and available within the Hugging Face ecosystem &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-mamba-7b&#34;&gt;here&lt;/a&gt; for anyone to use for their research or application purposes.&lt;/p&gt;
&lt;p&gt;In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages</title>
      <link>https://falcon-lm.github.io/blog/falcon2-11b/</link>
      <pubDate>Fri, 24 May 2024 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon2-11b/</guid>
      <description>&lt;h1 id=&#34;falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-and-11-languages&#34;&gt;Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages&lt;/h1&gt;
&lt;p&gt;&lt;a name=&#34;the-falcon-models&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-falcon-2-models&#34;&gt;The Falcon 2 Models&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;www.tii.ae&#34;&gt;TII&lt;/a&gt; is launching a new generation of models, &lt;a href=&#34;https://falconllm.tii.ae/&#34;&gt;Falcon 2&lt;/a&gt;, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.&lt;/p&gt;
&lt;p&gt;The first generation of Falcon models, featuring &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;Falcon-40B&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B&#34;&gt;Falcon-180B&lt;/a&gt;, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html&#34;&gt;RefinedWeb, Penedo et al., 2023&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2311.16867&#34;&gt;The Falcon Series of Open Language Models, Almazrouei et al., 2023&lt;/a&gt; papers, and the &lt;a href=&#34;https://huggingface.co/blog/falcon&#34;&gt;Falcon&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/blog/falcon-180b&#34;&gt;Falcon-180B&lt;/a&gt; blog posts.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Spread Your Wings: Falcon 180B is here</title>
      <link>https://falcon-lm.github.io/blog/falcon-180b/</link>
      <pubDate>Wed, 06 Sep 2023 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-180b/</guid>
      <description>&lt;h1 id=&#34;spread-your-wings-falcon-180b-is-here&#34;&gt;Spread Your Wings: Falcon 180B is here&lt;/h1&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Today, we&amp;rsquo;re excited to welcome &lt;a href=&#34;https://falconllm.tii.ae/&#34;&gt;TII&amp;rsquo;s&lt;/a&gt; Falcon 180B to HuggingFace!&lt;/strong&gt; Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII&amp;rsquo;s &lt;a href=&#34;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&#34;&gt;RefinedWeb&lt;/a&gt; dataset. This represents the longest single-epoch pretraining for an open model.&lt;/p&gt;
&lt;p&gt;You can find the model on the Hugging Face Hub (&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B&#34;&gt;base&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B-chat&#34;&gt;chat&lt;/a&gt; model) and interact with the model on the &lt;a href=&#34;https://huggingface.co/spaces/tiiuae/falcon-180b-chat&#34;&gt;Falcon Chat Demo Space&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
