<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on Falcon</title>
    <link>https://falcon-lm.github.io/blog/</link>
    <description>Recent content in Blog on Falcon</description>
    <image>
      <url>https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 21 May 2025 12:00:00 +0000</lastBuildDate><atom:link href="https://falcon-lm.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.</title>
      <link>https://falcon-lm.github.io/blog/falcon-edge/</link>
      <pubDate>Thu, 15 May 2025 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-edge/</guid>
      <description>&lt;p&gt;In this blogpost, we present the key highlights and rationales about the &lt;em&gt;Falcon-Edge&lt;/em&gt; series - a collection of &lt;em&gt;powerful&lt;/em&gt;, &lt;em&gt;universal&lt;/em&gt;, and &lt;em&gt;fine-tunable&lt;/em&gt; language models available in ternary format, based on the BitNet architecture.&lt;/p&gt;
&lt;p&gt;Drawing from our experience with BitNet, &lt;strong&gt;Falcon-Edge&lt;/strong&gt; introduces and validates an new pre-training paradigm that delivers a full-scope output from a single training process, simultaneously yielding both non-quantized and quantized model variants. This comprehensive approach produces a non-BitNet model in bfloat16 format, the native BitNet model, and a pre-quantized BitNet variant specifically engineered for effortless fine-tuning, enabling users and developers to precisely tailor these models to their specific applications and needs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Welcome to the Falcon 3 Family of Open Models!</title>
      <link>https://falcon-lm.github.io/blog/falcon-3/</link>
      <pubDate>Tue, 17 Dec 2024 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-3/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://chat.falconllm.tii.ae&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Falcon CHAT&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt;
&lt;a href=&#34;https://huggingface.co/spaces/tiiuae/Falcon3-demo&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DEMO&lt;/a&gt;
&lt;a href=&#34;https://discord.gg/vfw6k2G3&#34; class=&#34;btn external&#34; target=&#34;_blank&#34;&gt;DISCORD&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;welcome-to-the-falcon-3-family-of-open-models&#34;&gt;Welcome to the Falcon 3 Family of Open Models!&lt;/h1&gt;
&lt;p&gt;We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by
&lt;a href=&#34;https://www.tii.ae/ai-and-digital-science&#34;&gt;Technology Innovation Institute (TII)&lt;/a&gt; in Abu Dhabi. By pushing the
boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open
and accessible large foundation models.&lt;/p&gt;
&lt;p&gt;Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models&amp;rsquo; science, math, and code capabilities.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Welcome Falcon Mamba: The first strong attention-free 7B model</title>
      <link>https://falcon-lm.github.io/blog/falcon-mamba/</link>
      <pubDate>Mon, 12 Aug 2024 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-mamba/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html&#34;&gt;Falcon Mamba&lt;/a&gt; is a new model by &lt;a href=&#34;https://www.tii.ae/ai-and-digital-science&#34;&gt;Technology Innovation Institute (TII)&lt;/a&gt; in Abu Dhabi released under the &lt;a href=&#34;https://falconllm.tii.ae/falcon-mamba-7b-terms-and-conditions.html&#34;&gt;TII Falcon Mamba 7B License 1.0&lt;/a&gt;. The model is open access and available within the Hugging Face ecosystem &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-mamba-7b&#34;&gt;here&lt;/a&gt; for anyone to use for their research or application purposes.&lt;/p&gt;
&lt;p&gt;In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages</title>
      <link>https://falcon-lm.github.io/blog/falcon2-11b/</link>
      <pubDate>Fri, 24 May 2024 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon2-11b/</guid>
      <description>&lt;h1 id=&#34;falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-and-11-languages&#34;&gt;Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages&lt;/h1&gt;
&lt;p&gt;&lt;a name=&#34;the-falcon-models&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-falcon-2-models&#34;&gt;The Falcon 2 Models&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;www.tii.ae&#34;&gt;TII&lt;/a&gt; is launching a new generation of models, &lt;a href=&#34;https://falconllm.tii.ae/&#34;&gt;Falcon 2&lt;/a&gt;, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.&lt;/p&gt;
&lt;p&gt;The first generation of Falcon models, featuring &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;Falcon-40B&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B&#34;&gt;Falcon-180B&lt;/a&gt;, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html&#34;&gt;RefinedWeb, Penedo et al., 2023&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2311.16867&#34;&gt;The Falcon Series of Open Language Models, Almazrouei et al., 2023&lt;/a&gt; papers, and the &lt;a href=&#34;https://huggingface.co/blog/falcon&#34;&gt;Falcon&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/blog/falcon-180b&#34;&gt;Falcon-180B&lt;/a&gt; blog posts.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Spread Your Wings: Falcon 180B is here</title>
      <link>https://falcon-lm.github.io/blog/falcon-180b/</link>
      <pubDate>Wed, 06 Sep 2023 12:00:00 +0000</pubDate>
      
      <guid>https://falcon-lm.github.io/blog/falcon-180b/</guid>
      <description>&lt;h1 id=&#34;spread-your-wings-falcon-180b-is-here&#34;&gt;Spread Your Wings: Falcon 180B is here&lt;/h1&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Today, we&amp;rsquo;re excited to welcome &lt;a href=&#34;https://falconllm.tii.ae/&#34;&gt;TII&amp;rsquo;s&lt;/a&gt; Falcon 180B to HuggingFace!&lt;/strong&gt; Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII&amp;rsquo;s &lt;a href=&#34;https://huggingface.co/datasets/tiiuae/falcon-refinedweb&#34;&gt;RefinedWeb&lt;/a&gt; dataset. This represents the longest single-epoch pretraining for an open model.&lt;/p&gt;
&lt;p&gt;You can find the model on the Hugging Face Hub (&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B&#34;&gt;base&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/tiiuae/falcon-180B-chat&#34;&gt;chat&lt;/a&gt; model) and interact with the model on the &lt;a href=&#34;https://huggingface.co/spaces/tiiuae/falcon-180b-chat&#34;&gt;Falcon Chat Demo Space&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
