<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Spread Your Wings: Falcon 180B is here | Falcon</title>
<meta name=keywords content><meta name=description content="Spread Your Wings: Falcon 180B is here
Introduction
Today, we&rsquo;re excited to welcome TII&rsquo;s Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII&rsquo;s RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.
You can find the model on the Hugging Face Hub (base and chat model) and interact with the model on the Falcon Chat Demo Space."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/blog/falcon-180b/><link crossorigin=anonymous href=/assets/css/stylesheet.7830cbcd9b767346061a78959fa41ce89b383a4c509fbc467be75022b568582f.css integrity="sha256-eDDLzZt2c0YGGniVn6Qc6Js4OkxQn7xGe+dQIrVoWC8=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/blog/falcon-180b/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><meta property="og:title" content="Spread Your Wings: Falcon 180B is here"><meta property="og:description" content="Spread Your Wings: Falcon 180B is here
Introduction
Today, we&rsquo;re excited to welcome TII&rsquo;s Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII&rsquo;s RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.
You can find the model on the Hugging Face Hub (base and chat model) and interact with the model on the Falcon Chat Demo Space."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/blog/falcon-180b/"><meta property="og:image" content="https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-09-06T12:00:00+00:00"><meta property="article:modified_time" content="2023-09-06T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Spread Your Wings: Falcon 180B is here"><meta name=twitter:description content="Spread Your Wings: Falcon 180B is here
Introduction
Today, we&rsquo;re excited to welcome TII&rsquo;s Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII&rsquo;s RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.
You can find the model on the Hugging Face Hub (base and chat model) and interact with the model on the Falcon Chat Demo Space."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://falcon-lm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Spread Your Wings: Falcon 180B is here","item":"https://falcon-lm.github.io/blog/falcon-180b/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Spread Your Wings: Falcon 180B is here","name":"Spread Your Wings: Falcon 180B is here","description":"Spread Your Wings: Falcon 180B is here Introduction Today, we\u0026rsquo;re excited to welcome TII\u0026rsquo;s Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII\u0026rsquo;s RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.\nYou can find the model on the Hugging Face Hub (base and chat model) and interact with the model on the Falcon Chat Demo Space.\n","keywords":[],"articleBody":"Spread Your Wings: Falcon 180B is here Introduction Today, we‚Äôre excited to welcome TII‚Äôs Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII‚Äôs RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.\nYou can find the model on the Hugging Face Hub (base and chat model) and interact with the model on the Falcon Chat Demo Space.\nIn terms of capabilities, Falcon 180B achieves state-of-the-art results across natural language tasks. It topped the leaderboard for (pre-trained) open-access models (at the time of its release) and rivals proprietary models like PaLM-2. While difficult to rank definitively yet, it is considered on par with PaLM-2 Large, making Falcon 180B one of the most capable LLMs publicly known.\nIn this blog post, we explore what makes Falcon 180B so good by looking at some evaluation results and show how you can use the model.\nWhat is Falcon-180B? How good is Falcon 180B? How to use Falcon 180B? Demo Hardware requirements Prompt format Transformers Additional Resources What is Falcon-180B? Falcon 180B is a model released by TII that follows previous releases in the Falcon family.\nArchitecture-wise, Falcon 180B is a scaled-up version of Falcon 40B and builds on its innovations such as multiquery attention for improved scalability. We recommend reviewing the initial blog post introducing Falcon to dive into the architecture. Falcon 180B was trained on 3.5 trillion tokens on up to 4096 GPUs simultaneously, using Amazon SageMaker for a total of ~7,000,000 GPU hours. This means Falcon 180B is 2.5 times larger than Llama 2 and was trained with 4x more compute.\nThe dataset for Falcon 180B consists predominantly of web data from RefinedWeb (~85%). In addition, it has been trained on a mix of curated data such as conversations, technical papers, and a small fraction of code (~3%). This pretraining dataset is big enough that even 3.5 trillion tokens constitute less than an epoch.\nThe released chat model is fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.\n‚ÄºÔ∏è Commercial use: Falcon 180b can be commercially used but under very restrictive conditions, excluding any ‚Äúhosting use‚Äù. We recommend to check the license and consult your legal team if you are interested in using it for commercial purposes.\nHow good is Falcon 180B? Falcon 180B was the best openly released LLM at its release, outperforming Llama 2 70B and OpenAI‚Äôs GPT-3.5 on MMLU, and is on par with Google‚Äôs PaLM 2-Large on HellaSwag, LAMBADA, WebQuestions, Winogrande, PIQA, ARC, BoolQ, CB, COPA, RTE, WiC, WSC, ReCoRD. Falcon 180B typically sits somewhere between GPT 3.5 and GPT4 depending on the evaluation benchmark and further finetuning from the community will be very interesting to follow now that it‚Äôs openly released.\nWith 68.74 on the Hugging Face Leaderboard at the time of release, Falcon 180B was the highest-scoring openly released pre-trained LLM, surpassing Meta‚Äôs Llama 2.*\nModel Size Leaderboard score Commercial use or license Pretraining length Falcon 180B 67.85 üü† 3,500B Llama 2 70B 67.87 üü† 2,000B LLaMA 65B 61.19 üî¥ 1,400B Falcon 40B 58.07 üü¢ 1,000B MPT 30B 52.77 üü¢ 1,000B The Open LLM Leaderboard added two new benchmarks in November 2023, and we updated the table above to reflect the latest score (67.85). Falcon is on par with Llama 2 70B according to the new methodology. The quantized Falcon models preserve similar metrics across benchmarks. The results were similar when evaluating torch.float16, 8bit, and 4bit. See results in the Open LLM Leaderboard.\nHow to use Falcon 180B? Falcon 180B is available in the Hugging Face ecosystem, starting with Transformers version 4.33.\nDemo You can easily try the Big Falcon Model (180 billion parameters!) in this Space or in the playground embedded below:\nHardware requirements We ran several tests on the hardware needed to run the model for different use cases. Those are not the minimum numbers, but the minimum numbers for the configurations we had access to.\nType Kind Memory Example Falcon 180B Training Full fine-tuning 5120GB 8x 8x A100 80GB Falcon 180B Training LoRA with ZeRO-3 1280GB 2x 8x A100 80GB Falcon 180B Training QLoRA 160GB 2x A100 80GB Falcon 180B Inference BF16/FP16 640GB 8x A100 80GB Falcon 180B Inference GPTQ/int4 320GB 8x A100 40GB Prompt format The base model has no prompt format. Remember that it‚Äôs not a conversational model or trained with instructions, so don‚Äôt expect it to generate conversational responses‚Äîthe pretrained model is a great platform for further finetuning, but you probably shouldn‚Äôt driectly use it out of the box. The Chat model has a very simple conversation structure.\nSystem: Add an optional system prompt here User: This is the user input Falcon: This is what the model generates User: This might be a second turn input Falcon: and so on Transformers With the release of Transformers 4.33, you can use Falcon 180B and leverage all the tools in the HF ecosystem, such as:\ntraining and inference scripts and examples safe file format (safetensors) integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning) and GPTQ assisted generation (also known as ‚Äúspeculative decoding‚Äù) RoPE scaling support for larger context lengths rich and powerful generation parameters Use of the model requires you to accept its license and terms of use. Please, make sure you are logged into your Hugging Face account and ensure you have the latest version of transformers:\npip install --upgrade transformers huggingface-cli login bfloat16 This is how you‚Äôd use the base model in bfloat16. Falcon 180B is a big model, so please take into account the hardware requirements summarized in the table above.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM import transformers import torch model_id = \"tiiuae/falcon-180B\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", ) prompt = \"My name is Pedro, I live in\" inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") output = model.generate( input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], do_sample=True, temperature=0.6, top_p=0.9, max_new_tokens=50, ) output = output[0].to(\"cpu\") print(tokenizer.decode(output)) This could produce an output such as:\nMy name is Pedro, I live in Portugal and I am 25 years old. I am a graphic designer, but I am also passionate about photography and video. I love to travel and I am always looking for new adventures. I love to meet new people and explore new places. 8-bit and 4-bit with bitsandbytes The 8-bit and 4-bit quantized versions of Falcon 180B show almost no difference in evaluation with respect to the bfloat16 reference! This is very good news for inference, as you can confidently use a quantized version to reduce hardware requirements. Keep in mind, though, that 8-bit inference is much faster than running the model in 4-bit.\nTo use quantization, you need to install the bitsandbytes library and simply enable the corresponding flag when loading the model:\nmodel = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=torch.bfloat16, load_in_8bit=True, device_map=\"auto\", ) Chat Model As mentioned above, the version of the model fine-tuned to follow conversations used a very straightforward training template. We have to follow the same pattern in order to run chat-style inference. For reference, you can take a look at the format_prompt function in the Chat demo, which looks like this:\ndef format_prompt(message, history, system_prompt): prompt = \"\" if system_prompt: prompt += f\"System: {system_prompt}\\n\" for user_prompt, bot_response in history: prompt += f\"User: {user_prompt}\\n\" prompt += f\"Falcon: {bot_response}\\n\" prompt += f\"User: {message}\\nFalcon:\" return prompt As you can see, interactions from the user and responses by the model are preceded by User: and Falcon: separators. We concatenate them together to form a prompt containing the conversation‚Äôs whole history. We can provide a system prompt to tweak the generation style.\nAdditional Resources Models Demo The Falcon has landed in the Hugging Face ecosystem Official Announcement Acknowledgments Releasing such a model with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including Cl√©mentine and Eleuther Evaluation Harness for LLM evaluations; Loubna and BigCode for code evaluations; Nicolas for Inference support; Lysandre, Matt, Daniel, Amy, Joao, and Arthur for integrating Falcon into transformers. Thanks to Baptiste and Patrick for the open-source demo. Thanks to Thom, Lewis, TheBloke, Nouamane, Tim Dettmers for multiple contributions enabling this to get out. Finally, thanks to the HF Cluster for enabling running LLM evaluations as well as providing inference for a free, open-source demo of the model.\n","wordCount":"1392","inLanguage":"en","datePublished":"2023-09-06T12:00:00Z","dateModified":"2023-09-06T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/blog/falcon-180b/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Spread Your Wings: Falcon 180B is here</h1><div class=post-meta><span title='2023-09-06 12:00:00 +0000 UTC'>September 6, 2023</span>&nbsp;‚Ä¢&nbsp;7 min&nbsp;‚Ä¢&nbsp;1392 words&nbsp;‚Ä¢&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><div class=post-content><h1 id=spread-your-wings-falcon-180b-is-here>Spread Your Wings: Falcon 180B is here<a hidden class=anchor aria-hidden=true href=#spread-your-wings-falcon-180b-is-here>#</a></h1><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p><strong>Today, we&rsquo;re excited to welcome <a href=https://falconllm.tii.ae/>TII&rsquo;s</a> Falcon 180B to HuggingFace!</strong> Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII&rsquo;s <a href=https://huggingface.co/datasets/tiiuae/falcon-refinedweb>RefinedWeb</a> dataset. This represents the longest single-epoch pretraining for an open model.</p><p>You can find the model on the Hugging Face Hub (<a href=https://huggingface.co/tiiuae/falcon-180B>base</a> and <a href=https://huggingface.co/tiiuae/falcon-180B-chat>chat</a> model) and interact with the model on the <a href=https://huggingface.co/spaces/tiiuae/falcon-180b-chat>Falcon Chat Demo Space</a>.</p><p>In terms of capabilities, Falcon 180B achieves state-of-the-art results across natural language tasks. It topped the leaderboard for (pre-trained) open-access models (at the time of its release) and rivals proprietary models like PaLM-2. While difficult to rank definitively yet, it is considered on par with PaLM-2 Large, making Falcon 180B one of the most capable LLMs publicly known.</p><p>In this blog post, we explore what makes Falcon 180B so good by looking at some evaluation results and show how you can use the model.</p><ul><li><a href=/blog/falcon-180b/#what-is-falcon-180b>What is Falcon-180B?</a></li><li><a href=/blog/falcon-180b/#how-good-is-falcon-180b>How good is Falcon 180B?</a></li><li><a href=/blog/falcon-180b/#how-to-use-falcon-180b>How to use Falcon 180B?</a><ul><li><a href=/blog/falcon-180b/#demo>Demo</a></li><li><a href=/blog/falcon-180b/#hardware-requirements>Hardware requirements</a></li><li><a href=/blog/falcon-180b/#prompt-format>Prompt format</a></li><li><a href=/blog/falcon-180b/#transformers>Transformers</a></li></ul></li><li><a href=/blog/falcon-180b/#additional-resources>Additional Resources</a></li></ul><h2 id=what-is-falcon-180b>What is Falcon-180B?<a hidden class=anchor aria-hidden=true href=#what-is-falcon-180b>#</a></h2><p>Falcon 180B is a model released by <a href=https://falconllm.tii.ae/>TII</a> that follows previous releases in the Falcon family.</p><p>Architecture-wise, Falcon 180B is a scaled-up version of <a href=https://huggingface.co/tiiuae/falcon-40b>Falcon 40B</a> and builds on its innovations such as multiquery attention for improved scalability. We recommend reviewing the <a href=https://huggingface.co/blog/falcon>initial blog post</a> introducing Falcon to dive into the architecture. Falcon 180B was trained on 3.5 trillion tokens on up to 4096 GPUs simultaneously, using Amazon SageMaker for a total of ~7,000,000 GPU hours. This means Falcon 180B is 2.5 times larger than Llama 2 and was trained with 4x more compute.</p><p>The dataset for Falcon 180B consists predominantly of web data from <a href=https://arxiv.org/abs/2306.01116>RefinedWeb</a> (~85%). In addition, it has been trained on a mix of curated data such as conversations, technical papers, and a small fraction of code (~3%). This pretraining dataset is big enough that even 3.5 trillion tokens constitute less than an epoch.</p><p>The released <a href=https://huggingface.co/tiiuae/falcon-180B-chat>chat model</a> is fine-tuned on chat and instruction datasets with a mix of several large-scale conversational datasets.</p><p>‚ÄºÔ∏è Commercial use:
Falcon 180b can be commercially used but under very restrictive conditions, excluding any &ldquo;hosting use&rdquo;. We recommend to check the <a href=https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt>license</a> and consult your legal team if you are interested in using it for commercial purposes.</p><h2 id=how-good-is-falcon-180b>How good is Falcon 180B?<a hidden class=anchor aria-hidden=true href=#how-good-is-falcon-180b>#</a></h2><p>Falcon 180B was the best openly released LLM at its release, outperforming Llama 2 70B and OpenAI‚Äôs GPT-3.5 on MMLU, and is on par with Google&rsquo;s PaLM 2-Large on HellaSwag, LAMBADA, WebQuestions, Winogrande, PIQA, ARC, BoolQ, CB, COPA, RTE, WiC, WSC, ReCoRD. Falcon 180B typically sits somewhere between GPT 3.5 and GPT4 depending on the evaluation benchmark and further finetuning from the community will be very interesting to follow now that it&rsquo;s openly released.</p><p><img loading=lazy src=https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/palm2_480.jpg alt="Palm 2 comparison"></p><p>With 68.74 on the Hugging Face Leaderboard at the time of release, Falcon 180B was the highest-scoring openly released pre-trained LLM, surpassing Meta‚Äôs Llama 2.*</p><table><thead><tr><th>Model</th><th>Size</th><th>Leaderboard score</th><th>Commercial use or license</th><th>Pretraining length</th></tr></thead><tbody><tr><td>Falcon</td><td>180B</td><td>67.85</td><td>üü†</td><td>3,500B</td></tr><tr><td>Llama 2</td><td>70B</td><td>67.87</td><td>üü†</td><td>2,000B</td></tr><tr><td>LLaMA</td><td>65B</td><td>61.19</td><td>üî¥</td><td>1,400B</td></tr><tr><td>Falcon</td><td>40B</td><td>58.07</td><td>üü¢</td><td>1,000B</td></tr><tr><td>MPT</td><td>30B</td><td>52.77</td><td>üü¢</td><td>1,000B</td></tr></tbody></table><ul><li>The Open LLM Leaderboard added two new benchmarks in November 2023, and we updated the table above to reflect the latest score (67.85). Falcon is on par with Llama 2 70B according to the new methodology.</li></ul><p><img loading=lazy src=https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/162_falcon_180b/open_llm_leaderboard.jpg alt=open_llm_leaderboard.png></p><p>The quantized Falcon models preserve similar metrics across benchmarks. The results were similar when evaluating <code>torch.float16</code>, <code>8bit</code>, and <code>4bit</code>. See results in the <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a>.</p><h2 id=how-to-use-falcon-180b>How to use Falcon 180B?<a hidden class=anchor aria-hidden=true href=#how-to-use-falcon-180b>#</a></h2><p>Falcon 180B is available in the Hugging Face ecosystem, starting with Transformers version 4.33.</p><h3 id=demo>Demo<a hidden class=anchor aria-hidden=true href=#demo>#</a></h3><p>You can easily try the Big Falcon Model (180 billion parameters!) in <a href=https://huggingface.co/spaces/tiiuae/falcon-180b-demo>this Space</a> or in the playground embedded below:</p><script type=module src=https://gradio.s3-us-west-2.amazonaws.com/3.42.0/gradio.js> </script><p><gradio-app theme_mode=light space=tiiuae/falcon-180b-chat></gradio-app></p><h3 id=hardware-requirements>Hardware requirements<a hidden class=anchor aria-hidden=true href=#hardware-requirements>#</a></h3><p>We ran several tests on the hardware needed to run the model for different use cases. Those are not the minimum numbers, but the minimum numbers for the configurations we had access to.</p><table><thead><tr><th></th><th>Type</th><th>Kind</th><th>Memory</th><th>Example</th></tr></thead><tbody><tr><td>Falcon 180B</td><td>Training</td><td>Full fine-tuning</td><td>5120GB</td><td>8x 8x A100 80GB</td></tr><tr><td>Falcon 180B</td><td>Training</td><td>LoRA with ZeRO-3</td><td>1280GB</td><td>2x 8x A100 80GB</td></tr><tr><td>Falcon 180B</td><td>Training</td><td>QLoRA</td><td>160GB</td><td>2x A100 80GB</td></tr><tr><td>Falcon 180B</td><td>Inference</td><td>BF16/FP16</td><td>640GB</td><td>8x A100 80GB</td></tr><tr><td>Falcon 180B</td><td>Inference</td><td>GPTQ/int4</td><td>320GB</td><td>8x A100 40GB</td></tr></tbody></table><h3 id=prompt-format>Prompt format<a hidden class=anchor aria-hidden=true href=#prompt-format>#</a></h3><p>The base model has no prompt format. Remember that it‚Äôs not a conversational model or trained with instructions, so don‚Äôt expect it to generate conversational responses‚Äîthe pretrained model is a great platform for further finetuning, but you probably shouldn‚Äôt driectly use it out of the box. The Chat model has a very simple conversation structure.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>System: Add an optional system prompt here
</span></span><span class=line><span class=cl>User: This is the user input
</span></span><span class=line><span class=cl>Falcon: This is what the model generates
</span></span><span class=line><span class=cl>User: This might be a second turn input
</span></span><span class=line><span class=cl>Falcon: and so on
</span></span></code></pre></div><h3 id=transformers>Transformers<a hidden class=anchor aria-hidden=true href=#transformers>#</a></h3><p>With the release of Transformers 4.33, you can use Falcon 180B and leverage all the tools in the HF ecosystem, such as:</p><ul><li>training and inference scripts and examples</li><li>safe file format (safetensors)</li><li>integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning) and GPTQ</li><li>assisted generation (also known as ‚Äúspeculative decoding‚Äù)</li><li>RoPE scaling support for larger context lengths</li><li>rich and powerful generation parameters</li></ul><p>Use of the model requires you to accept its license and terms of use. Please, make sure you are logged into your Hugging Face account and ensure you have the latest version of <code>transformers</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install --upgrade transformers
</span></span><span class=line><span class=cl>huggingface-cli login
</span></span></code></pre></div><h4 id=bfloat16>bfloat16<a hidden class=anchor aria-hidden=true href=#bfloat16>#</a></h4><p>This is how you‚Äôd use the base model in <code>bfloat16</code>. Falcon 180B is a big model, so please take into account the hardware requirements summarized in the table above.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>transformers</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;tiiuae/falcon-180B&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;My name is Pedro, I live in&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=o>=</span><span class=n>inputs</span><span class=p>[</span><span class=s2>&#34;input_ids&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>attention_mask</span><span class=o>=</span><span class=n>inputs</span><span class=p>[</span><span class=s2>&#34;attention_mask&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>temperature</span><span class=o>=</span><span class=mf>0.6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>output</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output</span><span class=p>))</span>
</span></span></code></pre></div><p>This could produce an output such as:</p><pre tabindex=0><code>My name is Pedro, I live in Portugal and I am 25 years old. I am a graphic designer, but I am also passionate about photography and video.
I love to travel and I am always looking for new adventures. I love to meet new people and explore new places.
</code></pre><h4 id=8-bit-and-4-bit-with-bitsandbytes>8-bit and 4-bit with <code>bitsandbytes</code><a hidden class=anchor aria-hidden=true href=#8-bit-and-4-bit-with-bitsandbytes>#</a></h4><p>The 8-bit and 4-bit quantized versions of Falcon 180B show almost no difference in evaluation with respect to the <code>bfloat16</code> reference! This is very good news for inference, as you can confidently use a quantized version to reduce hardware requirements. Keep in mind, though, that 8-bit inference is <em>much faster</em> than running the model in <code>4-bit</code>.</p><p>To use quantization, you need to install the <code>bitsandbytes</code> library and simply enable the corresponding flag when loading the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>load_in_8bit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h4 id=chat-model>Chat Model<a hidden class=anchor aria-hidden=true href=#chat-model>#</a></h4><p>As mentioned above, the version of the model fine-tuned to follow conversations used a very straightforward training template. We have to follow the same pattern in order to run chat-style inference. For reference, you can take a look at the <a href=https://huggingface.co/spaces/tiiuae/falcon-180b-demo/blob/main/app.py#L28>format_prompt</a> function in the Chat demo, which looks like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>format_prompt</span><span class=p>(</span><span class=n>message</span><span class=p>,</span> <span class=n>history</span><span class=p>,</span> <span class=n>system_prompt</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>system_prompt</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt</span> <span class=o>+=</span> <span class=sa>f</span><span class=s2>&#34;System: </span><span class=si>{</span><span class=n>system_prompt</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>user_prompt</span><span class=p>,</span> <span class=n>bot_response</span> <span class=ow>in</span> <span class=n>history</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt</span> <span class=o>+=</span> <span class=sa>f</span><span class=s2>&#34;User: </span><span class=si>{</span><span class=n>user_prompt</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt</span> <span class=o>+=</span> <span class=sa>f</span><span class=s2>&#34;Falcon: </span><span class=si>{</span><span class=n>bot_response</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>prompt</span> <span class=o>+=</span> <span class=sa>f</span><span class=s2>&#34;User: </span><span class=si>{</span><span class=n>message</span><span class=si>}</span><span class=se>\n</span><span class=s2>Falcon:&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>prompt</span>
</span></span></code></pre></div><p>As you can see, interactions from the user and responses by the model are preceded by <code>User: </code>and <code>Falcon: </code>separators. We concatenate them together to form a prompt containing the conversation&rsquo;s whole history. We can provide a system prompt to tweak the generation style.</p><h2 id=additional-resources>Additional Resources<a hidden class=anchor aria-hidden=true href=#additional-resources>#</a></h2><ul><li><a href="https://huggingface.co/models?other=falcon&amp;sort=trending&amp;search=180">Models</a></li><li><a href=https://huggingface.co/spaces/tiiuae/falcon-180b-chat>Demo</a></li><li><a href=https://huggingface.co/blog/falcon>The Falcon has landed in the Hugging Face ecosystem</a></li><li><a href=https://falconllm.tii.ae/>Official Announcement</a></li></ul><h2 id=acknowledgments>Acknowledgments<a hidden class=anchor aria-hidden=true href=#acknowledgments>#</a></h2><p>Releasing such a model with support and evaluations in the ecosystem would not be possible without the contributions of many community members, including <a href=https://huggingface.co/clefourrier>Cl√©mentine</a> and <a href=https://github.com/EleutherAI/lm-evaluation-harness>Eleuther Evaluation Harness</a> for LLM evaluations; <a href=https://huggingface.co/loubnabnl>Loubna</a> and <a href=https://huggingface.co/bigcode>BigCode</a> for code evaluations; <a href=https://hf.co/narsil>Nicolas</a> for Inference support; <a href=https://huggingface.co/lysandre>Lysandre</a>, <a href=https://huggingface.co/Rocketknight1>Matt</a>, <a href=https://huggingface.co/DanielHesslow>Daniel</a>, <a href=https://huggingface.co/amyeroberts>Amy</a>, <a href=https://huggingface.co/joaogante>Joao</a>, and <a href=https://huggingface.co/ArthurZ>Arthur</a> for integrating Falcon into transformers. Thanks to <a href=https://huggingface.co/BapBap>Baptiste</a> and <a href=https://huggingface.co/patrickvonplaten>Patrick</a> for the open-source demo. Thanks to <a href=https://huggingface.co/thomwolf>Thom</a>, <a href=https://huggingface.co/lewtun>Lewis</a>, <a href=https://huggingface.co/thebloke>TheBloke</a>, <a href=https://huggingface.co/nouamanetazi>Nouamane</a>, <a href=https://huggingface.co/timdettmers>Tim Dettmers</a> for multiple contributions enabling this to get out. Finally, thanks to the HF Cluster for enabling running LLM evaluations as well as providing inference for a free, open-source demo of the model.</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>