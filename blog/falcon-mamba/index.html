<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Welcome Falcon Mamba: The first strong attention-free 7B model | Falcon</title>
<meta name=keywords content><meta name=description content="Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.
In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/blog/falcon-mamba/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/blog/falcon-mamba/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Welcome Falcon Mamba: The first strong attention-free 7B model"><meta property="og:description" content="Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.
In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/blog/falcon-mamba/"><meta property="og:image" content="https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-12T12:00:00+00:00"><meta property="article:modified_time" content="2024-08-12T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba.png"><meta name=twitter:title content="Welcome Falcon Mamba: The first strong attention-free 7B model"><meta name=twitter:description content="Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.
In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://falcon-lm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Welcome Falcon Mamba: The first strong attention-free 7B model","item":"https://falcon-lm.github.io/blog/falcon-mamba/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Welcome Falcon Mamba: The first strong attention-free 7B model","name":"Welcome Falcon Mamba: The first strong attention-free 7B model","description":"Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.\nIn this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.\n","keywords":[],"articleBody":"Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.\nIn this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.\nFirst general purpose large-scale pure Mamba model Transformers, based on the attention mechanism, are the dominant architecture used in all the strongest large language models today. Yet, the attention mechanism is fundamentally limited in processing large sequences due to the increase in compute and memory costs with sequence length. Various alternative architectures, in particular State Space Language Models (SSLMs), tried to address the sequence scaling limitation but fell back in performance compared to SoTA transformers.\nWith Falcon Mamba, we demonstrate that sequence scaling limitation can indeed be overcome without loss in performance. Falcon Mamba is based on the original Mamba architecture, proposed in Mamba: Linear-Time Sequence Modeling with Selective State Spaces, with the addition of extra RMS normalization layers to ensure stable training at scale. This choice of architecture ensures that Falcon Mamba:\ncan process sequences of arbitrary length without any increase in memory storage, in particular, fitting on a single A10 24GB GPU. takes a constant amount of time to generate a new token, regardless of the size of the context (see this section) Model training Falcon Mamba was trained with ~ 5500GT of data, mainly composed of RefinedWeb data with addition of high-quality technical data and code data from public sources. We used constant learning rate for the most of the training, followed by a relatively short learning rate decay stage. In this last stage, we also added a small portion of high-quality curated data to further enhance model performance.\nEvaluations We evaluate our model on all benchmarks of the new leaderboardâ€™s version using the lm-evaluation-harness package and then normalize the evaluation results with Hugging Face score normalization.\nmodel name IFEval BBH MATH LvL5 GPQA MUSR MMLU-PRO Average Pure SSM models Falcon Mamba-7B 33.36 19.88 3.63 8.05 10.86 14.47 15.04 TRI-ML/mamba-7b-rw* 22.46 6.71 0.45 1.12 5.51 1.69 6.25 Hybrid SSM-attention models recurrentgemma-9b 30.76 14.80 4.83 4.70 6.60 17.88 13.20 Zyphra/Zamba-7B-v1* 24.06 21.12 3.32 3.03 7.74 16.02 12.55 Transformer models Falcon2-11B 32.61 21.94 2.34 2.80 7.53 15.44 13.78 Meta-Llama-3-8B 14.55 24.50 3.25 7.38 6.24 24.55 13.41 Meta-Llama-3.1-8B 12.70 25.29 4.61 6.15 8.98 24.95 13.78 Mistral-7B-v0.1 23.86 22.02 2.49 5.59 10.68 22.36 14.50 Mistral-Nemo-Base-2407 (12B) 16.83 29.37 4.98 5.82 6.52 27.46 15.08 gemma-7B 26.59 21.12 6.42 4.92 10.98 21.64 15.28 Also, we evaluate our model on the benchmarks of the first version of the LLM Leaderboard using lighteval.\nmodel name ARC HellaSwag MMLU Winogrande TruthfulQA GSM8K Average Pure SSM models Falcon Mamba-7B* 62.03 80.82 62.11 73.64 53.42 52.54 64.09 TRI-ML/mamba-7b-rw* 51.25 80.85 33.41 71.11 32.08 4.70 45.52 Hybrid SSM-attention models recurrentgemma-9b** 52.00 80.40 60.50 73.60 38.60 42.60 57.95 Zyphra/Zamba-7B-v1* 56.14 82.23 58.11 79.87 52.88 30.78 60.00 Transformer models Falcon2-11B 59.73 82.91 58.37 78.30 52.56 53.83 64.28 Meta-Llama-3-8B 60.24 82.23 66.70 78.45 42.93 45.19 62.62 Meta-Llama-3.1-8B 58.53 82.13 66.43 74.35 44.29 47.92 62.28 Mistral-7B-v0.1 59.98 83.31 64.16 78.37 42.15 37.83 60.97 gemma-7B 61.09 82.20 64.56 79.01 44.79 50.87 63.75 For the models marked by star, we evaluated the tasks internally, while for the models marked by two stars, the results were taken from paper or model card.\nProcessing large sequences Following theoretical efficiency SSM models in processing large sequences, we perform a comparison of memory usage and generation throughput between Falcon Mamba and popular transfomer models using the optimum-benchmark library. For a fair comparison, we rescaled the vocabulary size of all transformer models to match Falcon Mamba since it has a big impact on the memory requirements of the model.\nBefore going to the results, letâ€™s first discuss the difference between the prompt (prefill) and generated (decode) parts of the sequence. As we will see, the details of prefill are more important for state space models than for transformer models. When a transformer generates the next token, it needs to attend to the keys and values of all previous tokens in the context. This implies linear scaling of both memory requirements and generation time with context length. A state space model attends to and stores only its recurrent state and, therefore, doesnâ€™t need additional memory or time to generate large sequences. While this explains the claimed advantage of SSMs over transformers in the decode stage, the prefill stage requires additional effort to fully utilize SSM architecture.\nA standard approach for prefill is to process the whole prompt in parallel to fully utilize GPU. This approach is used in optimum-benchmark library and we will refer to it as parallel prefill. Parallel prefill needs to store in memory the hidden states of each token in the prompt. For transformers, this additional memory is dominated by the memory of stored KV caches. For SSM models, no caching is required, and the memory for storing hidden states becomes the only component proportional to the prompt length. As a result, the memory requirement will scale with prompt length, and SSM models will lose the ability to process arbitrary long sequences, similar to transformers.\nAn alternative to parallel prefill is to process the prompt token by token, which we will refer to as sequential prefill. Akin to sequence parallelism, it can also be done on larger chunks of the prompt instead of individual tokens for better GPU usage. While sequential prefill makes little sense for transformers, it brings back the possibility of processing arbitrary long prompts by SSM models.\nWith these remarks in mind, we first test the largest sequence length that can fit on a single 24 GB A10 GPU, putting the results on the figure below. The batch size is fixed at 1, and we are using float32 precision. Even for parallel prefill, Falcon Mamba can fit larger sequences than a transformer, while in sequential prefill, it unlocks its full potential and can process arbitrary long prompt\nNext, we measure the generation throughput in a setting with a prompt of length 1 and up to 130k generated tokens, using batch size 1 and H100 GPU. The results are reported in the figure below. We observe that our Falcon Mamba is generating all the tokens at constant throughput and without any increase in CUDA peak memory. For the transformer model, the peak memory grows, and generation speed slows down as the number of generated tokens grows.\nHow to use it within Hugging Face transformers? The Falcon Mamba architecture will be available in the next release of the Hugging Face transformers library (\u003e4.45.0). To use the model, make sure to install the latest version of Hugging Face transformers or install the library from the source.\nFalcon Mamba is compatible with most of the APIs Hugging Face offers, which you are familiar with, such as AutoModelForCausalLM or pipeline :\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"tiiuae/falcon-mamba-7b\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\") inputs = tokenizer(\"Hello world, today\", return_tensors=\"pt\").to(0) output = model.generate(**inputs, max_new_tokens=100, do_sample=True) print(tokenizer.decode(Output[0], skip_special_tokens=True)) As the model is large, it also supports features such as bitsandbytes quantization to run the model on smaller GPU memory constraints, e.g.:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig model_id = \"tiiuae/falcon-mamba-7b\" tokenizer = AutoTokenizer.from_pretrained(model_id) quantization_config = BitsAndBytesConfig(load_in_4bit=True) model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config) inputs = tokenizer(\"Hello world, today\", return_tensors=\"pt\").to(0) output = model.generate(**inputs, max_new_tokens=100, do_sample=True) print(tokenizer.decode(output[0], skip_special_tokens=True)) We are also pleased to introduce the instruction-tuned version of Falcon Mamba, which has been fine-tuned with an additional 5 billion tokens of supervised fine-tuning (SFT) data. This extended training enhances the modelâ€™s ability to perform instructional tasks with better precision and effectiveness. You can experience the capabilities of the instruct model through our demo, available here. For the chat template we use the following format:\n\u003c|im_start|\u003euser prompt\u003c|im_end|\u003e \u003c|im_start|\u003eassistant You can also directly use the 4-bit converted version of both the base model and the instruct model. Make sure to have access to a GPU that is compatible with bitsandbytes library to run the quantized model.\nYou can also benefit from faster inference using torch.compile; simply call model = torch.compile(model) once you have loaded the model.\nAcknowledgments The authors of this blog post would like to thank the Hugging Face team for their smooth support and integration within their ecosystem, in particular\nAlina Lozovskaya and Clementine Fourrier for helping us evaluating the model on the leaderboard Arthur Zucker for the transformers integration Vaibhav Srivastav, hysts and Omar Sanseviero for their support with questions related to Hub The authors would also like to thank Tri Dao and Albert Gu for implementing and open-sourcing Mamba architecture to the community.\n","wordCount":"1463","inLanguage":"en","image":"https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba.png","datePublished":"2024-08-12T12:00:00Z","dateModified":"2024-08-12T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/blog/falcon-mamba/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:url(falcon-mamba.png)50%/cover no-repeat fixed"></div><div class=hero-gradient></div><div class=hero-blur></div><div class="hero text-light"><h1 class=post-title>Welcome Falcon Mamba: The first strong attention-free 7B model</h1><div class=post-meta><span title='2024-08-12 12:00:00 +0000 UTC'>August 12, 2024</span>&nbsp;â€¢&nbsp;7 min&nbsp;â€¢&nbsp;1463 words&nbsp;â€¢&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><figure class=entry-cover><a href=/blog/falcon-mamba/falcon-mamba.png target=_blank rel="noopener noreferrer"><img loading=lazy srcset="https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba_hu_10371b09a164012e.png 360w ,https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba_hu_7a88f8bb93f1597.png 480w ,https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba_hu_da30499463f73e41.png 720w ,https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba_hu_1dd0c259728d5ac2.png 1080w ,https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba_hu_cbcae55cb97aab5b.png 1500w ,https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba.png 2406w" sizes="(min-width: 768px) 720px, 100vw" src=https://falcon-lm.github.io/blog/falcon-mamba/falcon-mamba.png alt width=2406 height=1236></a></figure><div class=post-content><p><a href=https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html>Falcon Mamba</a> is a new model by <a href=https://www.tii.ae/ai-and-digital-science>Technology Innovation Institute (TII)</a> in Abu Dhabi released under the <a href=https://falconllm.tii.ae/falcon-mamba-7b-terms-and-conditions.html>TII Falcon Mamba 7B License 1.0</a>. The model is open access and available within the Hugging Face ecosystem <a href=https://huggingface.co/tiiuae/falcon-mamba-7b>here</a> for anyone to use for their research or application purposes.</p><p>In this blog, we will go through the design decisions behind the model, how the model is competitive with respect to other existing SoTA models, and how to use it within the Hugging Face ecosystem.</p><h2 id=first-general-purpose-large-scale-pure-mamba-model>First general purpose large-scale pure Mamba model<a hidden class=anchor aria-hidden=true href=#first-general-purpose-large-scale-pure-mamba-model>#</a></h2><p>Transformers, based on the attention mechanism, are the dominant architecture used in all the strongest large language models today. Yet, the attention mechanism is fundamentally limited in processing large sequences due to the increase in compute and memory costs with sequence length. Various alternative architectures, in particular State Space Language Models (SSLMs), tried to address the sequence scaling limitation but fell back in performance compared to SoTA transformers.</p><p>With Falcon Mamba, we demonstrate that sequence scaling limitation can indeed be overcome without loss in performance. Falcon Mamba is based on the original Mamba architecture, proposed in <a href=https://arxiv.org/abs/2312.00752><em>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</em></a>, with the addition of extra RMS normalization layers to ensure stable training at scale. This choice of architecture ensures that Falcon Mamba:</p><ul><li>can process sequences of arbitrary length without any increase in memory storage, in particular, fitting on a single A10 24GB GPU.</li><li>takes a constant amount of time to generate a new token, regardless of the size of the context (see this <a href=/blog/falcon-mamba/#hardware-performance>section</a>)</li></ul><h2 id=model-training>Model training<a hidden class=anchor aria-hidden=true href=#model-training>#</a></h2><p>Falcon Mamba was trained with ~ 5500GT of data, mainly composed of RefinedWeb data with addition of high-quality technical data and code data from public sources. We used constant learning rate for the most of the training, followed by a relatively short learning rate decay stage. In this last stage, we also added a small portion of high-quality curated data to further enhance model performance.</p><h2 id=evaluations>Evaluations<a hidden class=anchor aria-hidden=true href=#evaluations>#</a></h2><p>We evaluate our model on all benchmarks of the new leaderboard&rsquo;s version using the <code>lm-evaluation-harness</code> package and then normalize the evaluation results with Hugging Face score normalization.</p><table><thead><tr><th style=text-align:left><code>model name</code></th><th style=text-align:center><code>IFEval</code></th><th style=text-align:center><code>BBH</code></th><th style=text-align:center><code>MATH LvL5</code></th><th style=text-align:center><code>GPQA</code></th><th style=text-align:center><code>MUSR</code></th><th style=text-align:center><code>MMLU-PRO</code></th><th style=text-align:center><code>Average</code></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>Pure SSM models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left><code>Falcon Mamba-7B</code></td><td style=text-align:center>33.36</td><td style=text-align:center>19.88</td><td style=text-align:center>3.63</td><td style=text-align:center>8.05</td><td style=text-align:center>10.86</td><td style=text-align:center>14.47</td><td style=text-align:center><strong>15.04</strong></td></tr><tr><td style=text-align:left><code>TRI-ML/mamba-7b-rw</code><sup>*</sup></td><td style=text-align:center>22.46</td><td style=text-align:center>6.71</td><td style=text-align:center>0.45</td><td style=text-align:center>1.12</td><td style=text-align:center>5.51</td><td style=text-align:center>1.69</td><td style=text-align:center>6.25</td></tr><tr><td style=text-align:left><em><strong>Hybrid SSM-attention models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td></td></tr><tr><td style=text-align:left><code>recurrentgemma-9b</code></td><td style=text-align:center>30.76</td><td style=text-align:center>14.80</td><td style=text-align:center>4.83</td><td style=text-align:center>4.70</td><td style=text-align:center>6.60</td><td style=text-align:center>17.88</td><td style=text-align:center>13.20</td></tr><tr><td style=text-align:left><code>Zyphra/Zamba-7B-v1</code><sup>*</sup></td><td style=text-align:center>24.06</td><td style=text-align:center>21.12</td><td style=text-align:center>3.32</td><td style=text-align:center>3.03</td><td style=text-align:center>7.74</td><td style=text-align:center>16.02</td><td style=text-align:center>12.55</td></tr><tr><td style=text-align:left><em><strong>Transformer models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left><code>Falcon2-11B</code></td><td style=text-align:center>32.61</td><td style=text-align:center>21.94</td><td style=text-align:center>2.34</td><td style=text-align:center>2.80</td><td style=text-align:center>7.53</td><td style=text-align:center>15.44</td><td style=text-align:center>13.78</td></tr><tr><td style=text-align:left><code>Meta-Llama-3-8B</code></td><td style=text-align:center>14.55</td><td style=text-align:center>24.50</td><td style=text-align:center>3.25</td><td style=text-align:center>7.38</td><td style=text-align:center>6.24</td><td style=text-align:center>24.55</td><td style=text-align:center>13.41</td></tr><tr><td style=text-align:left><code>Meta-Llama-3.1-8B</code></td><td style=text-align:center>12.70</td><td style=text-align:center>25.29</td><td style=text-align:center>4.61</td><td style=text-align:center>6.15</td><td style=text-align:center>8.98</td><td style=text-align:center>24.95</td><td style=text-align:center>13.78</td></tr><tr><td style=text-align:left><code>Mistral-7B-v0.1</code></td><td style=text-align:center>23.86</td><td style=text-align:center>22.02</td><td style=text-align:center>2.49</td><td style=text-align:center>5.59</td><td style=text-align:center>10.68</td><td style=text-align:center>22.36</td><td style=text-align:center>14.50</td></tr><tr><td style=text-align:left><code>Mistral-Nemo-Base-2407 (12B)</code></td><td style=text-align:center>16.83</td><td style=text-align:center>29.37</td><td style=text-align:center>4.98</td><td style=text-align:center>5.82</td><td style=text-align:center>6.52</td><td style=text-align:center>27.46</td><td style=text-align:center>15.08</td></tr><tr><td style=text-align:left><code>gemma-7B</code></td><td style=text-align:center>26.59</td><td style=text-align:center>21.12</td><td style=text-align:center>6.42</td><td style=text-align:center>4.92</td><td style=text-align:center>10.98</td><td style=text-align:center>21.64</td><td style=text-align:center><strong>15.28</strong></td></tr></tbody></table><p>Also, we evaluate our model on the benchmarks of the first version of the LLM Leaderboard using <code>lighteval</code>.</p><table><thead><tr><th style=text-align:left><code>model name</code></th><th style=text-align:center><code>ARC</code></th><th style=text-align:center><code>HellaSwag</code></th><th style=text-align:center><code>MMLU</code></th><th style=text-align:center><code>Winogrande</code></th><th style=text-align:center><code>TruthfulQA</code></th><th style=text-align:center><code>GSM8K</code></th><th style=text-align:center><code>Average</code></th></tr></thead><tbody><tr><td style=text-align:left><em><strong>Pure SSM models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left><code>Falcon Mamba-7B</code><sup>*</sup></td><td style=text-align:center>62.03</td><td style=text-align:center>80.82</td><td style=text-align:center>62.11</td><td style=text-align:center>73.64</td><td style=text-align:center>53.42</td><td style=text-align:center>52.54</td><td style=text-align:center><strong>64.09</strong></td></tr><tr><td style=text-align:left><code>TRI-ML/mamba-7b-rw</code><sup>*</sup></td><td style=text-align:center>51.25</td><td style=text-align:center>80.85</td><td style=text-align:center>33.41</td><td style=text-align:center>71.11</td><td style=text-align:center>32.08</td><td style=text-align:center>4.70</td><td style=text-align:center>45.52</td></tr><tr><td style=text-align:left><em><strong>Hybrid SSM-attention models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left><code>recurrentgemma-9b</code><sup>**</sup></td><td style=text-align:center>52.00</td><td style=text-align:center>80.40</td><td style=text-align:center>60.50</td><td style=text-align:center>73.60</td><td style=text-align:center>38.60</td><td style=text-align:center>42.60</td><td style=text-align:center>57.95</td></tr><tr><td style=text-align:left><code>Zyphra/Zamba-7B-v1</code><sup>*</sup></td><td style=text-align:center>56.14</td><td style=text-align:center>82.23</td><td style=text-align:center>58.11</td><td style=text-align:center>79.87</td><td style=text-align:center>52.88</td><td style=text-align:center>30.78</td><td style=text-align:center>60.00</td></tr><tr><td style=text-align:left><em><strong>Transformer models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left><code>Falcon2-11B</code></td><td style=text-align:center>59.73</td><td style=text-align:center>82.91</td><td style=text-align:center>58.37</td><td style=text-align:center>78.30</td><td style=text-align:center>52.56</td><td style=text-align:center>53.83</td><td style=text-align:center><strong>64.28</strong></td></tr><tr><td style=text-align:left><code>Meta-Llama-3-8B</code></td><td style=text-align:center>60.24</td><td style=text-align:center>82.23</td><td style=text-align:center>66.70</td><td style=text-align:center>78.45</td><td style=text-align:center>42.93</td><td style=text-align:center>45.19</td><td style=text-align:center>62.62</td></tr><tr><td style=text-align:left><code>Meta-Llama-3.1-8B</code></td><td style=text-align:center>58.53</td><td style=text-align:center>82.13</td><td style=text-align:center>66.43</td><td style=text-align:center>74.35</td><td style=text-align:center>44.29</td><td style=text-align:center>47.92</td><td style=text-align:center>62.28</td></tr><tr><td style=text-align:left><code>Mistral-7B-v0.1</code></td><td style=text-align:center>59.98</td><td style=text-align:center>83.31</td><td style=text-align:center>64.16</td><td style=text-align:center>78.37</td><td style=text-align:center>42.15</td><td style=text-align:center>37.83</td><td style=text-align:center>60.97</td></tr><tr><td style=text-align:left><code>gemma-7B</code></td><td style=text-align:center>61.09</td><td style=text-align:center>82.20</td><td style=text-align:center>64.56</td><td style=text-align:center>79.01</td><td style=text-align:center>44.79</td><td style=text-align:center>50.87</td><td style=text-align:center>63.75</td></tr></tbody></table><p>For the models marked by <em>star</em>, we evaluated the tasks internally, while for the models marked by two <em>stars</em>, the results were taken from paper or model card.</p><h2 id=processing-large-sequences>Processing large sequences<a hidden class=anchor aria-hidden=true href=#processing-large-sequences>#</a></h2><p>Following theoretical efficiency SSM models in processing large sequences, we perform a comparison of memory usage and generation throughput between Falcon Mamba and popular transfomer models using the
<a href=https://github.com/huggingface/optimum-benchmark>optimum-benchmark</a> library. For a fair comparison, we rescaled the vocabulary size of all transformer models to match Falcon Mamba since it has a big impact on the memory requirements of the model.</p><p>Before going to the results, let&rsquo;s first discuss the difference between the prompt (prefill) and generated (decode) parts of the sequence. As we will see, the details of prefill are more important for state space models than for transformer models. When a transformer generates the next token, it needs to attend to the keys and values of all previous tokens in the context. This implies linear scaling of both memory requirements and generation time with context length. A state space model attends to and stores only its recurrent state and, therefore, doesn&rsquo;t need additional memory or time to generate large sequences. While this explains the claimed advantage of SSMs over transformers in the decode stage, the prefill stage requires additional effort to fully utilize SSM architecture.</p><p>A standard approach for prefill is to process the whole prompt in parallel to fully utilize GPU. This approach is used in <a href=https://github.com/huggingface/optimum-benchmark>optimum-benchmark</a> library and we will refer to it as parallel prefill. Parallel prefill needs to store in memory the hidden states of each token in the prompt. For transformers, this additional memory is dominated by the memory of stored KV caches. For SSM models, no caching is required, and the memory for storing hidden states becomes the only component proportional to the prompt length. As a result, the memory requirement will scale with prompt length, and SSM models will lose the ability to process arbitrary long sequences, similar to transformers.</p><p>An alternative to parallel prefill is to process the prompt token by token, which we will refer to as <em>sequential prefill</em>. Akin to sequence parallelism, it can also be done on larger chunks of the prompt instead of individual tokens for better GPU usage. While sequential prefill makes little sense for transformers, it brings back the possibility of processing arbitrary long prompts by SSM models.</p><p>With these remarks in mind, we first test the largest sequence length that can fit on a single 24 GB A10 GPU, putting the results on the <a href=/blog/falcon-mamba/#max-length>figure</a> below. The batch size is fixed at 1, and we are using float32 precision. Even for parallel prefill, Falcon Mamba can fit larger sequences than a transformer, while in sequential prefill, it unlocks its full potential and can process arbitrary long prompt</p><p><a id=max-length></a><img loading=lazy src=https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/falcon_mamba/max_len_llalma3-1.png alt="Model Performance"></p><p>Next, we measure the generation throughput in a setting with a prompt of length 1 and up to 130k generated tokens, using batch size 1 and H100 GPU. The results are reported in the <a href=/blog/falcon-mamba/#throughput>figure</a> below. We observe that our Falcon Mamba is generating all the tokens at constant throughput and without any increase in CUDA peak memory. For the transformer model, the peak memory grows, and generation speed slows down as the number of generated tokens grows.</p><p><a id=throughput></a><img loading=lazy src=https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/falcon_mamba/thoughput-llama3-1.png alt="Model Performance"></p><h2 id=how-to-use-it-within-hugging-face-transformers>How to use it within Hugging Face transformers?<a hidden class=anchor aria-hidden=true href=#how-to-use-it-within-hugging-face-transformers>#</a></h2><p>The Falcon Mamba architecture will be available in the next release of the Hugging Face transformers library (>4.45.0). To use the model, make sure to install the latest version of Hugging Face transformers or install the library from the source.</p><p>Falcon Mamba is compatible with most of the APIs Hugging Face offers, which you are familiar with, such as <code>AutoModelForCausalLM</code> or <code>pipeline</code> :</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;tiiuae/falcon-mamba-7b&#34;</span> 
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>,</span> <span class=n>torch_dtype</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span> <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&#34;Hello world, today&#34;</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>Output</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span> 
</span></span></code></pre></div><p>As the model is large, it also supports features such as <code>bitsandbytes</code> quantization to run the model on smaller GPU memory constraints, e.g.:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span><span class=p>,</span> <span class=n>BitsAndBytesConfig</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_id</span> <span class=o>=</span> <span class=s2>&#34;tiiuae/falcon-mamba-7b&#34;</span> 
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>quantization_config</span> <span class=o>=</span> <span class=n>BitsAndBytesConfig</span><span class=p>(</span><span class=n>load_in_4bit</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_id</span><span class=p>,</span> <span class=n>quantization_config</span><span class=o>=</span><span class=n>quantization_config</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&#34;Hello world, today&#34;</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span> 
</span></span></code></pre></div><p>We are also pleased to introduce the instruction-tuned version of Falcon Mamba, which has been fine-tuned with an additional 5 billion tokens of supervised fine-tuning (SFT) data. This extended training enhances the model&rsquo;s ability to perform instructional tasks with better precision and effectiveness. You can experience the capabilities of the instruct model through our demo, available <a href=https://huggingface.co/spaces/tiiuae/falcon-mamba-playground>here</a>. For the chat template we use the following format:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>&lt;<span class=p>|</span>im_start<span class=p>|</span>&gt;user
</span></span><span class=line><span class=cl>prompt&lt;<span class=p>|</span>im_end<span class=p>|</span>&gt;
</span></span><span class=line><span class=cl>&lt;<span class=p>|</span>im_start<span class=p>|</span>&gt;assistant
</span></span></code></pre></div><p>You can also directly use the 4-bit converted version of both the <a href=https://huggingface.co/tiiuae/falcon-mamba-7b-4bit>base model</a> and the <a href=https://huggingface.co/tiiuae/falcon-mamba-7b-instruct-4bit>instruct model</a>. Make sure to have access to a GPU that is compatible with <code>bitsandbytes</code> library to run the quantized model.</p><p>You can also benefit from faster inference using <code>torch.compile</code>; simply call <code>model = torch.compile(model)</code> once you have loaded the model.</p><h2 id=acknowledgments>Acknowledgments<a hidden class=anchor aria-hidden=true href=#acknowledgments>#</a></h2><p>The authors of this blog post would like to thank the Hugging Face team for their smooth support and integration within their ecosystem, in particular</p><ul><li><a href=https://huggingface.co/alozowski>Alina Lozovskaya</a> and <a href=https://huggingface.co/clefourrier>Clementine Fourrier</a> for helping us evaluating the model on the leaderboard</li><li><a href=https://huggingface.co/ArthurZ>Arthur Zucker</a> for the transformers integration</li><li><a href=https://huggingface.co/reach-vb>Vaibhav Srivastav</a>, <a href=https://huggingface.co/hysts>hysts</a> and <a href=https://huggingface.co/osanseviero>Omar Sanseviero</a> for their support with questions related to Hub</li></ul><p>The authors would also like to thank Tri Dao and Albert Gu for implementing and open-sourcing Mamba architecture to the community.</p><div class=post-contributors><div class=contributors-section><h4>Core Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Falcon LLM team" class=contributor-image><p class=contributor-name>Falcon LLM team</p></div></div></div><br></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>