<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script>!function(e,t){if(!e.rdt){var s,o,n=e.rdt=function(){n.sendEvent?n.sendEvent.apply(n,arguments):n.callQueue.push(arguments)};n.callQueue=[],s=t.createElement("script"),s.src="https://www.redditstatic.com/ads/pixel.js",s.async=!0,o=t.getElementsByTagName("script")[0],o.parentNode.insertBefore(s,o)}}(window,document),rdt("init","a2_f0we4sxffrnm"),rdt("track","PageVisit")</script><script async src="https://www.googletagmanager.com/gtag/js?id=AW-16575356988"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","AW-16575356988")</script><title>Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture | Falcon</title>
<meta name=keywords content><meta name=description content="
Check out the Arabic version translated by Falcon-H1-Arabic
The journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we&rsquo;re excited to announce Falcon-H1-Arabic, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in three powerful models that set new standards for Arabic natural language processing."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/blog/falcon-h1-arabic/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/blog/falcon-h1-arabic/><link rel=alternate hreflang=ar href=https://falcon-lm.github.io/ar/blog/falcon-h1-arabic/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture"><meta property="og:description" content="
Check out the Arabic version translated by Falcon-H1-Arabic
The journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we&rsquo;re excited to announce Falcon-H1-Arabic, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in three powerful models that set new standards for Arabic natural language processing."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/blog/falcon-h1-arabic/"><meta property="og:image" content="https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-01-05T01:00:00+00:00"><meta property="article:modified_time" content="2026-01-05T01:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic.png"><meta name=twitter:title content="Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture"><meta name=twitter:description content="
Check out the Arabic version translated by Falcon-H1-Arabic
The journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we&rsquo;re excited to announce Falcon-H1-Arabic, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in three powerful models that set new standards for Arabic natural language processing."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://falcon-lm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture","item":"https://falcon-lm.github.io/blog/falcon-h1-arabic/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture","name":"Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture","description":" Check out the Arabic version translated by Falcon-H1-Arabic\nThe journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we\u0026rsquo;re excited to announce Falcon-H1-Arabic, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in three powerful models that set new standards for Arabic natural language processing.\n","keywords":[],"articleBody":" Check out the Arabic version translated by Falcon-H1-Arabic\nThe journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we‚Äôre excited to announce Falcon-H1-Arabic, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in three powerful models that set new standards for Arabic natural language processing.\nBuilding on Success: The Evolution from Falcon-Arabic When we launched Falcon-Arabic a few months ago, the response from the community was both humbling and enlightening. Developers, researchers and students across the Arab world used the model for real use cases, pushing them to its limits and providing invaluable feedback. We learned where the model excelled and, more importantly, where it struggled. Long-context understanding, dialectal variations, mathematical reasoning, and domain-specific knowledge emerged as key areas requiring deeper attention.\nWe didn‚Äôt just want to make incremental improvements, we wanted to fundamentally rethink our approach. The result is Falcon-H1-Arabic, a model family that addresses every piece of feedback we received while introducing architectural innovations that were previously unexplored in Arabic language modeling.\nA First for Arabic NLP: Hybrid Mamba-Transformer Architecture Falcon-H1-Arabic is built on the Falcon-H1 hybrid architecture, which integrates State Space Models (Mamba) and Transformer attention within every block. Both components run in parallel and their representations are fused before the block‚Äôs output projection. This design provides the linear-time scalability of Mamba for extremely long sequences while preserving the precise long-range modeling capabilities of attention. For Arabic, with its rich morphology and flexible sentence structures, this approach significantly improves coherence and reasoning across extended text. We‚Äôve deployed this architecture across three scales (3B, 7B, 34B parameters), each balancing capacity, efficiency, and deployability for different use cases from edge devices to enterprise applications.\nFalcon-H1 architecture. Attention and SSM run in parallel within each block; their outputs are concatenated before the block‚Äôs output projection. The number of SSM/Attention heads depends on the model size. More details on the Falcon-H1 technical report. Breaking Context Boundaries We‚Äôve dramatically increased context capabilities from Falcon-Arabic‚Äôs 32K limit to 128K tokens for the 3B model and 256K tokens for both the 7B and 34B models. At 256K tokens (~200,000 words), these models can process several novels or hundreds of pages of technical documentation enabling applications in legal analysis, medical records, academic research, and extended conversations that were previously impractical. Our post-training specifically addresses ‚Äúlost in the middle‚Äù challenges to ensure models effectively utilize their full context range, not just accept long inputs.\nParameters Context Window Architecture Ideal Uses 3B 128K Hybrid Fast agents, high-QPS systems, lightweight analytics 7B 256K Hybrid Production assistants, reasoning, enterprise chat 34B 256K Hybrid Long-document analysis, research, high-stakes tasks Data Quality and Diversity: The Foundation of Excellence We rebuilt our pre-training data pipeline from the ground up to better reflect the complexity of Arabic. This began with a multi-stage quality filtering process tailored to Arabic orthography, morphology, diacritics, and syntactic patterns. Instead of heuristic filtering, we used deep linguistic analysis to isolate coherent, well-structured text and remove noise commonly found in open-web corpora. The result is a significantly cleaner, more stylistically consistent Arabic dataset.\nDialect coverage was another key priority. Arabic is not monolithic; Modern Standard Arabic coexists with dialects such as Egyptian, Levantine, Gulf, and Maghrebi, each with distinct vocabularies and grammatical constructions. We expanded dialectal sources substantially so the models would understand and generate the full spectrum of real-world Arabic rather than leaning disproportionately toward formal MSA. To maintain global reasoning and domain diversity, we also preserved the multilingual capabilities of Falcon-H1 by training the Arabic models on an almost equal mix of Arabic, English, and multilingual content totalling around 300 Billion Tokens. This ensures strong performance in code, STEM, and cross-lingual reasoning. The following figure illustrates the distribution of the pre-training data across languages and categories. All values are expressed in billions of tokens.\nPost-Training: Refining Capabilities Without Compromising Competence After pre-training, Falcon-H1-Arabic undergoes a focused post-training pipeline consisting of supervised fine-tuning (SFT) followed by direct preference optimization (DPO). During SFT, we expose the models to high-quality Arabic instructions, curated long-context examples, and structured reasoning tasks that teach them to follow directives, maintain coherence over extended sequences, and ground their responses in relevant information. This stage is crucial for ensuring that the models can actually use their large context windows which does not emerge automatically from architecture alone.\nWe follow SFT with a targeted DPO phase to refine alignment, conversational quality, and preference consistency. DPO helps the models balance long-context reasoning with general linguistic competence, improving helpfulness and reducing common failure modes such as drifting, overuse of context, or neglecting earlier information. Throughout both stages, we carefully monitor for catastrophic forgetting and maintain a controlled curriculum so gains in long-context behavior do not come at the expense of core reasoning or factual accuracy. The result is a family of models that handles extended documents and dialogue with ease while preserving strong performance on everyday language tasks.\nBeyond benchmark-oriented optimization, our post-training process deliberately strengthens areas that traditional evaluations do not fully capture, including conversational faithfulness, rhetorical organization, structured follow-ups, and discourse coherence. These enhancements significantly boost the model‚Äôs practical usefulness, making Falcon-H1-Arabic more dependable in real multi-turn dialogue, instruction execution, and long-context conversational flows.\nBenchmark Performance: Setting New Standards Numbers tell an important part of the story. On the Open Arabic LLM Leaderboard (OALL), a comprehensive benchmark evaluating Arabic language understanding across diverse tasks, Falcon-H1-Arabic achieves state-of-the-art results at every scale we tested. Note that our scores may vary slightly from those reported on the leaderboard, as we used vLLM as the backend instead of the leaderboard‚Äôs Accelerate-based implementation. These differences are typically under one point while offering significantly faster runtime.\nBeyond OALL, we also report results on the 3LM benchmark for STEM-related tasks on both synthetic and native splits; Arabculture for Arabic culture assessment; and AraDice for Arabic dialect coverage across Levantine, and Egyptian varieties as well as Arabic culture across 6 countries. The reported AraDice score is the average of all the 3 scores.\nStarting with the 3B model, the performance is exceptional. It reaches approximately 62% on OALL, outperforming all small-scale models, including Gemma-4B, Qwen3-4B, and Phi-4-mini by roughly ten points. On 3LM, the main Arabic STEM benchmark, it scores around 82% on the native split and 73% on the synthetic split. It also achieves about 62% on the ArabCulture benchmark and around 50% across AraDice dialect evaluation (Egyptian, Gulf, and Levantine). This makes Falcon-H1-Arabic-3B a high-quality, highly efficient model suitable for edge deployments, real-time applications, and agentic systems where latency and cost matter.\nThe 7B model continues this upward trajectory. With a score of 71.7% on OALL, it surpasses all models in the ~10B class, including Fanar-9B, Allam-7B*, and Qwen3-8B. On 3LM, it reaches about 92% on the native split and 85% on the synthetic one. AraDice scores rise into the mid-50s across all dialects, and ArabCulture results approach 80%. This model strikes an ideal balance between capability and deployability, making it the most practical choice for general-purpose Arabic NLP in production environments.\nThe 34B model represents our flagship system and establishes a new state of the art for Arabic language modeling. It reaches approximately 75% on OALL, outperforming not only models of similar size but even much larger systems such as Llama-3.3-70B and AceGPT2-32B. Its 3LM scores reach about 96% on the native split and 94% on the synthetic one. On ArabCulture it scores close to 80%, and on AraDice it reaches around 53 across dialects. The fact that a 34B hybrid model surpasses the performance of 70B-scale transformers demonstrates the effectiveness of the Falcon-H1 architecture, the quality of the data, and the strength of the post-training pipeline.\nThese benchmark results validate our approach but also highlight an important reality: the frontier of Arabic language modeling is advancing rapidly. Each percentage point on these benchmarks represents countless hours of engineering effort, careful dataset curation, and architectural refinement. The margins by which Falcon-H1-Arabic leads aren‚Äôt just statistical artifacts, they translate to meaningfully better user experiences in real-world applications.\nPractical Applications: From Edge to Enterprise Each model in the Falcon-H1-Arabic family is suited to different deployment scenarios. The 3B model is optimized for speed, cost-efficiency, and high-throughput systems, making it ideal for agentic workflows, on-device applications, low-latency chat, and environments with strict resource constraints. The 7B model serves as the general-purpose workhorse for most production applications, powering document understanding systems, chatbots, summarization pipelines, and content generation tools. The 34B model is designed for high-stakes domains where accuracy and long-range reasoning matter most, including legal analysis, medical summarization, academic research, and large-scale enterprise automation. Its extended context window makes it uniquely capable of analyzing hundreds of pages of text in a single pass while maintaining precise coherence.\nResponsible AI and Limitations Like all language models, Falcon-H1-Arabic may reflect biases from training data and can produce hallucinated information. Model outputs should not be used as sole authorities for medical, legal, or financial decisions without professional verification. Long-context performance may degrade at extreme ranges. We recommend task-specific evaluation and appropriate guardrails before deployment in production or sensitive applications.\nAcknowledgments This work stands on the shoulders of many. We extend our gratitude to the Arabic NLP research community, whose open sharing of benchmarks, datasets, and methodologies enables progress across the field. Special thanks to our colleagues at TII: Ilyas Chahed, Younes Belkada, Dhia Eddine Rhaiem, Puneesh Khanna, Jingwei Zuo, Mikhail Lubinets, Slim Frikha, Maksim Velikanov, Kacper Piskorski, and Suhail Mohmad for their invaluable support during this project.\nCitation @misc{Falcon-H1-Arabic-2026, title={Falcon-H1-Arabic: State-of-the-Art Arabic Language Models with Hybrid Mamba-Transformer Architecture}, author={Basma El Amel Boussaha and Mohammed Alyafeai and Ahmed Alzubaidi and Leen AlQadi and Shaikha Alsuwaidi and Omar Alkaabi and Hamza Alobeidli and Hakim Hacid}, url={https://huggingface.co/blog/tiiuae/falcon-h1-arabic}, month={January}, year={2026}, note={Available in 3B, 7B, and 34B parameter versions} } NB: the scores of ALLaM-7B-Instruct-preview in our evaluation are higher than those reported on the OALL leaderboard, as we used the newest release (7b-alpha-v2.33.0.30), while the leaderboard currently reflects results from the older version (7b-alpha-v1.27.2.25). Falcon-H1-Arabic models are available for use at the links below. For questions, collaborations, or feedback, reach us at falcon.info@tii.ae or join our community: ‚úâÔ∏è Contact UsüöÄ Try Falcon-H1-Arabicüí¨ Join Discord ","wordCount":"1711","inLanguage":"en","image":"https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic.png","datePublished":"2026-01-05T01:00:00Z","dateModified":"2026-01-05T01:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/blog/falcon-h1-arabic/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:url(/img/falcon-h1-arabic.png)50%/cover no-repeat fixed"></div><div class=hero-gradient></div><div class=hero-blur></div><div class="hero text-light"><h1 class=post-title>Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture</h1><div class=post-meta><span title='2026-01-05 01:00:00 +0000 UTC'>January 5, 2026</span>&nbsp;‚Ä¢&nbsp;9 min&nbsp;‚Ä¢&nbsp;1711 words&nbsp;‚Ä¢&nbsp;Falcon Team&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://falcon-lm.github.io/ar/blog/falcon-h1-arabic/>Arabic</a></li></ul></div></div></div><main class=main><article class=post-single><figure class=entry-cover><a href=/blog/falcon-h1-arabic/falcon-h1-arabic.png target=_blank rel="noopener noreferrer"><img loading=lazy srcset="https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic_hu_15506cb5cc64fd5a.png 360w ,https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic_hu_4be23de62313b3b8.png 480w ,https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic_hu_9426beba3e4f8364.png 720w ,https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic_hu_5751c11e5520f503.png 1080w ,https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic_hu_63b86f5f528a4d3a.png 1500w ,https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic.png 3168w" sizes="(min-width: 768px) 720px, 100vw" src=https://falcon-lm.github.io/blog/falcon-h1-arabic/falcon-h1-arabic.png alt width=3168 height=1344></a></figure><div class=post-content><blockquote><p>Check out the <a href=https://falcon-lm.github.io/ar/blog/falcon-h1-arabic/>Arabic version</a> translated by <strong>Falcon-H1-Arabic</strong></p></blockquote><p>The journey of building world-class Arabic language models has been one of continuous learning and iteration. Today, we&rsquo;re excited to announce <strong>Falcon-H1-Arabic</strong>, our most advanced Arabic language model family to date, representing a significant leap forward in both architecture and capabilities. This release embodies months of research, community feedback, and technical innovation, culminating in <strong>three</strong> powerful models that set new standards for Arabic natural language processing.</p><h2 id=building-on-success-the-evolution-from-falcon-arabic>Building on Success: The Evolution from Falcon-Arabic<a hidden class=anchor aria-hidden=true href=#building-on-success-the-evolution-from-falcon-arabic>#</a></h2><p>When we launched <a href=https://huggingface.co/blog/tiiuae/falcon-arabic>Falcon-Arabic</a> a few months ago, the response from the community was both humbling and enlightening. Developers, researchers and students across the Arab world used the model for real use cases, pushing them to its limits and providing invaluable feedback. We learned where the model excelled and, more importantly, where it struggled. Long-context understanding, dialectal variations, mathematical reasoning, and domain-specific knowledge emerged as key areas requiring deeper attention.</p><p>We didn&rsquo;t just want to make incremental improvements, we wanted to fundamentally rethink our approach. The result is Falcon-H1-Arabic, a model family that addresses every piece of feedback we received while introducing architectural innovations that were previously unexplored in Arabic language modeling.</p><iframe src=https://visualize.graphy.app/view/342df454-c924-4230-b4e5-1e638f5a351e width=100% height=600 frameborder=0 loading=lazy></iframe><h2 id=a-first-for-arabic-nlp-hybrid-mamba-transformer-architecture>A First for Arabic NLP: Hybrid Mamba-Transformer Architecture<a hidden class=anchor aria-hidden=true href=#a-first-for-arabic-nlp-hybrid-mamba-transformer-architecture>#</a></h2><p>Falcon-H1-Arabic is built on the <a href=https://huggingface.co/blog/tiiuae/falcon-h1><strong>Falcon-H1</strong></a> hybrid architecture, which integrates State Space Models (Mamba) and Transformer attention within every block. Both components run in parallel and their representations are fused before the block‚Äôs output projection. This design provides the linear-time scalability of Mamba for extremely long sequences while preserving the precise long-range modeling capabilities of attention. For Arabic, with its rich morphology and flexible sentence structures, this approach significantly improves coherence and reasoning across extended text. We&rsquo;ve deployed this architecture across three scales (3B, 7B, 34B parameters), each balancing capacity, efficiency, and deployability for different use cases from edge devices to enterprise applications.</p><p align=center><img src=https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/y1PdfPUZABgyA4q6J4-6r.png width=500><br><em>Falcon-H1 architecture. Attention and SSM run in parallel within each block; their outputs are concatenated before the block‚Äôs output projection. The number of SSM/Attention heads depends on the model size. More details on the <a href=https://arxiv.org/pdf/2507.22448 target=_blank>Falcon-H1 technical report</a>.</em></p><h2 id=breaking-context-boundaries>Breaking Context Boundaries<a hidden class=anchor aria-hidden=true href=#breaking-context-boundaries>#</a></h2><p>We&rsquo;ve dramatically increased context capabilities from Falcon-Arabic&rsquo;s 32K limit to 128K tokens for the 3B model and 256K tokens for both the 7B and 34B models. At 256K tokens (~200,000 words), these models can process several novels or hundreds of pages of technical documentation enabling applications in legal analysis, medical records, academic research, and extended conversations that were previously impractical. Our post-training specifically addresses &ldquo;lost in the middle&rdquo; challenges to ensure models effectively utilize their full context range, not just accept long inputs.</p><div align=center><table width=90% style=border-collapse:collapse;background:#f3f3f3><tr style=background:linear-gradient(90deg,#d6c5ff,#b9dbff)><th style="padding:10px;border:1px solid #cfcfe0;color:#1f2b66">Parameters</th><th style="padding:10px;border:1px solid #cfcfe0;color:#1f2b66">Context Window</th><th style="padding:10px;border:1px solid #cfcfe0;color:#1f2b66">Architecture</th><th style="padding:10px;border:1px solid #cfcfe0;color:#1f2b66">Ideal Uses</th></tr><tr style=color:#000><td style="padding:10px;border:1px solid #cfcfe0">3B</td><td style="padding:10px;border:1px solid #cfcfe0">128K</td><td style="padding:10px;border:1px solid #cfcfe0">Hybrid</td><td style="padding:10px;border:1px solid #cfcfe0">Fast agents, high-QPS systems, lightweight analytics</td></tr><tr style=color:#000><td style="padding:10px;border:1px solid #cfcfe0">7B</td><td style="padding:10px;border:1px solid #cfcfe0">256K</td><td style="padding:10px;border:1px solid #cfcfe0">Hybrid</td><td style="padding:10px;border:1px solid #cfcfe0">Production assistants, reasoning, enterprise chat</td></tr><tr style=color:#000><td style="padding:10px;border:1px solid #cfcfe0">34B</td><td style="padding:10px;border:1px solid #cfcfe0">256K</td><td style="padding:10px;border:1px solid #cfcfe0">Hybrid</td><td style="padding:10px;border:1px solid #cfcfe0">Long-document analysis, research, high-stakes tasks</td></tr></table></div><h2 id=data-quality-and-diversity-the-foundation-of-excellence>Data Quality and Diversity: The Foundation of Excellence<a hidden class=anchor aria-hidden=true href=#data-quality-and-diversity-the-foundation-of-excellence>#</a></h2><p>We rebuilt our pre-training data pipeline from the ground up to better reflect the complexity of Arabic. This began with a multi-stage quality filtering process tailored to Arabic orthography, morphology, diacritics, and syntactic patterns. Instead of heuristic filtering, we used deep linguistic analysis to isolate coherent, well-structured text and remove noise commonly found in open-web corpora. The result is a significantly cleaner, more stylistically consistent Arabic dataset.</p><p>Dialect coverage was another key priority. Arabic is not monolithic; Modern Standard Arabic coexists with dialects such as Egyptian, Levantine, Gulf, and Maghrebi, each with distinct vocabularies and grammatical constructions. We expanded dialectal sources substantially so the models would understand and generate the full spectrum of real-world Arabic rather than leaning disproportionately toward formal MSA. To maintain global reasoning and domain diversity, we also preserved the multilingual capabilities of Falcon-H1 by training the Arabic models on an almost equal mix of Arabic, English, and multilingual content totalling around 300 Billion Tokens. This ensures strong performance in code, STEM, and cross-lingual reasoning. The following figure illustrates the distribution of the pre-training data across languages and categories. All values are expressed in billions of tokens.</p><p align=center><img src=https://cdn-uploads.huggingface.co/production/uploads/659bc8a7b0f43ed69f0b2300/NrIio6sopNNC6wX1mWKzA.png><br></p><h2 id=post-training-refining-capabilities-without-compromising-competence>Post-Training: Refining Capabilities Without Compromising Competence<a hidden class=anchor aria-hidden=true href=#post-training-refining-capabilities-without-compromising-competence>#</a></h2><p>After pre-training, Falcon-H1-Arabic undergoes a focused post-training pipeline consisting of supervised fine-tuning (SFT) followed by direct preference optimization (DPO). During SFT, we expose the models to high-quality Arabic instructions, curated long-context examples, and structured reasoning tasks that teach them to follow directives, maintain coherence over extended sequences, and ground their responses in relevant information. This stage is crucial for ensuring that the models can actually use their large context windows which does not emerge automatically from architecture alone.</p><p>We follow SFT with a targeted DPO phase to refine alignment, conversational quality, and preference consistency. DPO helps the models balance long-context reasoning with general linguistic competence, improving helpfulness and reducing common failure modes such as drifting, overuse of context, or neglecting earlier information. Throughout both stages, we carefully monitor for catastrophic forgetting and maintain a controlled curriculum so gains in long-context behavior do not come at the expense of core reasoning or factual accuracy. The result is a family of models that handles extended documents and dialogue with ease while preserving strong performance on everyday language tasks.</p><p>Beyond benchmark-oriented optimization, our post-training process deliberately strengthens areas that traditional evaluations do not fully capture, including conversational faithfulness, rhetorical organization, structured follow-ups, and discourse coherence. These enhancements significantly boost the model‚Äôs practical usefulness, making Falcon-H1-Arabic more dependable in real multi-turn dialogue, instruction execution, and long-context conversational flows.</p><h2 id=benchmark-performance-setting-new-standards>Benchmark Performance: Setting New Standards<a hidden class=anchor aria-hidden=true href=#benchmark-performance-setting-new-standards>#</a></h2><p>Numbers tell an important part of the story. On the <a href=https://huggingface.co/spaces/OALL/Open-Arabic-LLM-Leaderboard>Open Arabic LLM Leaderboard (OALL)</a>, a comprehensive benchmark evaluating Arabic language understanding across diverse tasks, Falcon-H1-Arabic achieves state-of-the-art results at every scale we tested. Note that our scores may vary slightly from those reported on the leaderboard, as we used vLLM as the backend instead of the leaderboard‚Äôs Accelerate-based implementation. These differences are typically under one point while offering significantly faster runtime.</p><p>Beyond OALL, we also report results on the <a href=https://aclanthology.org/2025.arabicnlp-main.4.pdf>3LM benchmark</a> for STEM-related tasks on both synthetic and native splits; <a href=https://aclanthology.org/2025.loreslm-1.29.pdf>Arabculture</a> for Arabic culture assessment; and <a href=https://aclanthology.org/2025.coling-main.283.pdf>AraDice</a> for Arabic dialect coverage across Levantine, and Egyptian varieties as well as Arabic culture across 6 countries. The reported AraDice score is the average of all the 3 scores.</p><iframe src=https://visualize.graphy.app/view/f9b6bf39-6106-437d-aed7-21b3f7a17538 width=100% height=600 frameborder=0 loading=lazy></iframe>
<iframe src=https://visualize.graphy.app/view/354bac1b-bc97-4e40-9daf-bd3d3d4c7313 width=100% height=600 frameborder=0 loading=lazy></iframe><p>Starting with the 3B model, the performance is exceptional. It reaches approximately 62% on OALL, outperforming all small-scale models, including Gemma-4B, Qwen3-4B, and Phi-4-mini by roughly ten points. On 3LM, the main Arabic STEM benchmark, it scores around 82% on the native split and 73% on the synthetic split. It also achieves about 62% on the ArabCulture benchmark and around 50% across AraDice dialect evaluation (Egyptian, Gulf, and Levantine). This makes Falcon-H1-Arabic-3B a high-quality, highly efficient model suitable for edge deployments, real-time applications, and agentic systems where latency and cost matter.</p><iframe src=https://visualize.graphy.app/view/db964eca-46eb-46a4-9ece-8ad7a30ee964 width=100% height=600 frameborder=0 loading=lazy></iframe>
<iframe src=https://visualize.graphy.app/view/dcce6a9e-4e45-489f-85a9-5c962791a92d width=100% height=600 frameborder=0 loading=lazy></iframe><p>The 7B model continues this upward trajectory. With a score of 71.7% on OALL, it surpasses all models in the ~10B class, including Fanar-9B, Allam-7B*, and Qwen3-8B. On 3LM, it reaches about 92% on the native split and 85% on the synthetic one. AraDice scores rise into the mid-50s across all dialects, and ArabCulture results approach 80%. This model strikes an ideal balance between capability and deployability, making it the most practical choice for general-purpose Arabic NLP in production environments.</p><p>The 34B model represents our flagship system and establishes a new state of the art for Arabic language modeling. It reaches approximately 75% on OALL, outperforming not only models of similar size but even much larger systems such as Llama-3.3-70B and AceGPT2-32B. Its 3LM scores reach about 96% on the native split and 94% on the synthetic one. On ArabCulture it scores close to 80%, and on AraDice it reaches around 53 across dialects. The fact that a 34B hybrid model surpasses the performance of 70B-scale transformers demonstrates the effectiveness of the Falcon-H1 architecture, the quality of the data, and the strength of the post-training pipeline.</p><iframe src=https://visualize.graphy.app/view/9ffc3ac5-281f-49d0-8167-2d2b18cb3f46 width=100% height=600 frameborder=0 loading=lazy></iframe>
<iframe src=https://visualize.graphy.app/view/0fc43d92-6440-4347-97a7-f4503c4498e3 width=100% height=600 frameborder=0 loading=lazy></iframe><p>These benchmark results validate our approach but also highlight an important reality: the frontier of Arabic language modeling is advancing rapidly. Each percentage point on these benchmarks represents countless hours of engineering effort, careful dataset curation, and architectural refinement. The margins by which Falcon-H1-Arabic leads aren&rsquo;t just statistical artifacts, they translate to meaningfully better user experiences in real-world applications.</p><h2 id=practical-applications-from-edge-to-enterprise>Practical Applications: From Edge to Enterprise<a hidden class=anchor aria-hidden=true href=#practical-applications-from-edge-to-enterprise>#</a></h2><p>Each model in the Falcon-H1-Arabic family is suited to different deployment scenarios. The 3B model is optimized for speed, cost-efficiency, and high-throughput systems, making it ideal for agentic workflows, on-device applications, low-latency chat, and environments with strict resource constraints. The 7B model serves as the general-purpose workhorse for most production applications, powering document understanding systems, chatbots, summarization pipelines, and content generation tools. The 34B model is designed for high-stakes domains where accuracy and long-range reasoning matter most, including legal analysis, medical summarization, academic research, and large-scale enterprise automation. Its extended context window makes it uniquely capable of analyzing hundreds of pages of text in a single pass while maintaining precise coherence.</p><h2 id=responsible-ai-and-limitations>Responsible AI and Limitations<a hidden class=anchor aria-hidden=true href=#responsible-ai-and-limitations>#</a></h2><p>Like all language models, Falcon-H1-Arabic may reflect biases from training data and can produce hallucinated information. Model outputs should not be used as sole authorities for medical, legal, or financial decisions without professional verification. Long-context performance may degrade at extreme ranges. We recommend task-specific evaluation and appropriate guardrails before deployment in production or sensitive applications.</p><h2 id=acknowledgments>Acknowledgments<a hidden class=anchor aria-hidden=true href=#acknowledgments>#</a></h2><p>This work stands on the shoulders of many. We extend our gratitude to the Arabic NLP research community, whose open sharing of benchmarks, datasets, and methodologies enables progress across the field. Special thanks to our colleagues at TII: Ilyas Chahed, Younes Belkada, Dhia Eddine Rhaiem, Puneesh Khanna, Jingwei Zuo, Mikhail Lubinets, Slim Frikha, Maksim Velikanov, Kacper Piskorski, and Suhail Mohmad for their invaluable support during this project.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@misc</span><span class=p>{</span><span class=nl>Falcon-H1-Arabic-2026</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span><span class=p>=</span><span class=s>{Falcon-H1-Arabic: State-of-the-Art Arabic Language Models with Hybrid Mamba-Transformer Architecture}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span><span class=p>=</span><span class=s>{Basma El Amel Boussaha and Mohammed Alyafeai and Ahmed Alzubaidi and Leen AlQadi and Shaikha Alsuwaidi and Omar Alkaabi and Hamza Alobeidli and Hakim Hacid}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span><span class=p>=</span><span class=s>{https://huggingface.co/blog/tiiuae/falcon-h1-arabic}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span><span class=p>=</span><span class=s>{January}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span><span class=p>=</span><span class=s>{2026}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>note</span><span class=p>=</span><span class=s>{Available in 3B, 7B, and 34B parameter versions}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ul><li>NB: the scores of <a href=https://huggingface.co/humain-ai/ALLaM-7B-Instruct-preview>ALLaM-7B-Instruct-preview</a> in our evaluation are higher than those reported on the <a href=https://huggingface.co/spaces/OALL/Open-Arabic-LLM-Leaderboard>OALL leaderboard</a>, as we used the newest release (7b-alpha-v2.33.0.30), while the leaderboard currently reflects results from the older version (7b-alpha-v1.27.2.25).</li></ul><hr><div align=center style=margin-top:16px;margin-bottom:20px><div style=font-size:17px;font-weight:600;margin-bottom:12px>Falcon-H1-Arabic models are available for use at the links below. For questions, collaborations, or feedback, reach us at <strong>falcon.info@tii.ae</strong> or join our community:</div><div style=white-space:nowrap><a href=mailto:falcon.info@tii.ae style="box-shadow:0 2px 6px rgba(0,0,0,.25);display:inline-block;padding:12px 22px;margin:4px;background:linear-gradient(90deg,#d7dcff,#b8c5ff);color:#000;font-weight:600;text-decoration:none;border-radius:8px;border:1px solid #aeb7e6">‚úâÔ∏è Contact Us</a><a href=https://chat.falconllm.tii.ae/ style="box-shadow:0 2px 6px rgba(0,0,0,.25);display:inline-block;padding:12px 22px;margin:4px;background:linear-gradient(90deg,#c7b3ff,#9fc8ff);color:#333;font-weight:600;text-decoration:none;border-radius:8px;border:1px solid #b0b5d9">üöÄ Try Falcon-H1-Arabic</a><a href=https://discord.gg/W3KQ3MjbVx style="box-shadow:0 2px 6px rgba(0,0,0,.25);display:inline-block;padding:12px 22px;margin:4px;background:linear-gradient(90deg,#d4d9ff,#a8b6ff);color:#000;font-weight:600;text-decoration:none;border-radius:8px;border:1px solid #a7b0ef">üí¨ Join Discord</a></div></div><div class=post-contributors><div class=contributors-section><h4>Core Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/basma_boussaha.jpg alt="Basma El Amel Boussaha" class=contributor-image><p class=contributor-name>Basma El Amel Boussaha</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/mohammed_alyafeai.jpg alt="Mohammed Alyafeai" class=contributor-image><p class=contributor-name>Mohammed Alyafeai</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/ahmed_adel_alzubaidi.jpg alt="Ahmed Alzubaidi" class=contributor-image><p class=contributor-name>Ahmed Alzubaidi</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/leen_al_qadi.jpg alt="Leen AlQadi" class=contributor-image><p class=contributor-name>Leen AlQadi</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/shaikha_alsuwaidi.png alt="Shaikha Alsuwaidi" class=contributor-image><p class=contributor-name>Shaikha Alsuwaidi</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/omar_alkaabi.jpg alt="Omar Alkaabi" class=contributor-image><p class=contributor-name>Omar Alkaabi</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/hamza_alobeidli.jpg alt="Hamza Alobeidli" class=contributor-image><p class=contributor-name>Hamza Alobeidli</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/hakim_hacid.png alt="Hakim Hacid" class=contributor-image><p class=contributor-name>Hakim Hacid</p></div></div></div><br></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>
</span><span><a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html rel="noopener noreferrer" target=_blank>| Terms and Conditions</a>
</span><span><a href=https://www.tii.ae/privacy-policy rel="noopener noreferrer" target=_blank>| Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>