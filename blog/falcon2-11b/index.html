<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages | Falcon</title>
<meta name=keywords content><meta name=description content="Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages

The Falcon 2 Models
TII is launching a new generation of models, Falcon 2, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.
The first generation of Falcon models, featuring Falcon-40B and Falcon-180B, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the RefinedWeb, Penedo et al., 2023 and The Falcon Series of Open Language Models, Almazrouei et al., 2023 papers, and the Falcon and Falcon-180B blog posts."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/blog/falcon2-11b/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/blog/falcon2-11b/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages"><meta property="og:description" content="Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages

The Falcon 2 Models
TII is launching a new generation of models, Falcon 2, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.
The first generation of Falcon models, featuring Falcon-40B and Falcon-180B, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the RefinedWeb, Penedo et al., 2023 and The Falcon Series of Open Language Models, Almazrouei et al., 2023 papers, and the Falcon and Falcon-180B blog posts."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/blog/falcon2-11b/"><meta property="og:image" content="https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-05-24T12:00:00+00:00"><meta property="article:modified_time" content="2024-05-24T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages"><meta name=twitter:description content="Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages

The Falcon 2 Models
TII is launching a new generation of models, Falcon 2, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.
The first generation of Falcon models, featuring Falcon-40B and Falcon-180B, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the RefinedWeb, Penedo et al., 2023 and The Falcon Series of Open Language Models, Almazrouei et al., 2023 papers, and the Falcon and Falcon-180B blog posts."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://falcon-lm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages","item":"https://falcon-lm.github.io/blog/falcon2-11b/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages","name":"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages","description":"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages The Falcon 2 Models TII is launching a new generation of models, Falcon 2, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.\nThe first generation of Falcon models, featuring Falcon-40B and Falcon-180B, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the RefinedWeb, Penedo et al., 2023 and The Falcon Series of Open Language Models, Almazrouei et al., 2023 papers, and the Falcon and Falcon-180B blog posts.\n","keywords":[],"articleBody":"Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages The Falcon 2 Models TII is launching a new generation of models, Falcon 2, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.\nThe first generation of Falcon models, featuring Falcon-40B and Falcon-180B, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the RefinedWeb, Penedo et al., 2023 and The Falcon Series of Open Language Models, Almazrouei et al., 2023 papers, and the Falcon and Falcon-180B blog posts.\nThe second generation of models is focused on increased usability and integrability, building a multi-modal ecosystem. We start this journey by releasing not only the base 11B LLM, but also the 11B VLM model that incorporates image understanding capabilities. The vision-language model, or VLM, will allow users to engage in chats about visual content using text.\nAs with our previous work, the models offer support mainly in English but have good capabilities in ten other languages, including Spanish, French, and German.\nTable of Contents The Falcon 2 Models Falcon 2 11B LLM 11B LLM Training Details 11B LLM Evaluation 11B LLM Using the Model Falcon 2 11B VLM 11B VLM Training 11B VLM Evaluation 11B VLM Using the Model Licensing information Falcon2-11B LLM Training Data Falcon2-11B was trained on over 5,000 GT (billion tokens) of RefinedWeb, a high-quality filtered and deduplicated web dataset, enhanced with curated corpora. It followed a four-stage training strategy. The first three stages were focused on increasing the context length, from 2048 to 4096 and finally to 8192 tokens. The last stage aimed to further enhance performance using only high-quality data.\nOverall, the data sources included RefinedWeb-English, RefinedWeb-Europe (cs, de, es, fr, it, nl, pl, pt, ro, sv), high-quality technical data, code data, and conversational data extracted from public sources.\nThe training stages were as follows:\nStage Context Length GT Stage 1 2048 4500 Stage 2 4096 250 Stage 3 8192 250 Stage 4 8192 500 The data was tokenized with Falcon2-11B tokenizer, the same tokenizer as for the previous Falcon models.\nModel Architecture The following table summarizes some of the crucial details about the model architecture:\nDesign choice Value Number of Transformer Blocks 60 Number of Query Heads 32 Number of Key/Value Heads 8 Head Dimensions 128 Parallel Attention yes MLP Upscale Factor 4 Training Procedure Falcon2-11B was trained on 1024 A100 40GB GPUs for the majority of the training, using a 3D parallelism strategy (TP=8, PP=1, DP=128) combined with ZeRO and Flash-Attention 2.\nTraining Hyperparameters Hyperparameter Value Precision bfloat16 Optimizer AdamW Max LR 3.7e-4 Min LR 1.89e-5 LR schedule Cos decay (stage 1) Context length 8192 (stages 3 and 4) Weight decay 1e-1 Z-loss 1e-4 Batch size Variable Falcon2-11B Evaluation English performance Performance on Open LLM Leaderboard tasks:\nCheckpoint GT HellaSwag-10 Winogrande-5 ArcChallenge-25 TruthfulQA-0 MMLU-5 GSMK8k-5 Average Falcon2-11B 5500 82.91 78.30 59.73 52.56 58.37 53.83 64.28 Falcon-40B 1000 85.28 81.29 61.86 41.65 56.89 21.46 58.07 Falcon-7B 1500 78.13 72.38 47.87 34.26 27.79 4.62 44.17 Gemma-7B 6000 82.47 78.45 61.09 44.91 66.03 52.77 64.29 Llama3-8B 15000 82.09 77.35 59.47 43.90 66.69 44.79 62.38 Mistral-7B N/A 83.31 78.37 59.98 42.15 64.16 37.83 60.97 The Hugging Face Leaderboard team provided an official evaluation of our model on the Open LLM Leaderboard tasks. The model performs better than models such as Llama3-8B (trained on three times more data) and Mistral-7B, and on par with Gemma-7b.\nZero shot performance:\nCheckpoint GT HellaSwag ArcEasy Winogrande ArcChallenge Falcon2-11B 5500 82.07 77.78 78.30 50.17 Falcon-40B 1000 82.82 81.86 76.4 54.69 Falcon-7B 1500 76.31 74.74 67.17 43.43 The evaluation results show that the Falcon2-11B shows similar performance to Falcon-40B, at a four times smaller model size!\nMultilingual capabilities Using the Multilingual LLM leaderboard, we compare the Falcon2-11B model to the Llama-7B and Bloom-7B. For reference, we also include Falcon-40B (that supports the same languages), Falcon-7B (that supports French) and Mistral-7B.\nModel Language ID ArcChallenge-25 Hellaswag MMLU 25 TQA Average Falcon2-11B de 43.7 67.96 38.3 47.53 49.37 es 46.2 73.63 37.9 46.43 51.06 fr 45.8 72.41 39.53 47.30 51.27 it 45.6 70.83 38.05 47.14 50.42 nl 41.7 69.05 38.29 48.81 49.47 ro 42.4 66.24 38.01 45.53 48.04 Falcon-40B de 45.1 68.3 36.2 39.8 47.4 es 48.5 73.9 37.2 39.0 49.6 fr 47.6 72.9 37.3 38.5 49.1 it 46.3 70.2 36.4 40.7 48.4 nl 42.9 68.4 36.5 40.9 47.1 ro 43.2 66.0 35.7 39.8 46.2 Falcon-7B fr 37.3 64.1 28.4 34.0 40.9 Mistral-7B de 41.2 58.7 40.5 44.9 46.3 es 44.2 65.3 42.4 43.1 48.7 fr 44.9 64.4 41.9 43.0 48.6 it 43.2 60.9 39.7 43.1 46.7 nl 40.0 57.9 41.4 43.3 45.7 ro 40.7 53.6 39.3 43.6 44.3 Llama-7B de 35.1 49.9 29.9 38.3 38.3 es 36.8 56.4 30.3 37.0 40.1 fr 37.3 55.7 30.5 39.9 40.9 it 35.8 52.0 29.9 39.6 39.3 nl 33.6 48.7 29.8 40.0 38.0 ro 32.4 44.9 29.7 37.0 36.0 Bloom-7B de 26.3 32.4 28.1 43.7 32.6 es 38.1 56.7 28.9 40.4 41.0 fr 36.7 56.6 29.9 40.9 41.0 it 29.0 40.8 27.6 43.7 35.3 nl 23.1 31.7 27.5 42.7 31.3 ro 26.9 31.8 27.4 46.1 33.1 In the spirit of the original Falcon models, the Falcon2-11B was trained not only on English data but also on ten other languages. Our multilingual evaluation results show that the model presents good capabilities in the six languages (de, es, fr, it, nl, ro) featured on the Multilingual LLM Leaderboard and actually shows higher performance than the Falcon-40B and several other multilingual models on all the cited languages.\nWe will soon release more extensive evaluation results for multilingual capabilities in the Falcon2-11B model card!\nCode generation capabilities We check the model’s performance on code generation against the BigCode Leaderboard on the HumanEval benchmark for the Python language, obtaining pass@1 of 29.59%.\nUsing Falcon2-11B from transformers import AutoTokenizer import transformers import torch model = \"tiiuae/falcon-11B\" tokenizer = AutoTokenizer.from_pretrained(model) pipeline = transformers.pipeline( \"text-generation\", model=model, torch_dtype=torch.bfloat16, device_map=\"auto\", ) And then, you’d run text generation using code like the following:\nsequences = pipeline( \"Can you explain the concept of Quantum Computing?\", max_length=200, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, ) for seq in sequences: print(f\"Result: {seq['generated_text']}\") Falcon2-11B VLM Falcon2-11B VLM is a vision-language model (VLM) built on top of the LLM, that additionally handles image inputs and is capable of answering queries about the images. To achieve this, we integrate the pretrained CLIP ViT-L/14 vision encoder with our Falcon2-11B chat-finetuned model, and train with image-text data.\nTo enhance the VLM’s perception of fine-grained details w.r.t small objects in images, we employ a dynamic encoding mechanism at high-resolution for image inputs, similar to LLaVA-Next.\nTraining The training is done in two stages: pretraining and finetuning. In both stages, the visual encoder weights are kept frozen. In the pretraining stage, the LLM is kept frozen, and only the multimodal projector is trained on 558K image-caption pairs. This enables the multimodal projector to learn a mapping from visual to text embedding space. During finetuning, both the projector and LLM weights are trained on a corpus of 1.2M image-text instruction data from public datasets, which also includes multi-round conversations.\nFalcon2-11B VLM Evaluation Model MME GQA SQA POPE VQAv2 TextVQA MM-Bench SEED-IMG Average Falcon2-11B VLM 1589/343 64.5 74.9 88.4 82.1 66.7 72.0 72.3 74.4 LLaVA-1.6 (Vicuna-7B) 1519/332 64.2 70.1 86.5 81.8 64.9 67.4 70.2 72.1 LLaVA-1.6 (Vicuna-13B) 1575/326 65.4 73.6 86.2 82.8 67.1 70.0 71.9 73.8 LLaVA-1.6 (Mistral-7B) 1498/321 64.8 72.8 86.7 82.2 65.7 68.7 72.2 73.3 Using Falcon2-11B-FalconVLM from transformers import LlavaNextForConditionalGeneration, LlavaNextProcessor from PIL import Image import requests import torch processor = LlavaNextProcessor.from_pretrained(\"tiiuae/falcon-11B-vlm\") model = LlavaNextForConditionalGeneration.from_pretrained(\"tiiuae/falcon-11B-vlm\", torch_dtype=torch.bfloat16) url = \"https://merzougabirding.com/wp-content/uploads/2023/09/falcon-size.jpg\" falcon_image = Image.open(requests.get(url, stream=True).raw) prompt = \"User: \\nWhat's special about this bird's vision?\" inputs = processor(prompt, images=falcon_image, return_tensors=\"pt\", padding=True).to('cuda:0') model.to('cuda:0') output = model.generate(**inputs, max_new_tokens=256) prompt_length = inputs['input_ids'].shape[1] generated_captions = processor.decode(output[0], skip_special_tokens=True).strip() print(generated_captions) License information The Falcon 2 models are made available under the TII Falcon 2 License, a permissive Apache 2.0-based software license which includes an acceptable use policy that promotes the responsible use of AI. This license was crafted within the spirit of TII’s commitment to the open source community.\n","wordCount":"1384","inLanguage":"en","datePublished":"2024-05-24T12:00:00Z","dateModified":"2024-05-24T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/blog/falcon2-11b/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages</h1><div class=post-meta><span title='2024-05-24 12:00:00 +0000 UTC'>May 24, 2024</span>&nbsp;•&nbsp;7 min&nbsp;•&nbsp;1384 words&nbsp;•&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><div class=post-content><h1 id=falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-and-11-languages>Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages<a hidden class=anchor aria-hidden=true href=#falcon-2-an-11b-parameter-pretrained-language-model-and-vlm-trained-on-over-5000b-tokens-and-11-languages>#</a></h1><p><a name=the-falcon-models></a></p><h2 id=the-falcon-2-models>The Falcon 2 Models<a hidden class=anchor aria-hidden=true href=#the-falcon-2-models>#</a></h2><p><a href=www.tii.ae>TII</a> is launching a new generation of models, <a href=https://falconllm.tii.ae/>Falcon 2</a>, focused on providing the open-source community with a series of smaller models with enhanced performance and multi-modal support. Our goal is to enable cheaper inference and encourage the development of more downstream applications with improved usability.</p><p>The first generation of Falcon models, featuring <a href=https://huggingface.co/tiiuae/falcon-40b>Falcon-40B</a> and <a href=https://huggingface.co/tiiuae/falcon-180B>Falcon-180B</a>, made a significant contribution to the open-source community, promoting the release of advanced LLMs with permissive licenses. More detailed information on the previous generation of Falcon models can be found in the <a href=https://proceedings.neurips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html>RefinedWeb, Penedo et al., 2023</a> and <a href=https://arxiv.org/abs/2311.16867>The Falcon Series of Open Language Models, Almazrouei et al., 2023</a> papers, and the <a href=https://huggingface.co/blog/falcon>Falcon</a> and <a href=https://huggingface.co/blog/falcon-180b>Falcon-180B</a> blog posts.</p><p>The second generation of models is focused on increased usability and integrability, building a multi-modal ecosystem. We start this journey by releasing not only the base <a href=https://huggingface.co/tiiuae/falcon-11B>11B LLM</a>, but also the <a href=https://huggingface.co/tiiuae/Falcon-11B-vlm>11B VLM model</a> that incorporates image understanding capabilities. The vision-language model, or VLM, will allow users to engage in chats about visual content using text.</p><p>As with our previous work, the models offer support mainly in English but have good capabilities in ten other languages, including Spanish, French, and German.</p><h2 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h2><ul><li><a href=/blog/falcon2-11b/#the-falcon-models>The Falcon 2 Models</a></li><li>Falcon 2 11B LLM<ul><li><a href=/blog/falcon2-11b/#falcon2-11b-llm>11B LLM Training Details</a></li><li><a href=/blog/falcon2-11b/#falcon2-11b-evaluation>11B LLM Evaluation</a></li><li><a href=/blog/falcon2-11b/#using-falcon2-11b>11B LLM Using the Model</a></li></ul></li><li>Falcon 2 11B VLM<ul><li><a href=/blog/falcon2-11b/#falcon2-11b-vlm>11B VLM Training</a></li><li><a href=/blog/falcon2-11b/#falcon2-11b-vlm-evaluation>11B VLM Evaluation</a></li><li><a href=/blog/falcon2-11b/#using-falcon2-11b-falconvlm>11B VLM Using the Model</a></li></ul></li><li><a href=/blog/falcon2-11b/#license-information>Licensing information</a></li></ul><p><a name=falcon2-11b-llm></a></p><h2 id=falcon2-11b-llm>Falcon2-11B LLM<a hidden class=anchor aria-hidden=true href=#falcon2-11b-llm>#</a></h2><h3 id=training-data>Training Data<a hidden class=anchor aria-hidden=true href=#training-data>#</a></h3><p>Falcon2-11B was trained on over 5,000 GT (billion tokens) of RefinedWeb, a high-quality filtered and deduplicated web dataset, enhanced with curated corpora. It followed a four-stage training strategy. The first three stages were focused on increasing the context length, from 2048 to 4096 and finally to 8192 tokens. The last stage aimed to further enhance performance using only high-quality data.</p><p>Overall, the data sources included RefinedWeb-English, RefinedWeb-Europe (<em>cs</em>, <em>de</em>, <em>es</em>, <em>fr</em>, <em>it</em>, <em>nl</em>, <em>pl</em>, <em>pt</em>, <em>ro</em>, <em>sv</em>), high-quality technical data, code data, and conversational data extracted from public sources.</p><p>The training stages were as follows:</p><table><thead><tr><th>Stage</th><th>Context Length</th><th>GT</th></tr></thead><tbody><tr><td>Stage 1</td><td>2048</td><td>4500</td></tr><tr><td>Stage 2</td><td>4096</td><td>250</td></tr><tr><td>Stage 3</td><td>8192</td><td>250</td></tr><tr><td>Stage 4</td><td>8192</td><td>500</td></tr></tbody></table><p>The data was tokenized with <a href=https://huggingface.co/tiiuae/falcon-11B/blob/main/tokenizer.json>Falcon2-11B tokenizer</a>, the same tokenizer as for the previous Falcon models.</p><h3 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h3><p>The following table summarizes some of the crucial details about the model architecture:</p><table><thead><tr><th>Design choice</th><th>Value</th></tr></thead><tbody><tr><td>Number of Transformer Blocks</td><td>60</td></tr><tr><td>Number of Query Heads</td><td>32</td></tr><tr><td>Number of Key/Value Heads</td><td>8</td></tr><tr><td>Head Dimensions</td><td>128</td></tr><tr><td>Parallel Attention</td><td>yes</td></tr><tr><td>MLP Upscale Factor</td><td>4</td></tr></tbody></table><h3 id=training-procedure>Training Procedure<a hidden class=anchor aria-hidden=true href=#training-procedure>#</a></h3><p>Falcon2-11B was trained on 1024 A100 40GB GPUs for the majority of the training, using a 3D parallelism strategy (TP=8, PP=1, DP=128) combined with ZeRO and Flash-Attention 2.</p><h3 id=training-hyperparameters>Training Hyperparameters<a hidden class=anchor aria-hidden=true href=#training-hyperparameters>#</a></h3><table><thead><tr><th>Hyperparameter</th><th>Value</th></tr></thead><tbody><tr><td>Precision</td><td>bfloat16</td></tr><tr><td>Optimizer</td><td>AdamW</td></tr><tr><td>Max LR</td><td>3.7e-4</td></tr><tr><td>Min LR</td><td>1.89e-5</td></tr><tr><td>LR schedule</td><td>Cos decay (stage 1)</td></tr><tr><td>Context length</td><td>8192 (stages 3 and 4)</td></tr><tr><td>Weight decay</td><td>1e-1</td></tr><tr><td>Z-loss</td><td>1e-4</td></tr><tr><td>Batch size</td><td>Variable</td></tr></tbody></table><p><a name=falcon2-11b-evaluation></a></p><h2 id=falcon2-11b-evaluation>Falcon2-11B Evaluation<a hidden class=anchor aria-hidden=true href=#falcon2-11b-evaluation>#</a></h2><h3 id=english-performance>English performance<a hidden class=anchor aria-hidden=true href=#english-performance>#</a></h3><p>Performance on Open LLM Leaderboard tasks:</p><table><thead><tr><th>Checkpoint</th><th>GT</th><th>HellaSwag-10</th><th>Winogrande-5</th><th>ArcChallenge-25</th><th>TruthfulQA-0</th><th>MMLU-5</th><th>GSMK8k-5</th><th>Average</th></tr></thead><tbody><tr><td>Falcon2-11B</td><td>5500</td><td>82.91</td><td>78.30</td><td>59.73</td><td>52.56</td><td>58.37</td><td>53.83</td><td>64.28</td></tr><tr><td>Falcon-40B</td><td>1000</td><td>85.28</td><td>81.29</td><td>61.86</td><td>41.65</td><td>56.89</td><td>21.46</td><td>58.07</td></tr><tr><td>Falcon-7B</td><td>1500</td><td>78.13</td><td>72.38</td><td>47.87</td><td>34.26</td><td>27.79</td><td>4.62</td><td>44.17</td></tr><tr><td>Gemma-7B</td><td>6000</td><td>82.47</td><td>78.45</td><td>61.09</td><td>44.91</td><td>66.03</td><td>52.77</td><td>64.29</td></tr><tr><td>Llama3-8B</td><td>15000</td><td>82.09</td><td>77.35</td><td>59.47</td><td>43.90</td><td>66.69</td><td>44.79</td><td>62.38</td></tr><tr><td>Mistral-7B</td><td>N/A</td><td>83.31</td><td>78.37</td><td>59.98</td><td>42.15</td><td>64.16</td><td>37.83</td><td>60.97</td></tr></tbody></table><p>The Hugging Face Leaderboard team provided an official evaluation of our model on the <a href=https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard>Open LLM Leaderboard</a> tasks. The model performs better than models such as Llama3-8B (trained on three times more data) and Mistral-7B, and on par with Gemma-7b.</p><p>Zero shot performance:</p><table><thead><tr><th>Checkpoint</th><th>GT</th><th>HellaSwag</th><th>ArcEasy</th><th>Winogrande</th><th>ArcChallenge</th></tr></thead><tbody><tr><td>Falcon2-11B</td><td>5500</td><td>82.07</td><td>77.78</td><td>78.30</td><td>50.17</td></tr><tr><td>Falcon-40B</td><td>1000</td><td>82.82</td><td>81.86</td><td>76.4</td><td>54.69</td></tr><tr><td>Falcon-7B</td><td>1500</td><td>76.31</td><td>74.74</td><td>67.17</td><td>43.43</td></tr></tbody></table><p>The evaluation results show that the Falcon2-11B shows similar performance to Falcon-40B, at a four times smaller model size!</p><h3 id=multilingual-capabilities>Multilingual capabilities<a hidden class=anchor aria-hidden=true href=#multilingual-capabilities>#</a></h3><p>Using the <a href=https://huggingface.co/spaces/uonlp/open_multilingual_llm_leaderboard>Multilingual LLM leaderboard</a>, we compare the Falcon2-11B model to the Llama-7B and Bloom-7B. For reference, we also include Falcon-40B (that supports the same languages), Falcon-7B (that supports French) and Mistral-7B.</p><table><thead><tr><th>Model</th><th>Language ID</th><th>ArcChallenge-25</th><th>Hellaswag</th><th>MMLU 25</th><th>TQA</th><th>Average</th></tr></thead><tbody><tr><td>Falcon2-11B</td><td><em>de</em></td><td>43.7</td><td>67.96</td><td>38.3</td><td>47.53</td><td><strong>49.37</strong></td></tr><tr><td></td><td><em>es</em></td><td>46.2</td><td>73.63</td><td>37.9</td><td>46.43</td><td><strong>51.06</strong></td></tr><tr><td></td><td><em>fr</em></td><td>45.8</td><td>72.41</td><td>39.53</td><td>47.30</td><td><strong>51.27</strong></td></tr><tr><td></td><td><em>it</em></td><td>45.6</td><td>70.83</td><td>38.05</td><td>47.14</td><td><strong>50.42</strong></td></tr><tr><td></td><td><em>nl</em></td><td>41.7</td><td>69.05</td><td>38.29</td><td>48.81</td><td><strong>49.47</strong></td></tr><tr><td></td><td><em>ro</em></td><td>42.4</td><td>66.24</td><td>38.01</td><td>45.53</td><td><strong>48.04</strong></td></tr><tr><td>Falcon-40B</td><td><em>de</em></td><td>45.1</td><td>68.3</td><td>36.2</td><td>39.8</td><td>47.4</td></tr><tr><td></td><td><em>es</em></td><td>48.5</td><td>73.9</td><td>37.2</td><td>39.0</td><td>49.6</td></tr><tr><td></td><td><em>fr</em></td><td>47.6</td><td>72.9</td><td>37.3</td><td>38.5</td><td>49.1</td></tr><tr><td></td><td><em>it</em></td><td>46.3</td><td>70.2</td><td>36.4</td><td>40.7</td><td>48.4</td></tr><tr><td></td><td><em>nl</em></td><td>42.9</td><td>68.4</td><td>36.5</td><td>40.9</td><td>47.1</td></tr><tr><td></td><td><em>ro</em></td><td>43.2</td><td>66.0</td><td>35.7</td><td>39.8</td><td>46.2</td></tr><tr><td>Falcon-7B</td><td><em>fr</em></td><td>37.3</td><td>64.1</td><td>28.4</td><td>34.0</td><td>40.9</td></tr><tr><td>Mistral-7B</td><td><em>de</em></td><td>41.2</td><td>58.7</td><td>40.5</td><td>44.9</td><td>46.3</td></tr><tr><td></td><td><em>es</em></td><td>44.2</td><td>65.3</td><td>42.4</td><td>43.1</td><td>48.7</td></tr><tr><td></td><td><em>fr</em></td><td>44.9</td><td>64.4</td><td>41.9</td><td>43.0</td><td>48.6</td></tr><tr><td></td><td><em>it</em></td><td>43.2</td><td>60.9</td><td>39.7</td><td>43.1</td><td>46.7</td></tr><tr><td></td><td><em>nl</em></td><td>40.0</td><td>57.9</td><td>41.4</td><td>43.3</td><td>45.7</td></tr><tr><td></td><td><em>ro</em></td><td>40.7</td><td>53.6</td><td>39.3</td><td>43.6</td><td>44.3</td></tr><tr><td>Llama-7B</td><td><em>de</em></td><td>35.1</td><td>49.9</td><td>29.9</td><td>38.3</td><td>38.3</td></tr><tr><td></td><td><em>es</em></td><td>36.8</td><td>56.4</td><td>30.3</td><td>37.0</td><td>40.1</td></tr><tr><td></td><td><em>fr</em></td><td>37.3</td><td>55.7</td><td>30.5</td><td>39.9</td><td>40.9</td></tr><tr><td></td><td><em>it</em></td><td>35.8</td><td>52.0</td><td>29.9</td><td>39.6</td><td>39.3</td></tr><tr><td></td><td><em>nl</em></td><td>33.6</td><td>48.7</td><td>29.8</td><td>40.0</td><td>38.0</td></tr><tr><td></td><td><em>ro</em></td><td>32.4</td><td>44.9</td><td>29.7</td><td>37.0</td><td>36.0</td></tr><tr><td>Bloom-7B</td><td><em>de</em></td><td>26.3</td><td>32.4</td><td>28.1</td><td>43.7</td><td>32.6</td></tr><tr><td></td><td><em>es</em></td><td>38.1</td><td>56.7</td><td>28.9</td><td>40.4</td><td>41.0</td></tr><tr><td></td><td><em>fr</em></td><td>36.7</td><td>56.6</td><td>29.9</td><td>40.9</td><td>41.0</td></tr><tr><td></td><td><em>it</em></td><td>29.0</td><td>40.8</td><td>27.6</td><td>43.7</td><td>35.3</td></tr><tr><td></td><td><em>nl</em></td><td>23.1</td><td>31.7</td><td>27.5</td><td>42.7</td><td>31.3</td></tr><tr><td></td><td><em>ro</em></td><td>26.9</td><td>31.8</td><td>27.4</td><td>46.1</td><td>33.1</td></tr></tbody></table><p>In the spirit of the original Falcon models, the Falcon2-11B was trained not only on English data but also on ten other languages. Our multilingual evaluation results show that the model presents good capabilities in the six languages (<em>de</em>, <em>es</em>, <em>fr</em>, <em>it</em>, <em>nl</em>, <em>ro</em>) featured on the Multilingual LLM Leaderboard and actually shows higher performance than the Falcon-40B and several other multilingual models on all the cited languages.</p><p>We will soon release more extensive evaluation results for multilingual capabilities in the <a href=https://huggingface.co/tiiuae/falcon-11B>Falcon2-11B model card</a>!</p><h3 id=code-generation-capabilities>Code generation capabilities<a hidden class=anchor aria-hidden=true href=#code-generation-capabilities>#</a></h3><p>We check the model&rsquo;s performance on code generation against the <a href=https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard>BigCode Leaderboard</a> on the HumanEval benchmark for the Python language, obtaining pass@1 of 29.59%.</p><p><a name=using-falcon2-11b></a></p><h2 id=using-falcon2-11b>Using Falcon2-11B<a hidden class=anchor aria-hidden=true href=#using-falcon2-11b>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>transformers</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=s2>&#34;tiiuae/falcon-11B&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pipeline</span> <span class=o>=</span> <span class=n>transformers</span><span class=o>.</span><span class=n>pipeline</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;text-generation&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>And then, you&rsquo;d run text generation using code like the following:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sequences</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=s2>&#34;Can you explain the concept of Quantum Computing?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_length</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_k</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>num_return_sequences</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>eos_token_id</span><span class=o>=</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>seq</span> <span class=ow>in</span> <span class=n>sequences</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Result: </span><span class=si>{</span><span class=n>seq</span><span class=p>[</span><span class=s1>&#39;generated_text&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><a name=falcon2-11b-vlm></a></p><h2 id=falcon2-11b-vlm>Falcon2-11B VLM<a hidden class=anchor aria-hidden=true href=#falcon2-11b-vlm>#</a></h2><p><a href=https://huggingface.co/tiiuae/Falcon-11B-vlm>Falcon2-11B VLM</a> is a vision-language model (VLM) built on top of the LLM, that additionally handles image inputs and is capable of answering queries about the images. To achieve this, we integrate the pretrained CLIP ViT-L/14 vision encoder with our Falcon2-11B chat-finetuned model, and train with image-text data.</p><p>To enhance the VLM&rsquo;s perception of fine-grained details w.r.t small objects in images, we employ a dynamic encoding mechanism at high-resolution for image inputs, similar to <a href=https://llava-vl.github.io/blog/2024-01-30-llava-next/>LLaVA-Next</a>.</p><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p>The training is done in two stages: pretraining and finetuning. In both stages, the visual encoder weights are kept frozen. In the pretraining stage, the LLM is kept frozen, and only the multimodal projector is trained on 558K image-caption pairs.
This enables the multimodal projector to learn a mapping from visual to text embedding space. During finetuning, both the projector and LLM weights are trained on a corpus of 1.2M image-text instruction data from public datasets, which also includes multi-round conversations.</p><p><a name=falcon2-11b-vlm-evaluation></a></p><h2 id=falcon2-11b-vlm-evaluation>Falcon2-11B VLM Evaluation<a hidden class=anchor aria-hidden=true href=#falcon2-11b-vlm-evaluation>#</a></h2><table><thead><tr><th>Model</th><th>MME</th><th>GQA</th><th>SQA</th><th>POPE</th><th>VQAv2</th><th>TextVQA</th><th>MM-Bench</th><th>SEED-IMG</th><th>Average</th></tr></thead><tbody><tr><td>Falcon2-11B VLM</td><td><strong>1589/343</strong></td><td>64.5</td><td><strong>74.9</strong></td><td><strong>88.4</strong></td><td>82.1</td><td>66.7</td><td><strong>72.0</strong></td><td><strong>72.3</strong></td><td><strong>74.4</strong></td></tr><tr><td>LLaVA-1.6 (Vicuna-7B)</td><td>1519/332</td><td>64.2</td><td>70.1</td><td>86.5</td><td>81.8</td><td>64.9</td><td>67.4</td><td>70.2</td><td>72.1</td></tr><tr><td>LLaVA-1.6 (Vicuna-13B)</td><td>1575/326</td><td><strong>65.4</strong></td><td>73.6</td><td>86.2</td><td><strong>82.8</strong></td><td><strong>67.1</strong></td><td>70.0</td><td>71.9</td><td>73.8</td></tr><tr><td>LLaVA-1.6 (Mistral-7B)</td><td>1498/321</td><td>64.8</td><td>72.8</td><td>86.7</td><td>82.2</td><td>65.7</td><td>68.7</td><td>72.2</td><td>73.3</td></tr></tbody></table><p><a name=using-falcon2-11b-falconvlm></a></p><h2 id=using-falcon2-11b-falconvlm>Using Falcon2-11B-FalconVLM<a hidden class=anchor aria-hidden=true href=#using-falcon2-11b-falconvlm>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>LlavaNextForConditionalGeneration</span><span class=p>,</span> <span class=n>LlavaNextProcessor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>PIL</span> <span class=kn>import</span> <span class=n>Image</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>LlavaNextProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;tiiuae/falcon-11B-vlm&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>LlavaNextForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;tiiuae/falcon-11B-vlm&#34;</span><span class=p>,</span> <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;https://merzougabirding.com/wp-content/uploads/2023/09/falcon-size.jpg&#34;</span>
</span></span><span class=line><span class=cl><span class=n>falcon_image</span> <span class=o>=</span> <span class=n>Image</span><span class=o>.</span><span class=n>open</span><span class=p>(</span><span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>stream</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>raw</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;User: &lt;image&gt;</span><span class=se>\n</span><span class=s2>What&#39;s special about this bird&#39;s vision?&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>images</span><span class=o>=</span><span class=n>falcon_image</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s1>&#39;cuda:0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s1>&#39;cuda:0&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=o>**</span><span class=n>inputs</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt_length</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=s1>&#39;input_ids&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>generated_captions</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>output</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>strip</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>generated_captions</span><span class=p>)</span>
</span></span></code></pre></div><p align=center><img src=https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/179_falcon2-11b/falcon_example_tiny.png></p><p><a name=license-information></a></p><h2 id=license-information>License information<a hidden class=anchor aria-hidden=true href=#license-information>#</a></h2><p>The Falcon 2 models are made available under the <a href=https://falconllm-staging.tii.ae/falcon-2-terms-and-conditions.html>TII Falcon 2 License</a>, a permissive Apache 2.0-based software license which includes an <a href=https://falconllm-staging.tii.ae/falcon-2-acceptable-use-policy.html>acceptable use policy</a> that promotes the responsible use of AI. This license was crafted within the spirit of TII&rsquo;s commitment to the open source community.</p><div class=post-contributors><div class=contributors-section><h4>Core Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Falcon LLM team" class=contributor-image><p class=contributor-name>Falcon LLM team</p></div></div></div><br></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>