<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><script>!function(e,t){if(!e.rdt){var s,o,n=e.rdt=function(){n.sendEvent?n.sendEvent.apply(n,arguments):n.callQueue.push(arguments)};n.callQueue=[],s=t.createElement("script"),s.src="https://www.redditstatic.com/ads/pixel.js",s.async=!0,o=t.getElementsByTagName("script")[0],o.parentNode.insertBefore(s,o)}}(window,document),rdt("init","a2_f0we4sxffrnm"),rdt("track","PageVisit")</script><script async src="https://www.googletagmanager.com/gtag/js?id=AW-16575356988"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","AW-16575356988")</script><title>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance | Falcon</title>
<meta name=keywords content><meta name=description content="
Falcon CHAT
Hugging Face
Github
DEMO
DISCORD

  
    
  

Introduction
Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers."><meta name=author content="Falcon Team"><link rel=canonical href=https://falcon-lm.github.io/blog/falcon-h1/><link crossorigin=anonymous href=/assets/css/stylesheet.8b9fa41d05770f933657a6befdf3e59416a8572dcdccb2def3ee65a2976037d3.css integrity="sha256-i5+kHQV3D5M2V6a+/fPllBaoVy3NzLLe8+5lopdgN9M=" rel="preload stylesheet" as=style><link rel=icon href=https://falcon-lm.github.io/img/favicon.png><link rel=apple-touch-icon href=https://falcon-lm.github.io/img/favicon.png><link rel=manifest href=https://falcon-lm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://falcon-lm.github.io/blog/falcon-h1/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.c0c4976150cc57e4e574f010d054d68896d28645b524650723d1cbb26891c0a3.js integrity="sha256-wMSXYVDMV+TldPAQ0FTWiJbShkW1JGUHI9HLsmiRwKM="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-5PVYBMYHS6"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-5PVYBMYHS6")}</script><meta property="og:title" content="Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance"><meta property="og:description" content="
Falcon CHAT
Hugging Face
Github
DEMO
DISCORD

  
    
  

Introduction
Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers."><meta property="og:type" content="article"><meta property="og:url" content="https://falcon-lm.github.io/blog/falcon-h1/"><meta property="og:image" content="https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo.png"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-20T12:00:00+00:00"><meta property="article:modified_time" content="2025-05-20T12:00:00+00:00"><meta property="og:site_name" content="Falcon"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo.png"><meta name=twitter:title content="Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance"><meta name=twitter:description content="
Falcon CHAT
Hugging Face
Github
DEMO
DISCORD

  
    
  

Introduction
Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://falcon-lm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance","item":"https://falcon-lm.github.io/blog/falcon-h1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance","name":"Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance","description":"\rFalcon CHAT Hugging Face Github DEMO DISCORD\nIntroduction Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers.\n","keywords":[],"articleBody":"\rFalcon CHAT Hugging Face Github DEMO DISCORD\nIntroduction Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers.\nIn this release, we feature six open-weight models: 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B, along with their instruct versions. All our open-source models are with a permissive license based on Apache 2.0.\nModel Size\rBase Model\rInstruct Model\r0.5B\r🤗 Falcon-H1-0.5B-Base\r🤗 Falcon-H1-0.5B-Instruct\r1.5B\r🤗 Falcon-H1-1.5B-Base\r🤗 Falcon-H1-1.5B-Instruct\r1.5B-Deep\r🤗 Falcon-H1-1.5B-Deep-Base\r🤗 Falcon-H1-1.5B-Deep-Instruct\r3B\r🤗 Falcon-H1-3B-Base\r🤗 Falcon-H1-3B-Instruct\r7B\r🤗 Falcon-H1-7B-Base\r🤗 Falcon-H1-7B-Instruct\r34B\r🤗 Falcon-H1-34B-Base\r🤗 Falcon-H1-34B-Instruct\rKey Features of Falcon-H1 Hybrid Architecture (Attention + SSM): We combine attention and Mamba-2 heads in parallel within our hybrid mixer block. Importantly, the amount of attention and mamba heads can be adjusted independently, allowing for an optimal attention/SSM ratio. This hybrid design enables faster inference, lower memory usage, and strong generalization across tasks.\nWide Range of Model Sizes: Available in six scales—0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B—with both base and instruction-tuned variants, suitable for everything from edge devices to large-scale deployments.\nMultilingual by Design: Supports 18 languages natively, including Arabic (ar), Czech (cs), German (de), English (en), Spanish (es), French (fr), Hindi (hi), Italian (it), Japanese (ja), Korean (ko), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Swedish (sv), Urdu (ur), and Chinese (zh) — with scalability to 100+ languages, thanks to our multilingual tokenizer trained on diverse language datasets.\nCompact Models, Big Performance: Falcon-H1-0.5B delivers performance on par with typical 7B models from 2024, while Falcon-H1-1.5B-Deep rivals many of the current leading 7B–10B models. Each Falcon-H1 model is designed to match or exceed the performance of models at least twice its size, making them ideal for low-resource and edge deployments without compromising on capability.\n256K Context Support: Falcon-H1 models support up to 256K context length, enabling applications in long-document processing, multi-turn dialogue, and long-range reasoning.\nExceptional STEM capabilities: Falcon-H1 models deliver strong performance in math and science domains thanks to the focus on high-quality STEM data during training.\nRobust Training Strategy: Uses a high-efficiency data strategy and customized Maximal Update Parametrization (μP) to ensure smooth and scalable training across model sizes.\nMain Principles behind building Falcon-H1 When embarking on the Falcon-H1 series development, we chose to fundamentally rethink the training approach. While the field of LLM development has converged on many established practices that reliably produce strong models, these conventions were primarily validated on classical transformer architectures. The shift from pure attention mechanisms to a hybrid attention-SSM design represents a significant architectural change, making it uncertain whether these standard practices would remain optimal.\nGiven this uncertainty, we conducted an extensive experimentation phase, systematically revisiting nearly every aspect of model design and training methodology before launching our final training runs. While we will provide comprehensive details in our upcoming technical report, we’d like to share the key insights that shaped the Falcon-H1 models.\nArchitecture The hybrid attention-SSM models have a larger configuration space of all the parameters that define the model architecture. Our goal was to probe each of these configuration parameters to check its impact on model performance and efficiency. As a result, we reveal regions of model configuration space with an increased performance at a mild efficiency cost. We can roughly divide the hybrid model configuration space in the following 4 blocks:\nSSM specific parameters. Our SSM layer is based on mamba-2 architecture that organized into groups of heads, similar to attention in modern transformer models. We have found that deviation of the number of groups or heads from the values typically used in the literature doesn’t improve performance but could degrade efficiency. In contrast, using a larger memory size, an SSM-specific variable that does not have an attention analog, gives a boost in performance with only a mild efficiency cost. Attention specific parameters. We employ a standard full attention layer. However, we have found that using an extremely large-scale parameter in rotary positional embeddings (RoPE) significantly improves the model performance. Our hypothesis is that, compared to pure transformers, in hybrid models such large values become possible since some positional information is natively processed by the SSM part of the model. Combining mamba and attention. There are many ways to combine attention and SSM in one model, with a sequential or parallel approach being the main design choice. We have converged on the parallel approach demonstrated in the diagram above. The key feature of our parallel hybrid design is the possibility of adjusting the ratio of attention and SSM heads, where we have found that a relatively small fraction of attention is sufficient for good performance. General parameters. In our experiments we observed the increased model depth to have the largest impact on the performance, though at efficiency cost. This makes choosing the model’s depth a tough tradeoff that depends on specific use cases. Our Falcon-H1-1.5B-deep is motivated by this tradeoff and targets usage scenarios requiring maximal performance at a small parameter count. Data strategy Capabilities of language models are known to come mainly from the training data, and that stays true for Falcon-H1 series. Besides the raw data prepared for the model, it is crucial how and when this data is shown during training. One such data strategy is commonly called curriculum learning, where simpler data is shown at the beginning of the training while the samples requiring more advanced reasoning are left for the end. Surprisingly, a completely opposite strategy worked best for us. Giving even the most complicated data, an advanced math problem or a long context sample, from the beginning of the training seems to give the model more time to learn features essential for handling the respective complex tasks.\nAnother key aspect is the scarcity of high-quality data. A common concern when training large models is brute force memorization of the data as opposed to its real understanding. To minimize the risk of such memorization, a common practice is not reusing data samples during training, or doing it at most a few times for the highest quality samples. A by-product of this strategy is data mixture being dominated by web samples that have disproportionally large volume compared to high-quality sources. We have found that the memorization effect might be a bit overestimated, and carefully estimating model’s memorization window allows to reuse high-quality samples more often without any harm to model’s generalization ability.\nCustomized maximal update parametrization (μP) Classical μP is a technique heavily rooted in theory of neural networks but with a clear practical application: if one finds optimal training hyperparameters at a single base model size, it can be effortlessly transferred to other, typically bigger, model sizes using Mup scaling rules. We employed Mup hyperparameter transfer for the whole Falcon-H1 series, greatly reducing experimentation time and making it possible to train 6 models in parallel.\nOn top of that, we made the next step into inner workings behind μP to further boost the model performance. In a nutshell, each component of the model “wants” to train at its own intensity, and that intensity depends on the size of the component. μP scaling rules take into account this dependence through so-called ``μP multipliers’’ to enable optimal hyperparameter transfer. However, classical μP uses trivial multipliers of 1 at the base model size, which corresponds to a nasusmption that intensity of all components are already optimal at the base size. We discard this assumption and tune the multipliers at the base model size. Specifically, we have divided model parameters into 35 fine-grained groups and performed a joint optimization of the respective 35 multipliers.\nTraining dynamics One of our first steps in working on Falcon-H1 series was treating and removing spikes that are known to be a serious issue for SSM-based models. The solution that has worked the best for us is placing dampening μP multipliers at a certain location of the SSM block. In addition to the smooth final model training, the removal of spikes is essential to get clean signals in the subsequent experiments.\nWe have observed that many aspects of the training dynamics are linked together under a common theme of noise interpretation and control. This includes learning rate and batch size schedules, scaling of the learning rate with batch size, and the behavior of parameter norms. In particular, we have found the parameter norms to be mostly determined by the training hyperparameters rather than the model fitting the data. To take this into account, we have included weight decay, a hyperparameter that primarily controls parameter norms, into both the training schedule and μP multipliers.\nPerformance Instruct Models The current Falcon-H1 models were trained without reasoning-specific fine-tuning, yet they already demonstrate strong general instruction-following capabilities. To highlight their performance, we present a detailed comparison of Falcon-H1-34B-Instruct against other top-performing Transformer models of similar or larger scales, including: Qwen3-32B (non-thinking mode), Qwen2.5-72B, Qwen2.5-32B, Gemma3-27B, Llama-4-Scout-17B-16E (109B) and LLaMA3.3-70B. For full evaluation settings and methodology, please refer to the Falcon-H1 GitHub page.\nOne of the standout features of the Falcon-H1 series is the strong performance of its compact models. Below, we compare 1.5B-scale instruct models. Falcon-H1-1.5B-Deep-Instruct clearly outperforms leading models in its class, such as Qwen3-1.7B-Instruct. Even more notably, it performs on par with—or better than many 7B models, including Falcon3-7B-Instruct and Qwen2.5-7B-Instruct.\n🔎 Note: Falcon-H1-1.5B-Deep and Falcon-H1-1.5B were trained using identical settings; the only difference lies in their architectural depth and width.\nMultilingual Benchmarks To give a picture of Falcon-H1 performance across languages, we provide average between Hellaswag and MMLU scores for 30B scale models and for a set of selected languages, including Arabic, German, Spanish, French, Hindi, Italian, Dutch, Portuguese, Romanian, Russian, and Swedish. It also demonstrates on-par performance in the other supported languages.\nLong Context Benchmarks One of the standout features of Falcon-H1 is its ability to handle long-context inputs, an area where State Space Models (SSMs) offer significant advantages in terms of memory efficiency and computational cost.\nTo demonstrate these capabilities, we evaluate Falcon-H1-34B-Instruct against Qwen2.5-72B-Instruct across a set of long-context benchmarks. We focus on three core task categories drawn from the Helmet benchmark suite - Retrieval-Augmented Generation (RAG): Natural Questions, TriviaQA, PopQA, HotpotQA; Recall tasks: JSON KV, RULER MK Needle, RULER MK UUID, RULER MV; Long Document QA tasks: ∞BENCH QA, ∞BENCH MC. These evaluations highlight Falcon-H1’s strength in scaling to longer sequences while maintaining high performance and efficiency.\nIn addition, we conducted a comprehensive evaluation of the Falcon-H1 series alongside leading Transformer-based models across 23 benchmarks, covering multiple domains and model scales. You can explore the interactive results below—simply select the benchmarks most relevant to your use case to view the corresponding aggregated performance scores.\nBase Models We provide a detailed comparison of Falcon-H1-34B-Base with other leading base models at the same or larger scale, including Qwen2.5-72B, Qwen2.5-32B, Llama-4-Scout-17B-16E (109B) and Gemma3-27B.\n🔎 Note: Qwen3-32B does not currently offer a base model checkpoint.\nBelow, we compare 1.5B-scale base models. Falcon-H1-1.5B-Deep-Base clearly outperforms leading models in its class, such as Qwen3-1.7B-Base. Notably, it performs on par with Falcon3-7B, and even exceeds it on math and reasoning tasks, making it an excellent foundation for building small-scale reasoning-focused models.\nFor the base models, we also provide an interactive plot showcasing their performance across 14 benchmarks, spanning multiple domains and various model scales.\nModel Efficiency We compare input (prefill) and output (generation) throughput between Falcon-H1 and Qwen2.5-32B in the plots below. While Transformers are slightly faster at shorter context lengths, our hybrid model becomes significantly more efficient as the context grows—achieving up to 4× speedup in input throughput and 8× in output throughput at longer sequence lengths. Benchmarks were run using our Falcon-H1 vLLM implementation and the official vLLM implementation of Qwen2.5-32B.\nThis performance gain highlights the scalability of the Falcon-H1 architecture. We attribute the throughput gap at small context lengths to the more mature optimizations of attention mechanisms, compared to current State Space Models (SSMs) implementations, in current inference pipelines.\n⚙️ We invite the community to contribute to further optimizing SSM implementations — a promising direction for advancing the next generation of efficient LLMs.\nPrompt Examples Below are a few example outputs generated by Falcon-H1-34B-Instruct. Search Language All Languages Source All Sources Category All Categories Loading examples... No examples match your filters. Try changing your selection. Debug Information × Open Source Commitment In line with our mission to foster AI accessibility and collaboration, Falcon-H1 is released under the Falcon LLM license. We hope the AI community finds these models valuable for research, application development, and further experimentation. Falcon-H1 is a continuation of our efforts to create more capable and efficient foundation models. We welcome feedback and collaboration from the community as we continue to refine and advance the capabilities of these models.\nUseful Links Access to our models (including GPTQ and GGUF) through the Falcon-H1 HuggingFace collection. Check out our Github page for the latest technical updates on Falcon-H1 models. Feel free to join our discord server if you have any questions or to interact with our researchers and developers. Check out the Falcon-LLM License link for more details about the license. Citation @misc{tiifalconh1,\rtitle = {Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance},\rurl = {https://falcon-lm.github.io/blog/falcon-h1},\rauthor = {Falcon-LLM Team},\rmonth = {May},\ryear = {2025}\r} ","wordCount":"2277","inLanguage":"en","image":"https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo.png","datePublished":"2025-05-20T12:00:00Z","dateModified":"2025-05-20T12:00:00Z","author":{"@type":"Person","name":"Falcon Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://falcon-lm.github.io/blog/falcon-h1/"},"publisher":{"@type":"Organization","name":"Falcon","logo":{"@type":"ImageObject","url":"https://falcon-lm.github.io/img/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Falcon (Alt + H)"><img src=https://falcon-lm.github.io/img/logo.svg alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.falconllm.tii.ae/ title="Try Falcon Chat"><span>Try Falcon Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:url(falcon-h1-logo.png)50%/cover no-repeat fixed"></div><div class=hero-gradient></div><div class=hero-blur></div><div class="hero text-light"><h1 class=post-title>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance</h1><div class=post-meta><span title='2025-05-20 12:00:00 +0000 UTC'>May 20, 2025</span>&nbsp;•&nbsp;11 min&nbsp;•&nbsp;2277 words&nbsp;•&nbsp;Falcon Team</div></div></div><main class=main><article class=post-single><figure class=entry-cover><a href=/blog/falcon-h1/falcon-h1-logo.png target=_blank rel="noopener noreferrer"><img loading=lazy srcset="https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo_hu_6531e8baca40b34e.png 360w ,https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo_hu_160dccdfb8eba71d.png 480w ,https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo_hu_a2b10d8aec0fd21d.png 720w ,https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo_hu_b7e2e30fb7cbb79f.png 1080w ,https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo_hu_86ace78ddb043c4c.png 1500w ,https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo.png 1552w" sizes="(min-width: 768px) 720px, 100vw" src=https://falcon-lm.github.io/blog/falcon-h1/falcon-h1-logo.png alt width=1552 height=658></a></figure><div class=post-content><style>html,body{background:#f0f2f9}</style><p><a href=https://chat.falconllm.tii.ae class="btn external" target=_blank>Falcon CHAT</a>
<a href=https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df class="btn external" target=_blank>Hugging Face</a>
<a href=https://github.com/tiiuae/falcon-h1 class="btn external" target=_blank>Github</a>
<a href=https://huggingface.co/spaces/tiiuae/Falcon-H1-Playground class="btn external" target=_blank>DEMO</a>
<a href=https://discord.gg/vfw6k2G3 class="btn external" target=_blank>DISCORD</a></p><div style=display:flex;justify-content:center><div style=position:relative;width:100%;max-width:700px;aspect-ratio:700/600><iframe src=/plots_h1/falcon_h1_performance_scatter_2.html style=border:none;position:absolute;width:100%;height:100%;left:0;top:0 allowfullscreen></iframe></div></div><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Today, we are proud to introduce the Falcon-H1 series, a collection of six open-source models ranging from 0.5B to 34B parameters, each available in both base and instruction-tuned variants. At the core of these models lies a hybrid architecture that combines the strengths of the classical Transformer-based attention mechanism with the State Space Model (SSM), known for its superior long-context memory and computational efficiency. This architectural innovation is further enhanced by fundamental advancements in training dynamics and data utilization, enabling Falcon-H1 models to deliver uncompromised performance that rivals the top Transformer-based models across all covered size tiers.</p><p>In this release, we feature six open-weight models: 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B, along with their instruct versions. All our open-source models are with a permissive license based on Apache 2.0.</p><div style=font-size:90%;justify-content:center><table style=width:100%;border-collapse:collapse><thead><tr><th style=text-align:center;padding:8px>Model Size</th><th style=text-align:center;padding:8px>Base Model</th><th style=text-align:center;padding:8px>Instruct Model</th></tr></thead><tbody><tr><td><strong>0.5B</strong></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-0.5b-base target=_blank>🤗 Falcon-H1-0.5B-Base</a></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-0.5b-instruct target=_blank>🤗 Falcon-H1-0.5B-Instruct</a></td></tr><tr><td><strong>1.5B</strong></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-1.5b-base target=_blank>🤗 Falcon-H1-1.5B-Base</a></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-1.5b-instruct target=_blank>🤗 Falcon-H1-1.5B-Instruct</a></td></tr><tr><td><strong>1.5B-Deep</strong></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-1.5b-deep-base target=_blank>🤗 Falcon-H1-1.5B-Deep-Base</a></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-1.5b-deep-instruct target=_blank>🤗 Falcon-H1-1.5B-Deep-Instruct</a></td></tr><tr><td><strong>3B</strong></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-3b-base target=_blank>🤗 Falcon-H1-3B-Base</a></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-3b-instruct target=_blank>🤗 Falcon-H1-3B-Instruct</a></td></tr><tr><td><strong>7B</strong></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-7b-base target=_blank>🤗 Falcon-H1-7B-Base</a></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-7b-instruct target=_blank>🤗 Falcon-H1-7B-Instruct</a></td></tr><tr><td><strong>34B</strong></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-34b-base target=_blank>🤗 Falcon-H1-34B-Base</a></td><td><a href=https://huggingface.co/tiiuae/falcon-h1-34b-instruct target=_blank>🤗 Falcon-H1-34B-Instruct</a></td></tr></tbody></table></div><h1 id=key-features-of-falcon-h1>Key Features of Falcon-H1<a hidden class=anchor aria-hidden=true href=#key-features-of-falcon-h1>#</a></h1><ul><li><p><strong>Hybrid Architecture (Attention + SSM):</strong>
We combine attention and Mamba-2 heads in parallel within our hybrid mixer block. Importantly, the amount of attention and mamba heads can be adjusted independently, allowing for an optimal attention/SSM ratio. This hybrid design enables <strong>faster inference</strong>, <strong>lower memory usage</strong>, and <strong>strong generalization</strong> across tasks.</p></li><li><p><strong>Wide Range of Model Sizes:</strong>
Available in <strong>six scales</strong>—0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B—with both <strong>base</strong> and <strong>instruction-tuned</strong> variants, suitable for everything from edge devices to large-scale deployments.</p></li><li><p><strong>Multilingual by Design:</strong>
Supports <strong>18 languages</strong> natively, including Arabic (ar), Czech (cs), German (de), English (en), Spanish (es), French (fr), Hindi (hi), Italian (it), Japanese (ja), Korean (ko), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Swedish (sv), Urdu (ur), and Chinese (zh) — with scalability to <strong>100+ languages</strong>, thanks to our multilingual tokenizer trained on diverse language datasets.</p></li><li><p><strong>Compact Models, Big Performance:</strong>
<strong>Falcon-H1-0.5B</strong> delivers performance on par with typical <strong>7B models from 2024</strong>, while <strong>Falcon-H1-1.5B-Deep</strong> rivals many of the current leading <strong>7B–10B models</strong>. Each Falcon-H1 model is designed to <strong>match or exceed the performance of models at least twice its size</strong>, making them ideal for <strong>low-resource and edge deployments</strong> without compromising on capability.</p></li><li><p><strong>256K Context Support:</strong>
Falcon-H1 models support up to <strong>256K context length</strong>, enabling applications in long-document processing, multi-turn dialogue, and long-range reasoning.</p></li><li><p><strong>Exceptional STEM capabilities:</strong>
Falcon-H1 models deliver strong performance in math and science domains thanks to the focus on high-quality STEM data during training.</p></li><li><p><strong>Robust Training Strategy:</strong>
Uses a <strong>high-efficiency data strategy</strong> and customized <strong>Maximal Update Parametrization (μP)</strong> to ensure smooth and scalable training across model sizes.</p></li></ul><p align=center><img src=/plots_h1/h1_architecture.png alt="Falcon-H1 model architectures"></p><h1 id=main-principles-behind-building-falcon-h1>Main Principles behind building Falcon-H1<a hidden class=anchor aria-hidden=true href=#main-principles-behind-building-falcon-h1>#</a></h1><p>When embarking on the Falcon-H1 series development, we chose to fundamentally rethink the training approach. While the field of LLM development has converged on many established practices that reliably produce strong models, these conventions were primarily validated on classical transformer architectures. The shift from pure attention mechanisms to a hybrid attention-SSM design represents a significant architectural change, making it uncertain whether these standard practices would remain optimal.</p><p>Given this uncertainty, we conducted an extensive experimentation phase, systematically revisiting nearly every aspect of model design and training methodology before launching our final training runs. While we will provide comprehensive details in our upcoming technical report, we&rsquo;d like to share the key insights that shaped the Falcon-H1 models.</p><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p align=center><img src=/plots_h1/h1_hybrid_design.png alt="Falcon-H1 hybrid model desig"></p><p>The hybrid attention-SSM models have a larger configuration space of all the parameters that define the model architecture. Our goal was to probe each of these configuration parameters to check its impact on model performance and efficiency. As a result, we reveal regions of model configuration space with an increased performance at a mild efficiency cost. We can roughly divide the hybrid model configuration space in the following 4 blocks:</p><ul><li><strong>SSM specific parameters</strong>. Our SSM layer is based on mamba-2 architecture that organized into groups of heads, similar to attention in modern transformer models. We have found that deviation of the number of groups or heads from the values typically used in the literature doesn&rsquo;t improve performance but could degrade efficiency. In contrast, using a larger memory size, an SSM-specific variable that does not have an attention analog, gives a boost in performance with only a mild efficiency cost.</li><li><strong>Attention specific parameters</strong>. We employ a standard full attention layer. However, we have found that using an extremely large-scale parameter in rotary positional embeddings (RoPE) significantly improves the model performance. Our hypothesis is that, compared to pure transformers, in hybrid models such large values become possible since some positional information is natively processed by the SSM part of the model.</li><li><strong>Combining mamba and attention</strong>. There are many ways to combine attention and SSM in one model, with a sequential or parallel approach being the main design choice. We have converged on the parallel approach demonstrated in the diagram above. The key feature of our parallel hybrid design is the possibility of adjusting the ratio of attention and SSM heads, where we have found that a relatively small fraction of attention is sufficient for good performance.</li><li><strong>General parameters</strong>. In our experiments we observed the increased model depth to have the largest impact on the performance, though at efficiency cost. This makes choosing the model&rsquo;s depth a tough tradeoff that depends on specific use cases. Our Falcon-H1-1.5B-deep is motivated by this tradeoff and targets usage scenarios requiring maximal performance at a small parameter count.</li></ul><h2 id=data-strategy>Data strategy<a hidden class=anchor aria-hidden=true href=#data-strategy>#</a></h2><p>Capabilities of language models are known to come mainly from the training data, and that stays true for Falcon-H1 series. Besides the raw data prepared for the model, it is crucial how and when this data is shown during training. One such data strategy is commonly called <em>curriculum learning</em>, where simpler data is shown at the beginning of the training while the samples requiring more advanced reasoning are left for the end. Surprisingly, a completely opposite strategy worked best for us. Giving even the most complicated data, an advanced math problem or a long context sample, from the beginning of the training seems to give the model more time to learn features essential for handling the respective complex tasks.</p><p>Another key aspect is the scarcity of high-quality data. A common concern when training large models is brute force memorization of the data as opposed to its real understanding. To minimize the risk of such memorization, a common practice is not reusing data samples during training, or doing it at most a few times for the highest quality samples. A by-product of this strategy is data mixture being dominated by web samples that have disproportionally large volume compared to high-quality sources. We have found that the memorization effect might be a bit overestimated, and carefully estimating model&rsquo;s <em>memorization window</em> allows to reuse high-quality samples more often without any harm to model&rsquo;s generalization ability.</p><h2 id=customized-maximal-update-parametrization-μp>Customized maximal update parametrization (μP)<a hidden class=anchor aria-hidden=true href=#customized-maximal-update-parametrization-μp>#</a></h2><p>Classical μP is a technique heavily rooted in theory of neural networks but with a clear practical application: if one finds optimal training hyperparameters at a single base model size, it can be effortlessly transferred to other, typically bigger, model sizes using Mup scaling rules. We employed Mup hyperparameter transfer for the whole Falcon-H1 series, greatly reducing experimentation time and making it possible to train 6 models in parallel.</p><p>On top of that, we made the next step into inner workings behind μP to further boost the model performance. In a nutshell, each component of the model “wants” to train at its own intensity, and that intensity depends on the size of the component. μP scaling rules take into account this dependence through so-called ``μP multipliers&rsquo;&rsquo; to enable optimal hyperparameter transfer. However, classical μP uses trivial multipliers of 1 at the base model size, which corresponds to a nasusmption that intensity of all components are already optimal at the base size. We discard this assumption and tune the multipliers at the base model size. Specifically, we have divided model parameters into 35 fine-grained groups and performed a joint optimization of the respective 35 multipliers.</p><h2 id=training-dynamics>Training dynamics<a hidden class=anchor aria-hidden=true href=#training-dynamics>#</a></h2><p>One of our first steps in working on Falcon-H1 series was treating and removing spikes that are known to be a serious issue for SSM-based models. The solution that has worked the best for us is placing dampening μP multipliers at a certain location of the SSM block. In addition to the smooth final model training, the removal of spikes is essential to get clean signals in the subsequent experiments.</p><p>We have observed that many aspects of the training dynamics are linked together under a common theme of noise interpretation and control. This includes learning rate and batch size schedules, scaling of the learning rate with batch size, and the behavior of parameter norms. In particular, we have found the parameter norms to be mostly determined by the training hyperparameters rather than the model fitting the data. To take this into account, we have included weight decay, a hyperparameter that primarily controls parameter norms, into both the training schedule and μP multipliers.</p><h1 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h1><h2 id=instruct-models>Instruct Models<a hidden class=anchor aria-hidden=true href=#instruct-models>#</a></h2><p>The current Falcon-H1 models were trained without reasoning-specific fine-tuning, yet they already demonstrate strong general instruction-following capabilities. To highlight their performance, we present a detailed comparison of Falcon-H1-34B-Instruct against other top-performing Transformer models of similar or larger scales, including: Qwen3-32B (non-thinking mode), Qwen2.5-72B, Qwen2.5-32B, Gemma3-27B, Llama-4-Scout-17B-16E (109B) and LLaMA3.3-70B. For full evaluation settings and methodology, please refer to the <a href=https://github.com/tiiuae/falcon-H1>Falcon-H1 GitHub page</a>.</p><p align=center><img src=/plots_h1/h1_34B_comparison.png alt="Falcon-H1-34B model comparison"></p><p>One of the standout features of the Falcon-H1 series is the strong performance of its compact models. Below, we compare 1.5B-scale instruct models. Falcon-H1-1.5B-Deep-Instruct clearly outperforms leading models in its class, such as Qwen3-1.7B-Instruct. Even more notably, it performs on par with—or better than many 7B models, including Falcon3-7B-Instruct and Qwen2.5-7B-Instruct.</p><blockquote><p>🔎 <strong>Note:</strong> Falcon-H1-1.5B-Deep and Falcon-H1-1.5B were trained using identical settings; the only difference lies in their architectural depth and width.</p></blockquote><p align=center><img src=/plots_h1/h1_1B_comparison.png alt="Falcon-H1-1B model comparison"></p><h3 id=multilingual-benchmarks>Multilingual Benchmarks<a hidden class=anchor aria-hidden=true href=#multilingual-benchmarks>#</a></h3><p>To give a picture of Falcon-H1 performance across languages, we provide average between Hellaswag and MMLU scores for 30B scale models and for a set of selected languages, including Arabic, German, Spanish, French, Hindi, Italian, Dutch, Portuguese, Romanian, Russian, and Swedish. It also demonstrates on-par performance in the other supported languages.</p><div id=chart-multilingual-eval style=margin-top:-40px;margin-bottom:40px></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`\r
[ \r
\r
  {"category": "Arabic", "model": "Falcon-H1-34B", "value": 0.6821},\r
  {"category": "Arabic", "model": "Qwen3-32B", "value": 0.5905},\r
  {"category": "Arabic", "model": "Qwen2.5-72B", "value": 0.6843},\r
  {"category": "Arabic", "model": "Llama4-scout", "value": 0.6205},\r
  {"category": "Arabic", "model": "Gemma3-27B", "value": 0.6435},\r
\r
  {"category": "German", "model": "Falcon-H1-34B", "value": 0.7797},\r
  {"category": "German", "model": "Qwen3-32B", "value": 0.654},\r
  {"category": "German", "model": "Qwen2.5-72B", "value": 0.7475},\r
  {"category": "German", "model": "Llama4-scout", "value": 0.6909},\r
  {"category": "German", "model": "Gemma3-27B", "value": 0.7124},\r
\r
  {"category": "Spanish", "model": "Falcon-H1-34B", "value": 0.8032},\r
  {"category": "Spanish", "model": "Qwen3-32B", "value": 0.6845},\r
  {"category": "Spanish", "model": "Qwen2.5-72B", "value": 0.7853},\r
  {"category": "Spanish", "model": "Llama4-scout", "value": 0.7298},\r
  {"category": "Spanish", "model": "Gemma3-27B", "value": 0.7429},\r
\r
  {"category": "French", "model": "Falcon-H1-34B", "value": 0.8047},\r
  {"category": "French", "model": "Qwen3-32B", "value": 0.6747},\r
  {"category": "French", "model": "Qwen2.5-72B", "value": 0.7812},\r
  {"category": "French", "model": "Llama4-scout", "value": 0.7232},\r
  {"category": "French", "model": "Gemma3-27B", "value": 0.7391},\r
\r
  {"category": "Hindi", "model": "Falcon-H1-34B", "value": 0.6267},\r
  {"category": "Hindi", "model": "Qwen3-32B", "value": 0.5301},\r
  {"category": "Hindi", "model": "Qwen2.5-72B", "value": 0.6123},\r
  {"category": "Hindi", "model": "Llama4-scout", "value": 0.5631},\r
  {"category": "Hindi", "model": "Gemma3-27B", "value": 0.5966},\r
\r
  {"category": "Italian", "model": "Falcon-H1-34B", "value": 0.7952},\r
  {"category": "Italian", "model": "Qwen3-32B", "value": 0.6805},\r
  {"category": "Italian", "model": "Qwen2.5-72B", "value": 0.7732},\r
  {"category": "Italian", "model": "Llama4-scout", "value": 0.7088},\r
  {"category": "Italian", "model": "Gemma3-27B", "value": 0.7367},\r
\r
  {"category": "Dutch", "model": "Falcon-H1-34B", "value": 0.7832},\r
  {"category": "Dutch", "model": "Qwen3-32B", "value": 0.6345},\r
  {"category": "Dutch", "model": "Qwen2.5-72B", "value": 0.7529},\r
  {"category": "Dutch", "model": "Llama4-scout", "value": 0.7076},\r
  {"category": "Dutch", "model": "Gemma3-27B", "value": 0.7163},\r
\r
  {"category": "Portuguese", "model": "Falcon-H1-34B", "value": 0.8037},\r
  {"category": "Portuguese", "model": "Qwen3-32B", "value": 0.468},\r
  {"category": "Portuguese", "model": "Qwen2.5-72B", "value": 0.7848},\r
  {"category": "Portuguese", "model": "Llama4-scout", "value": 0.7247},\r
  {"category": "Portuguese", "model": "Gemma3-27B", "value": 0.7444},\r
\r
  {"category": "Romanian", "model": "Falcon-H1-34B", "value": 0.7655},\r
  {"category": "Romanian", "model": "Qwen3-32B", "value": 0.6497},\r
  {"category": "Romanian", "model": "Qwen2.5-72B", "value": 0.7241},\r
  {"category": "Romanian", "model": "Llama4-scout", "value": 0.6917},\r
  {"category": "Romanian", "model": "Gemma3-27B", "value": 0.715},\r
\r
  {"category": "Russian", "model": "Falcon-H1-34B", "value": 0.7562},\r
  {"category": "Russian", "model": "Qwen3-32B", "value": 0.6482},\r
  {"category": "Russian", "model": "Qwen2.5-72B", "value": 0.7387},\r
  {"category": "Russian", "model": "Llama4-scout", "value": 0.6712},\r
  {"category": "Russian", "model": "Gemma3-27B", "value": 0.6992},\r
\r
  {"category": "Swedish", "model": "Falcon-H1-34B", "value": 0.7766},\r
  {"category": "Swedish", "model": "Qwen3-32B", "value": 0.6379},\r
  {"category": "Swedish", "model": "Qwen2.5-72B", "value": 0.7417},\r
  {"category": "Swedish", "model": "Llama4-scout", "value": 0.697},\r
  {"category": "Swedish", "model": "Gemma3-27B", "value": 0.7221}\r
\r
]\r
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const j=[],b=[],t=j.length?j:Array.from(new Set(n.map(e=>e.model))),g=b.length?b:Array.from(new Set(n.map(e=>e.category))),r="Falcon-H1-34B",i="",l="#b987ff";function d(e,t=.4){const i=parseInt(e.slice(1),16);let n=i>>16&255,s=i>>8&255,o=i&255;return n=Math.round(n+(255-n)*t),s=Math.round(s+(255-s)*t),o=Math.round(o+(255-o)*t),"#"+((1<<24)+(n<<16)+(s<<8)+o).toString(16).slice(1)}const _=d(l,.45),f=!0,e={top:40,right:30,bottom:50,left:130},m=800-e.left-e.right,c="800"-e.top-e.bottom,a=d3.select("#chart-multilingual-eval").append("svg").attr("width",m+e.left+e.right).attr("height",c+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-multilingual-eval")||d3.select("body").append("div").attr("id","tooltip-multilingual-eval").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const p=d3.select("#tooltip-multilingual-eval"),h=d3.scaleBand().domain(g).range([0,c]).padding(.2);a.append("g").call(d3.axisLeft(h).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove(),d3.select(this).append("foreignObject").attr("x",-160).attr("y",-10).attr("width",150).attr("height",50).append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","right").style("width","100%").text(e)});const v=d3.scaleBand().domain(t).range([0,h.bandwidth()]).padding(.05);let o="0.45",s="0.85";if(o===null||s===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;o=o===null?Math.max(0,Math.min(...e)-t):o,s=s===null?Math.max(...e)+t:s}const y=d3.scaleLinear().domain([o,s]).range([0,m]);a.append("g").attr("transform",`translate(0,${c})`).call(d3.axisBottom(y).tickFormat(e=>f?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");function u(e){if(e.model===r)return l;if(e.model===i)return _;const s=t.indexOf(e.model),o=Math.max(t.length-1,1),n=128-s*(0-80)/o;return`rgb(${n},${n+8},${n+16})`}function w(e){if(e===r)return d(l,.15);if(e===i)return d(_,.15);const s=t.indexOf(e),o=Math.max(t.length-1,1),a=128-s*(0-80)/o,n=Math.min(a+30,230);return`rgb(${n},${n+8},${n+16})`}g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`cat-${e}`);s.selectAll(`rect-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",0).attr("y",t=>h(e)+v(t.model)).attr("width",e=>y(e.value)).attr("height",v.bandwidth()).attr("fill",e=>u(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){return w(d3.select(this).attr("data-model"))}),p.style("opacity",1).html(`<strong style="font-size:16px;">${e}</strong><br/><span style="font-size:12px;">`+t.map(e=>f?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>").style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",e=>u(e)),p.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("x",m/2).attr("y",c+e.bottom-10).text("Performance %").style("font-family","inherit");const O=d3.select("#chart-multilingual-eval").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","10px"),x=[...t].sort((e,n)=>e===r?1:n===r?1:e===i?1:n===i?1:t.indexOf(e)-t.indexOf(n));x.forEach(e=>{O.append("div").style("display","flex").style("align-items","center").style("margin","0 10px").html(`<div style="width:20px;height:20px;margin-right:5px;background:${u({model:e})};"></div>${e}`)})})()</script><h3 id=long-context-benchmarks>Long Context Benchmarks<a hidden class=anchor aria-hidden=true href=#long-context-benchmarks>#</a></h3><p>One of the standout features of <strong>Falcon-H1</strong> is its ability to handle <strong>long-context inputs</strong>, an area where <strong>State Space Models (SSMs)</strong> offer significant advantages in terms of <strong>memory efficiency and computational cost</strong>.</p><p>To demonstrate these capabilities, we evaluate <strong>Falcon-H1-34B-Instruct</strong> against <strong>Qwen2.5-72B-Instruct</strong> across a set of long-context benchmarks. We focus on three core task categories drawn from the Helmet benchmark suite - Retrieval-Augmented Generation (RAG): <em>Natural Questions</em>, <em>TriviaQA</em>, <em>PopQA</em>, <em>HotpotQA</em>; Recall tasks: <em>JSON KV</em>, <em>RULER MK Needle</em>, <em>RULER MK UUID</em>, <em>RULER MV</em>; Long Document QA tasks: <em>∞BENCH QA</em>, <em>∞BENCH MC</em>. These evaluations highlight Falcon-H1’s strength in scaling to longer sequences while maintaining high performance and efficiency.</p><iframe src=/plots_h1/barplot_vertical_fh1.html width=1500 height=450 style=border:none></iframe><p>In addition, we conducted a comprehensive evaluation of the Falcon-H1 series alongside leading Transformer-based models across 23 benchmarks, covering multiple domains and model scales. You can explore the interactive results below—simply select the benchmarks most relevant to your use case to view the corresponding aggregated performance scores.</p><iframe src=/plots_h1/test_perf_instruct.html width=1500 height=850 style=border:none></iframe><h2 id=base-models>Base Models<a hidden class=anchor aria-hidden=true href=#base-models>#</a></h2><p>We provide a detailed comparison of Falcon-H1-34B-Base with other leading base models at the same or larger scale, including Qwen2.5-72B, Qwen2.5-32B, Llama-4-Scout-17B-16E (109B) and Gemma3-27B.</p><blockquote><p>🔎 <strong>Note:</strong> Qwen3-32B does not currently offer a base model checkpoint.</p></blockquote><div id=chart-base-eval-34b></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`\r
[    \r
    { "category": "MMLU", "model": "Falcon-H1-34B-Base", "value": 0.8346 },\r
    { "category": "MMLU", "model": "Qwen2.5-72B", "value": 0.8596 },\r
    { "category": "MMLU", "model": "Qwen2.5-32B", "value": 0.8318 },\r
    { "category": "MMLU", "model": "Llama-4-Scout-17B-16E", "value": 0.7798 },\r
    { "category": "MMLU", "model": "gemma-3-27b-pt", "value": 0.7832 },\r
    \r
    { "category": "MMLU-stem",     "model": "Falcon-H1-34B-Base",    "value": 0.8382 },\r
    { "category": "MMLU-stem",     "model": "Qwen2.5-72B",           "value": 0.8481 },\r
    { "category": "MMLU-stem",     "model": "Qwen2.5-32B",           "value": 0.8281 },\r
    { "category": "MMLU-stem",     "model": "Llama-4-Scout-17B-16E", "value": 0.7257 },\r
    { "category": "MMLU-stem",     "model": "gemma-3-27b-pt",        "value": 0.7659 },\r
\r
    \r
    { "category": "MMLU-Pro",      "model": "Falcon-H1-34B-Base",    "value": 0.5718 },\r
    { "category": "MMLU-Pro",      "model": "Qwen2.5-72B",           "value": 0.6022 },\r
    { "category": "MMLU-Pro",      "model": "Qwen2.5-32B",           "value": 0.5805 },\r
    { "category": "MMLU-Pro",      "model": "Llama-4-Scout-17B-16E", "value": 0.5016 },\r
    { "category": "MMLU-Pro",      "model": "gemma-3-27b-pt",        "value": 0.4964 },\r
\r
\r
    { "category": "GPQA",          "model": "Falcon-H1-34B-Base",    "value": 0.427  },\r
    { "category": "GPQA",          "model": "Qwen2.5-72B",           "value": 0.4228 },\r
    { "category": "GPQA",          "model": "Qwen2.5-32B",           "value": 0.3968 },\r
    { "category": "GPQA",          "model": "Llama-4-Scout-17B-16E", "value": 0.3599 },\r
    { "category": "GPQA",          "model": "gemma-3-27b-pt",        "value": 0.3582 },\r
\r
    { "category": "BBH",           "model": "Falcon-H1-34B-Base",    "value": 0.6936 },\r
    { "category": "BBH",           "model": "Qwen2.5-72B",           "value": 0.6777 },\r
    { "category": "BBH",           "model": "Qwen2.5-32B",           "value": 0.6745 },\r
    { "category": "BBH",           "model": "Llama-4-Scout-17B-16E", "value": 0.6171 },\r
    { "category": "BBH",           "model": "gemma-3-27b-pt",        "value": 0.6160 },\r
\r
    { "category": "ARC-Challenge", "model": "Falcon-H1-34B-Base",    "value": 0.7125 },\r
    { "category": "ARC-Challenge", "model": "Qwen2.5-72B",           "value": 0.7244 },\r
    { "category": "ARC-Challenge", "model": "Qwen2.5-32B",           "value": 0.7048 },\r
    { "category": "ARC-Challenge", "model": "Llama-4-Scout-17B-16E", "value": 0.6297 },\r
    { "category": "ARC-Challenge", "model": "gemma-3-27b-pt",        "value": 0.7031 },\r
\r
    { "category": "Winogrande",    "model": "Falcon-H1-34B-Base",    "value": 0.8272 },\r
    { "category": "Winogrande",    "model": "Qwen2.5-72B",           "value": 0.8374 },\r
    { "category": "Winogrande",    "model": "Qwen2.5-32B",           "value": 0.8232 },\r
    { "category": "Winogrande",    "model": "Llama-4-Scout-17B-16E", "value": 0.7893 },\r
    { "category": "Winogrande",    "model": "gemma-3-27b-pt",        "value": 0.8240 },\r
\r
    { "category": "HellaSwag",     "model": "Falcon-H1-34B-Base",    "value": 0.8568 },\r
    { "category": "HellaSwag",     "model": "Qwen2.5-72B",           "value": 0.8757 },\r
    { "category": "HellaSwag",     "model": "Qwen2.5-32B",           "value": 0.8513 },\r
    { "category": "HellaSwag",     "model": "Llama-4-Scout-17B-16E", "value": 0.8401 },\r
    { "category": "HellaSwag",     "model": "gemma-3-27b-pt",        "value": 0.8619 },\r
\r
    { "category": "GSM8K",         "model": "Falcon-H1-34B-Base",    "value": 0.7650 },\r
    { "category": "GSM8K",         "model": "Qwen2.5-72B",           "value": 0.8976 },\r
    { "category": "GSM8K",         "model": "Qwen2.5-32B",           "value": 0.9014 },\r
    { "category": "GSM8K",         "model": "Llama-4-Scout-17B-16E", "value": 0.8324 },\r
    { "category": "GSM8K",         "model": "gemma-3-27b-pt",        "value": 0.8135 },\r
\r
    { "category": "MATH_lv5",      "model": "Falcon-H1-34B-Base",    "value": 0.4071 },\r
    { "category": "MATH_lv5",      "model": "Qwen2.5-72B",           "value": 0.3814 },\r
    { "category": "MATH_lv5",      "model": "Qwen2.5-32B",           "value": 0.3640 },\r
    { "category": "MATH_lv5",      "model": "Llama-4-Scout-17B-16E", "value": 0.2719 },\r
    { "category": "MATH_lv5",      "model": "gemma-3-27b-pt",        "value": 0.2538 },\r
\r
    { "category": "HumanEval",     "model": "Falcon-H1-34B-Base",    "value": 0.7012 },\r
    { "category": "HumanEval",     "model": "Qwen2.5-72B",           "value": 0.5915 },\r
    { "category": "HumanEval",     "model": "Qwen2.5-32B",           "value": 0.5976 },\r
    { "category": "HumanEval",     "model": "Llama-4-Scout-17B-16E", "value": 0.5732 },\r
    { "category": "HumanEval",     "model": "gemma-3-27b-pt",        "value": 0.4878 },\r
\r
    { "category": "HumanEval-plus",    "model": "Falcon-H1-34B-Base",    "value": 0.6463 },\r
    { "category": "HumanEval-plus",    "model": "Qwen2.5-72B",           "value": 0.5122 },\r
    { "category": "HumanEval-plus",    "model": "Qwen2.5-32B",           "value": 0.5183 },\r
    { "category": "HumanEval-plus",    "model": "Llama-4-Scout-17B-16E", "value": 0.4878 },\r
    { "category": "HumanEval-plus",    "model": "gemma-3-27b-pt",        "value": 0.4085 },\r
\r
    { "category": "MBPP",          "model": "Falcon-H1-34B-Base",    "value": 0.8333 },\r
    { "category": "MBPP",          "model": "Qwen2.5-72B",           "value": 0.8704 },\r
    { "category": "MBPP",          "model": "Qwen2.5-32B",           "value": 0.8307 },\r
    { "category": "MBPP",          "model": "Llama-4-Scout-17B-16E", "value": 0.7778 },\r
    { "category": "MBPP",          "model": "gemma-3-27b-pt",        "value": 0.7619 },\r
\r
    { "category": "MBPP-plus",         "model": "Falcon-H1-34B-Base",    "value": 0.7037 },\r
    { "category": "MBPP-plus",         "model": "Qwen2.5-72B",           "value": 0.7063 },\r
    { "category": "MBPP-plus",         "model": "Qwen2.5-32B",           "value": 0.6878 },\r
    { "category": "MBPP-plus",         "model": "Llama-4-Scout-17B-16E", "value": 0.6429 },\r
    { "category": "MBPP-plus",         "model": "gemma-3-27b-pt",        "value": 0.6164 },\r
\r
    { "category": "Average",       "model": "Falcon-H1-34B-Base",    "value": 0.7013 },\r
    { "category": "Average",       "model": "Qwen2.5-72B",           "value": 0.7005 },\r
    { "category": "Average",       "model": "Qwen2.5-32B",           "value": 0.6851 },\r
    { "category": "Average",       "model": "Llama-4-Scout-17B-16E", "value": 0.6307 },\r
    { "category": "Average",       "model": "gemma-3-27b-pt",        "value": 0.6250 }\r
]\r
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const p=[],t=p.length>0?p:Array.from(new Set(n.map(e=>e.model))),f=[],u=f.length>0?f:Array.from(new Set(n.map(e=>e.category))),b="Falcon-H1-34B-Base",r=b||t[0],v=!0,e={top:40,right:30,bottom:50,left:130},d=800-e.left-e.right,i="700"-e.top-e.bottom,a=d3.select("#chart-base-eval-34b").append("svg").attr("width",d+e.left+e.right).attr("height",i+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-base-eval-34b")||d3.select("body").append("div").attr("id","tooltip-base-eval-34b").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const h=d3.select("#tooltip-base-eval-34b"),l=d3.scaleBand().domain(u).range([0,i]).padding(.2);a.append("g").call(d3.axisLeft(l).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove();const t=d3.select(this).append("foreignObject").attr("x",-160).attr("y",-10).attr("width",150).attr("height",50);t.append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","right").style("width","100%").text(e)});const m=d3.scaleBand().domain(t).range([0,l.bandwidth()]).padding(.05);let o="0.25",s="0.95";if(o===null||s===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;o=o===null?Math.max(0,Math.min(...e)-t):o,s=s===null?Math.max(...e)+t:s}const g=d3.scaleLinear().domain([o,s]).range([0,d]);a.append("g").attr("transform",`translate(0,${i})`).call(d3.axisBottom(g).tickFormat(e=>v?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");const c=e=>{if(e.model===r)return"#b987ff";const s=t.indexOf(e.model),o=t.length,n=128-s*(0-80)/(o-1||1);return`rgb(${n}, ${n+8}, ${n+16})`},j=e=>{if(e===r)return"#b088ff";const s=t.indexOf(e),o=t.length,i=128-s*(0-80)/(o-1||1),n=Math.min(i+30,230);return`rgb(${n}, ${n+8}, ${n+16})`};u.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`category-group-${e}`);s.selectAll(`.bar-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",0).attr("y",t=>l(e)+m(t.model)).attr("width",e=>g(e.value)).attr("height",m.bandwidth()).attr("fill",e=>c(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){const e=d3.select(this).attr("data-model");return j(e)});const s=`<strong style="font-size: 16px;">${e}</strong><br/><span style="font-size: 12px;">`+t.map(e=>v?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>";h.style("opacity",1).html(s).style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",function(){const e=d3.select(this).attr("data-model");return c({model:e})}),h.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("x",d/2).attr("y",i+e.bottom-10).text("Performance %").style("font-family","inherit");const y=d3.select("#chart-base-eval-34b").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","10px"),_=[...t].sort((e,n)=>e===r?-1:n===r?1:t.indexOf(e)-t.indexOf(n));_.forEach(e=>{const n=c({model:e}),t=y.append("div").style("display","flex").style("align-items","center").style("margin","0 10px");t.append("div").style("width","20px").style("height","20px").style("margin-right","5px").style("background-color",n),t.append("div").text(e)})})()</script><p>Below, we compare 1.5B-scale base models. Falcon-H1-1.5B-Deep-Base clearly outperforms leading models in its class, such as Qwen3-1.7B-Base. Notably, it performs on par with Falcon3-7B, and even exceeds it on math and reasoning tasks, making it an excellent foundation for building small-scale reasoning-focused models.</p><div id=chart-base-eval-1b style=margin-top:-40px;margin-bottom:40px></div><script src=https://cdnjs.cloudflare.com/ajax/libs/d3/7.8.5/d3.min.js></script><script>(function(){let n;try{n=JSON.parse(`\r
[    \r
    { "category": "MMLU", "model": "Falcon3-7B-Base", "value": 0.6998 },\r
    { "category": "MMLU", "model": "Falcon-H1-1.5B-Deep-Base", "value": 0.6629 },\r
    { "category": "MMLU", "model": "Falcon-H1-1.5B-Base", "value": 0.6181 },\r
    { "category": "MMLU", "model": "Qwen3-1.7B-Base", "value": 0.6246 },\r
    { "category": "MMLU", "model": "gemma-3-1b-pt", "value": 0.2633 },\r
    \r
    { "category": "MMLU-stem",     "model": "Falcon3-7B-Base",           "value": 0.6771 },\r
    { "category": "MMLU-stem",     "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.6743 },\r
    { "category": "MMLU-stem",     "model": "Falcon-H1-1.5B-Base",           "value": 0.6337 },\r
    { "category": "MMLU-stem",     "model": "Qwen3-1.7B-Base", "value": 0.6153 },\r
    { "category": "MMLU-stem",     "model": "gemma-3-1b-pt",        "value": 0.2759 },\r
\r
    { "category": "MMLU-Pro",      "model": "Falcon3-7B-Base",           "value": 0.3923 },\r
    { "category": "MMLU-Pro",      "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.4107 },\r
    { "category": "MMLU-Pro",      "model": "Falcon-H1-1.5B-Base",           "value": 0.3553 },\r
    { "category": "MMLU-Pro",      "model": "Qwen3-1.7B-Base", "value": 0.3381 },\r
    { "category": "MMLU-Pro",      "model": "gemma-3-1b-pt",        "value": 0.1131 },\r
\r
    { "category": "GPQA",          "model": "Falcon3-7B-Base",           "value": 0.3507 },\r
    { "category": "GPQA",          "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.328  },\r
    { "category": "GPQA",          "model": "Falcon-H1-1.5B-Base",           "value": 0.2911 },\r
    { "category": "GPQA",          "model": "Qwen3-1.7B-Base", "value": 0.2945 },\r
    { "category": "GPQA",          "model": "gemma-3-1b-pt",        "value": 0.2466 },\r
\r
    { "category": "BBH",           "model": "Falcon3-7B-Base",           "value": 0.5088 },\r
    { "category": "BBH",           "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.5237 },\r
    { "category": "BBH",           "model": "Falcon-H1-1.5B-Base",           "value": 0.4657 },\r
    { "category": "BBH",           "model": "Qwen3-1.7B-Base", "value": 0.4305 },\r
    { "category": "BBH",           "model": "gemma-3-1b-pt",        "value": 0.3026 },\r
\r
    { "category": "ARC-Challenge", "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.5589 },\r
    { "category": "ARC-Challenge", "model": "Falcon-H1-1.5B-Base",           "value": 0.5324 },\r
    { "category": "ARC-Challenge", "model": "Falcon3-7B-Base",           "value": 0.6271 },\r
    { "category": "ARC-Challenge", "model": "Qwen3-1.7B-Base", "value": 0.5572 },\r
    { "category": "ARC-Challenge", "model": "gemma-3-1b-pt",        "value": 0.3933 },\r
\r
    { "category": "Winogrande",    "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.6709 },\r
    { "category": "Winogrande",    "model": "Falcon-H1-1.5B-Base",           "value": 0.6559 },\r
    { "category": "Winogrande",    "model": "Falcon3-7B-Base",           "value": 0.7364 },\r
    { "category": "Winogrande",    "model": "Qwen3-1.7B-Base", "value": 0.663 },\r
    { "category": "Winogrande",    "model": "gemma-3-1b-pt",        "value": 0.6259 },\r
\r
    { "category": "HellaSwag",     "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.6972 },\r
    { "category": "HellaSwag",     "model": "Falcon-H1-1.5B-Base",           "value": 0.6676 },\r
    { "category": "HellaSwag",     "model": "Falcon3-7B-Base",           "value": 0.7669 },\r
    { "category": "HellaSwag",     "model": "Qwen3-1.7B-Base", "value": 0.6709 },\r
    { "category": "HellaSwag",     "model": "gemma-3-1b-pt",        "value": 0.6294 },\r
\r
    { "category": "GSM8K",         "model": "Falcon3-7B-Base",           "value": 0.7695 },\r
    { "category": "GSM8K",         "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.6869 },\r
    { "category": "GSM8K",         "model": "Falcon-H1-1.5B-Base",           "value": 0.5201 },\r
    { "category": "GSM8K",         "model": "Qwen3-1.7B-Base", "value": 0.7074 },\r
    { "category": "GSM8K",         "model": "gemma-3-1b-pt",        "value": 0.022 },\r
\r
    { "category": "MATH_lv5",      "model": "Falcon3-7B-Base",           "value": 0.2009 },\r
    { "category": "MATH_lv5",      "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.2477 },\r
    { "category": "MATH_lv5",      "model": "Falcon-H1-1.5B-Base",           "value": 0.2039 },\r
    { "category": "MATH_lv5",      "model": "Qwen3-1.7B-Base", "value": 0.1639 },\r
    { "category": "MATH_lv5",      "model": "gemma-3-1b-pt",        "value": 0.012 },\r
\r
    { "category": "HumanEval",     "model": "Falcon3-7B-Base",           "value": 0.50 },\r
    { "category": "HumanEval",     "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.5244 },\r
    { "category": "HumanEval",     "model": "Falcon-H1-1.5B-Base",           "value": 0.50 },\r
    { "category": "HumanEval",     "model": "Qwen3-1.7B-Base", "value": 0.6768 },\r
    { "category": "HumanEval",     "model": "gemma-3-1b-pt",        "value": 0.067 },\r
\r
    { "category": "HumanEval-plus",    "model": "Falcon3-7B-Base",           "value": 0.4329 },\r
    { "category": "HumanEval-plus",    "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.4634 },\r
    { "category": "HumanEval-plus",    "model": "Falcon-H1-1.5B-Base",           "value": 0.4268 },\r
    { "category": "HumanEval-plus",    "model": "Qwen3-1.7B-Base", "value": 0.6098 },\r
    { "category": "HumanEval-plus",    "model": "gemma-3-1b-pt",        "value": 0.0549 },\r
\r
    { "category": "MBPP",          "model": "Falcon3-7B-Base",           "value": 0.6799 },\r
    { "category": "MBPP",          "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.709 },\r
    { "category": "MBPP",          "model": "Falcon-H1-1.5B-Base",           "value": 0.6508 },\r
    { "category": "MBPP",          "model": "Qwen3-1.7B-Base", "value": 0.6772 },\r
    { "category": "MBPP",          "model": "gemma-3-1b-pt",        "value": 0.127 },\r
\r
    { "category": "MBPP-plus",         "model": "Falcon3-7B-Base",           "value": 0.5714 },\r
    { "category": "MBPP-plus",         "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.6032 },\r
    { "category": "MBPP-plus",         "model": "Falcon-H1-1.5B-Base",           "value": 0.5503 },\r
    { "category": "MBPP-plus",         "model": "Qwen3-1.7B-Base", "value": 0.5899 },\r
    { "category": "MBPP-plus",         "model": "gemma-3-1b-pt",        "value": 0.0952 },\r
\r
    { "category": "Average",       "model": "Falcon3-7B-Base",           "value": 0.5653 },\r
    { "category": "Average",       "model": "Falcon-H1-1.5B-Deep-Base",    "value": 0.5544 },\r
    { "category": "Average",       "model": "Falcon-H1-1.5B-Base",           "value": 0.5051 },\r
    { "category": "Average",       "model": "Qwen3-1.7B-Base", "value": 0.5442 },\r
    { "category": "Average",       "model": "gemma-3-1b-pt",        "value": 0.2306 }\r
]\r
`)}catch(e){console.error("Error parsing chart data:",e),n=[]}const j=[],b=[],t=j.length?j:Array.from(new Set(n.map(e=>e.model))),g=b.length?b:Array.from(new Set(n.map(e=>e.category))),r="Falcon-H1-1.5B-Deep-Base",i="Falcon-H1-1.5B-Base",l="#b987ff";function d(e,t=.4){const i=parseInt(e.slice(1),16);let n=i>>16&255,s=i>>8&255,o=i&255;return n=Math.round(n+(255-n)*t),s=Math.round(s+(255-s)*t),o=Math.round(o+(255-o)*t),"#"+((1<<24)+(n<<16)+(s<<8)+o).toString(16).slice(1)}const _=d(l,.45),f=!0,e={top:40,right:30,bottom:50,left:130},m=800-e.left-e.right,c="700"-e.top-e.bottom,a=d3.select("#chart-base-eval-1b").append("svg").attr("width",m+e.left+e.right).attr("height",c+e.top+e.bottom).append("g").attr("transform",`translate(${e.left},${e.top})`);document.getElementById("tooltip-base-eval-1b")||d3.select("body").append("div").attr("id","tooltip-base-eval-1b").attr("class","tooltip").style("position","absolute").style("padding","10px").style("background","#f9f9f9").style("border","1px solid #ddd").style("border-radius","5px").style("pointer-events","none").style("opacity","0").style("transition","opacity 0.3s").style("font-family","inherit").style("font-size","14px").style("box-shadow","0 2px 5px rgba(0,0,0,0.1)");const p=d3.select("#tooltip-base-eval-1b"),h=d3.scaleBand().domain(g).range([0,c]).padding(.2);a.append("g").call(d3.axisLeft(h).tickSize(0)).selectAll(".tick").each(function(e){d3.select(this).select("text").remove(),d3.select(this).append("foreignObject").attr("x",-160).attr("y",-10).attr("width",150).attr("height",50).append("xhtml:div").style("font-weight","bold").style("font-family","inherit").style("word-wrap","break-word").style("text-align","right").style("width","100%").text(e)});const v=d3.scaleBand().domain(t).range([0,h.bandwidth()]).padding(.05);let o="0",s="0.8";if(o===null||s===null){const e=n.map(e=>e.value),t=(Math.max(...e)-Math.min(...e))*.1;o=o===null?Math.max(0,Math.min(...e)-t):o,s=s===null?Math.max(...e)+t:s}const y=d3.scaleLinear().domain([o,s]).range([0,m]);a.append("g").attr("transform",`translate(0,${c})`).call(d3.axisBottom(y).tickFormat(e=>f?d3.format(".0%")(e):d3.format(".2f")(e))).selectAll("text").style("font-family","inherit");function u(e){if(e.model===r)return l;if(e.model===i)return _;const s=t.indexOf(e.model),o=Math.max(t.length-1,1),n=128-s*(0-80)/o;return`rgb(${n},${n+8},${n+16})`}function w(e){if(e===r)return d(l,.15);if(e===i)return d(_,.15);const s=t.indexOf(e),o=Math.max(t.length-1,1),a=128-s*(0-80)/o,n=Math.min(a+30,230);return`rgb(${n},${n+8},${n+16})`}g.forEach(e=>{const t=n.filter(t=>t.category===e),s=a.append("g").attr("class",`cat-${e}`);s.selectAll(`rect-${e}`).data(t).enter().append("rect").attr("class","bar").attr("x",0).attr("y",t=>h(e)+v(t.model)).attr("width",e=>y(e.value)).attr("height",v.bandwidth()).attr("fill",e=>u(e)).attr("data-model",e=>e.model).style("transition","opacity 0.3s"),s.on("mouseover",function(n){d3.select(this).selectAll("rect").attr("stroke","#333").attr("stroke-width",2).attr("fill",function(){return w(d3.select(this).attr("data-model"))}),p.style("opacity",1).html(`<strong style="font-size:16px;">${e}</strong><br/><span style="font-size:12px;">`+t.map(e=>f?`${e.model}: ${(e.value*100).toFixed(2)}%`:`${e.model}: ${e.value.toFixed(3)}`).join("<br/>")+"</span>").style("left",n.pageX+15+"px").style("top",n.pageY-28+"px")}).on("mouseout",function(){d3.select(this).selectAll("rect").attr("stroke","none").attr("fill",e=>u(e)),p.style("opacity",0)})}),a.append("text").attr("text-anchor","middle").attr("x",m/2).attr("y",c+e.bottom-10).text("Performance %").style("font-family","inherit");const O=d3.select("#chart-base-eval-1b").append("div").style("display","flex").style("justify-content","center").style("font-size","14px").style("margin-top","10px"),x=[...t].sort((e,n)=>e===r?1:n===r?1:e===i?1:n===i?1:t.indexOf(e)-t.indexOf(n));x.forEach(e=>{O.append("div").style("display","flex").style("align-items","center").style("margin","0 10px").html(`<div style="width:20px;height:20px;margin-right:5px;background:${u({model:e})};"></div>${e}`)})})()</script><p>For the base models, we also provide an interactive plot showcasing their performance across 14 benchmarks, spanning multiple domains and various model scales.</p><iframe src=/plots_h1/test_perf_base.html width=1500 height=850 style=border:none></iframe><h2 id=model-efficiency>Model Efficiency<a hidden class=anchor aria-hidden=true href=#model-efficiency>#</a></h2><p>We compare input (prefill) and output (generation) throughput between Falcon-H1 and Qwen2.5-32B in the plots below. While Transformers are slightly faster at shorter context lengths, our hybrid model becomes significantly more efficient as the context grows—achieving up to <strong>4× speedup in input throughput</strong> and <strong>8× in output throughput</strong> at longer sequence lengths. Benchmarks were run using our <strong>Falcon-H1 vLLM implementation</strong> and the official vLLM implementation of Qwen2.5-32B.</p><p>This performance gain highlights the <strong>scalability</strong> of the Falcon-H1 architecture. We attribute the throughput gap at small context lengths to the more mature optimizations of attention mechanisms, compared to current State Space Models (SSMs) implementations, in current inference pipelines.</p><blockquote><p>⚙️ <em>We invite the community to contribute to further optimizing SSM implementations</em> — a promising direction for advancing the next generation of efficient LLMs.</p></blockquote><iframe src=/plots_h1/throughput_combined.html width=1500 height=1350 style=border:none></iframe><h2 id=prompt-examples>Prompt Examples<a hidden class=anchor aria-hidden=true href=#prompt-examples>#</a></h2><p>Below are a few example outputs generated by Falcon-H1-34B-Instruct.<div class=llm-gallery-container><div class=filters><div class=filter-group><label>Search</label>
<input type=text class=search-box placeholder="Search examples..."></div><div class=filter-group><label for=language-filter>Language</label>
<select id=language-filter><option value>All Languages</option></select></div><div class=filter-group><label for=source-filter>Source</label>
<select id=source-filter><option value>All Sources</option></select></div><div class=filter-group><label for=category-filter>Category</label>
<select id=category-filter><option value>All Categories</option></select></div></div><div class=cards-container id=cards-container><div class=loading>Loading examples...</div></div><div class=pagination id=pagination></div><div class=no-results id=no-results style=display:none>No examples match your filters. Try changing your selection.</div><div id=debug-info style=margin-top:20px;padding:10px;background-color:#f5f5f5;border-radius:4px;display:none><h4>Debug Information</h4><div id=debug-content></div></div></div><div id=example-modal class=modal><div class=modal-content><span class=close-btn>&#215;</span><div class=modal-header><h3 class=modal-title></h3><div class=modal-metadata></div></div><div class=modal-prompt></div><div class=modal-answer></div></div></div><style>.llm-gallery-container{font-family:-apple-system,BlinkMacSystemFont,segoe ui,Roboto,Oxygen,Ubuntu,Cantarell,open sans,helvetica neue,sans-serif;max-width:800px;margin:0 auto}.loading{text-align:center;padding:40px;font-size:16px;color:#666}.filters{margin-bottom:20px;display:flex;flex-direction:row;flex-wrap:nowrap;gap:10px;justify-content:space-between;max-width:800px}.filter-group{flex:1;display:flex;flex-direction:column}.filter-group label{font-weight:600;margin-bottom:5px;color:#333;white-space:nowrap;text-align:center}.filter-group select,.filter-group .search-box{padding:8px 10px;border-radius:4px;border:1px solid #ddd;background-color:#fff;width:100%;font-size:14px;height:38px}.search-box{padding:8px 10px;border-radius:4px;border:1px solid #ddd;flex:1.5;font-size:14px;height:36px}.cards-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(250px,1fr));gap:20px;margin-top:20px}.card{border-radius:8px;overflow:hidden;box-shadow:0 4px 6px rgba(0,0,0,.1);transition:transform .2s,box-shadow .2s;background-color:#fff;position:relative;height:350px;display:flex;flex-direction:column}.card:hover{transform:translateY(-5px);box-shadow:0 8px 15px rgba(0,0,0,.1)}.card-header{padding:15px;background-color:#f7f7f7;border-bottom:1px solid #eee}.card-title{margin:0;font-size:16px;font-weight:600;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.metadata{display:flex;flex-wrap:wrap;gap:8px;margin-top:8px}.tag{font-size:12px;padding:3px 8px;border-radius:12px;background-color:#f0f0f0}.tag.language{background-color:#e3f2fd;color:#0d47a1}.tag.source{background-color:#e8f5e9;color:#1b5e20}.tag.category{background-color:#fff3e0;color:#e65100}.card-content{padding:15px;flex-grow:1;overflow:hidden;display:flex;flex-direction:column}.prompt{font-weight:500;margin-bottom:10px;white-space:pre-line;overflow:hidden;max-height:4.5em;line-height:1.5}.answer{font-size:14px;line-height:1.5;color:#444;white-space:pre-line;overflow:hidden;max-height:7.5em;flex-grow:1}.expand-btn{background-color:#2196f3;color:#fff;border:none;padding:8px 15px;border-radius:4px;cursor:pointer;font-size:14px;margin-top:10px;align-self:flex-end}.expand-btn:hover{background-color:#1976d2}.modal{display:none;position:fixed;z-index:1000;left:0;top:0;width:100%;height:100%;overflow:auto;background-color:rgba(0,0,0,.7)}.modal-content{background-color:#fefefe;margin:5% auto;padding:20px;border-radius:8px;width:80%;max-width:800px;max-height:80vh;overflow-y:auto}.close-btn{color:#aaa;float:right;font-size:28px;font-weight:700;cursor:pointer}.close-btn:hover{color:#000}.modal-header{margin-bottom:15px;padding-bottom:15px;border-bottom:1px solid #eee}.modal-title{margin:0;font-size:20px;font-weight:600}.modal-metadata{display:flex;flex-wrap:wrap;gap:10px;margin-top:10px}.modal-prompt{margin-bottom:20px;padding:15px;background-color:#f5f5f5;border-radius:6px;font-weight:500;white-space:pre-line}.modal-answer{padding:15px;background-color:#f9f9f9;border-radius:6px;line-height:1.6;white-space:pre-line}.pagination{display:flex;justify-content:center;margin-top:30px;gap:8px}.pagination button{padding:8px 15px;border:1px solid #ddd;background-color:#fff;border-radius:4px;cursor:pointer}.pagination button.active{background-color:#2196f3;color:#fff;border-color:#2196f3}.pagination button:hover:not(.active){background-color:#f5f5f5}.no-results{text-align:center;padding:40px;font-size:16px;color:#666}#debug-info{font-family:monospace;font-size:12px;white-space:pre-wrap}@media(max-width:800px){.filters{flex-wrap:wrap}.search-box{flex:1 0 100%;order:-1;margin-bottom:10px}.filter-group{flex:1 0 30%}.llm-gallery-container{padding:0 10px}}body.dark .card{background-color:#2d2d2d;box-shadow:0 4px 6px rgba(0,0,0,.3)}body.dark .card-header{background-color:#222;border-bottom:1px solid #333}body.dark .card-title{color:#e0e0e0}body.dark .tag{background-color:#444;color:#e0e0e0}body.dark .tag.language{background-color:#1a3c71;color:#b3d4fc}body.dark .tag.source{background-color:#1e4620;color:#b8dab8}body.dark .tag.category{background-color:#6d3200;color:#ffe0b2}body.dark .answer{color:#bbb}body.dark .modal-content{background-color:#2d2d2d;color:#e0e0e0}body.dark .modal-header{border-bottom:1px solid #444}body.dark .modal-prompt{background-color:#222}body.dark .modal-answer{background-color:#333}body.dark .close-btn{color:#ccc}body.dark .close-btn:hover{color:#fff}body.dark .search-box,body.dark .filter-group select{background-color:#333;color:#e0e0e0;border-color:#555}body.dark .pagination button{background-color:#333;color:#e0e0e0;border-color:#555}body.dark .pagination button.active{background-color:#1976d2;color:#fff;border-color:#1976d2}body.dark .pagination button:hover:not(.active){background-color:#444}</style><script>document.addEventListener("DOMContentLoaded",function(){const i=document.getElementById("cards-container"),a=document.getElementById("pagination"),_=document.getElementById("no-results"),j=document.querySelector(".search-box"),v=document.getElementById("language-filter"),g=document.getElementById("source-filter"),p=document.getElementById("category-filter"),n=document.getElementById("example-modal"),w=document.querySelector(".close-btn"),O=document.getElementById("debug-info"),C=document.getElementById("debug-content"),E=!1;function e(e,t){E&&(console.log(e,t),O.style.display="block",C.innerHTML+=`<div>${e}: ${JSON.stringify(t)}</div>`)}const o="content/blog/falcon-H1/llm_examples.json";e("Raw JSON Path",o);const y=[o,`/`+o,o.replace(/^\//,""),o.replace(/^content\//,"/"),o.replace(/^content\//,""),window.location.pathname.replace(/\/[^/]*$/,"/")+o.split("/").pop()];e("Trying paths",y);let c=[],s=[],t=1;const f=6;let d=new Set,m=new Set,l=new Set;w.onclick=function(){n.style.display="none"},window.onclick=function(e){e.target==n&&(n.style.display="none")};async function h(t,n=0){if(n>=t.length){i.innerHTML=`<div class="no-results">Failed to load examples data. Tried paths: ${t.join(", ")}</div>`;return}try{const s=t[n];e("Trying path",s);const a=await fetch(s);if(!a.ok)return e("Failed to load from path",{path:s,status:a.status}),h(t,n+1);const o=await a.text();if(e("File content preview",o.substring(0,200)),!o||o.trim()==="")return e("Empty file content from path",s),h(t,n+1);try{const t=JSON.parse(o);e("Parsed JSON data",{type:Array.isArray(t)?"array":"object",length:Array.isArray(t)?t.length:Object.keys(t).length});let n;if(Array.isArray(t))n=t;else if(t.data&&Array.isArray(t.data))n=t.data;else{const e=Object.keys(t).filter(e=>Array.isArray(t[e]));e.length>0?n=t[e[0]]:n=[t]}if(e("Processed JSON data",{count:n.length,sample:n.slice(0,2)}),!n||n.length===0){e("No records found in JSON",{path:s}),i.innerHTML=`<div class="no-results">No examples found in data file.</div>`;return}x(n)}catch(t){e("Error parsing JSON",{error:t.message}),i.innerHTML=`<div class="no-results">Error parsing JSON data: ${t.message}</div>`}}catch(s){return e("Error fetching from path",{path:t[n],error:s.message}),h(t,n+1)}}function x(t){d=new Set,m=new Set,l=new Set,c=t.map((e,t)=>(e.language&&d.add(e.language.trim()),e.source&&m.add(e.source.trim()),e.category&&l.add(e.category.trim()),{id:t,prompt:e.prompt||"",answer:e.answer||"",language:e.language||"",source:e.source||"",category:e.category||""})),e("Processed examples",{count:c.length,filters:{languages:[...d],sources:[...m],categories:[...l]}}),b(v,d),b(g,m),b(p,l),s=[...c],r()}function b(t,n){if(n.size===0){e("No values for filter",{id:t.id});return}const s=Array.from(n).sort();e("Populating filter",{id:t.id,values:s}),s.forEach(e=>{const n=document.createElement("option");n.value=e,n.textContent=e,t.appendChild(n)})}function u(){const n=j.value.toLowerCase(),o=v.value,i=g.value,a=p.value;e("Applying filters",{searchTerm:n,language:o,source:i,category:a}),s=c.filter(e=>{const t=n===""||e.prompt.toLowerCase().includes(n)||e.answer.toLowerCase().includes(n),s=o===""||e.language===o,r=i===""||e.source===i,c=a===""||e.category===a;return t&&s&&r&&c}),e("Filtered examples",{count:s.length}),t=1,r()}function r(){if(s.length===0){i.innerHTML="",a.innerHTML="",_.style.display="block";return}_.style.display="none";const n=Math.ceil(s.length/f),o=(t-1)*f,c=Math.min(o+f,s.length),r=s.slice(o,c);e("Rendering examples",{page:t,totalPages:n,showing:r.length}),i.innerHTML="",r.forEach(e=>{const t=document.createElement("div");t.className="card",t.innerHTML=`
          <div class="card-header">
            <h3 class="card-title">Example #${e.id+1}</h3>
            <div class="metadata">
              ${e.language?`<span class="tag language">${e.language}</span>`:""}
              ${e.source?`<span class="tag source">${e.source}</span>`:""}
              ${e.category?`<span class="tag category">${e.category}</span>`:""}
            </div>
          </div>
          <div class="card-content">
            <div class="prompt"></div>
            <div class="answer"></div>
            <button class="expand-btn" data-id="${e.id}">View Full</button>
          </div>
        `;const n=t.querySelector(".prompt"),s=t.querySelector(".answer");n.textContent=e.prompt,s.textContent=e.answer,i.appendChild(t)}),document.querySelectorAll(".expand-btn").forEach(e=>{e.addEventListener("click",function(){const e=parseInt(this.getAttribute("data-id"));A(e)})}),k(n)}function k(e){if(a.innerHTML="",e<=1)return;const s=document.createElement("button");s.textContent="«",s.disabled=t===1,s.addEventListener("click",()=>{t>1&&(t--,r())}),a.appendChild(s);const o=5;let n=Math.max(1,t-Math.floor(o/2));const c=Math.min(e,n+o-1);c-n+1<o&&n>1&&(n=Math.max(1,c-o+1));for(let e=n;e<=c;e++){const s=document.createElement("button");s.textContent=e,s.classList.toggle("active",e===t),s.addEventListener("click",()=>{t=e,r()}),a.appendChild(s)}const i=document.createElement("button");i.textContent="»",i.disabled=t===e,i.addEventListener("click",()=>{t<e&&(t++,r())}),a.appendChild(i)}function A(e){const t=c.find(t=>t.id===e);if(!t)return;const r=n.querySelector(".modal-title"),l=n.querySelector(".modal-metadata"),i=n.querySelector(".modal-prompt"),a=n.querySelector(".modal-answer");r.textContent=`Example #${t.id+1}`,l.innerHTML=`
        ${t.language?`<span class="tag language">${t.language}</span>`:""}
        ${t.source?`<span class="tag source">${t.source}</span>`:""}
        ${t.category?`<span class="tag category">${t.category}</span>`:""}
      `,i.innerHTML="<strong>Prompt:</strong>",a.innerHTML="<strong>Answer:</strong>";const s=document.createElement("div");s.textContent=t.prompt,s.style.whiteSpace="pre-line",s.style.marginTop="8px",i.appendChild(s);const o=document.createElement("div");o.textContent=t.answer,o.style.whiteSpace="pre-line",o.style.marginTop="8px",a.appendChild(o),n.style.display="block"}j.addEventListener("input",u),v.addEventListener("change",u),g.addEventListener("change",u),p.addEventListener("change",u),h(y)})</script></p><h2 id=open-source-commitment>Open Source Commitment<a hidden class=anchor aria-hidden=true href=#open-source-commitment>#</a></h2><p>In line with our mission to foster AI accessibility and collaboration, <span class=bold>Falcon-H1</span> is released under the <a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html><span class=bold>Falcon LLM license</span></a>. We hope the AI community finds these models valuable for research, application development, and further experimentation. <span class=bold>Falcon-H1</span> is a continuation of our efforts to create more capable and efficient foundation models. We welcome feedback and collaboration from the community as we continue to refine and advance the capabilities of these models.</p><h2 id=useful-links>Useful Links<a hidden class=anchor aria-hidden=true href=#useful-links>#</a></h2><ul><li>Access to our models (including GPTQ and GGUF) through <a href=https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df>the Falcon-H1 HuggingFace collection</a>.</li><li>Check out our <a href=https://github.com/tiiuae/falcon-h1>Github page</a> for the latest technical updates on Falcon-H1 models.</li><li>Feel free to join <a href=https://discord.gg/bHx5QEaQ>our discord server</a> if you have any questions or to interact with our researchers and developers.</li><li>Check out the <a href=https://falconllm.tii.ae/falcon-terms-and-conditions.html>Falcon-LLM License link</a> for more details about the license.</li></ul><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><pre tabindex=0><code>@misc{tiifalconh1,
    title = {Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance},
    url = {https://falcon-lm.github.io/blog/falcon-h1},
    author = {Falcon-LLM Team},
    month = {May},
    year = {2025}
}
</code></pre><div class=post-contributors><div class=contributors-section><h4>Core Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Jingwei Zuo" class=contributor-image><p class=contributor-name>Jingwei Zuo</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Maksim Velikanov" class=contributor-image><p class=contributor-name>Maksim Velikanov</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Ilyas Chahed" class=contributor-image><p class=contributor-name>Ilyas Chahed</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Younes Belkada" class=contributor-image><p class=contributor-name>Younes Belkada</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Dhia Eddine Rhayem" class=contributor-image><p class=contributor-name>Dhia Eddine Rhayem</p></div><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Guillaume Kunsch" class=contributor-image><p class=contributor-name>Guillaume Kunsch</p></div></div></div><br><div class=contributors-section><h4>Additional Contributors</h4><div class=contributors-grid><div class=contributor><img src=https://falcon-lm.github.io/img/contributors/default.png alt="Falcon LLM team" class=contributor-image><p class=contributor-name>Falcon LLM team</p></div></div></div></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://falcon-lm.github.io/>Falcon</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)"}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>